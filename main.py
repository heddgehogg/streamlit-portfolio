import streamlit as st
from streamlit_option_menu import option_menu
import pandas as pd
import numpy as np
import pickle
from PIL import Image
import re
import math
import random
import matplotlib.pyplot as plt
import plotly.express as px
from scipy.stats import skew
from scipy.stats import binom
from scipy.stats import poisson
from scipy.stats import norm
from scipy.stats import chisquare

st.set_page_config(page_title='Portfolio',
                   page_icon='‚ú®',
                   layout='wide')

#SIDEBAR
sidebar_style = """
    <style>
    [data-testid="stSidebar"] {
        background-color: #EEF7FF;  
    }
    </style>
"""
st.markdown(sidebar_style, unsafe_allow_html=True)
with st.sidebar:
    selected = option_menu(
        menu_title='Julie Shcherbyna',
        menu_icon='lightning',
        options=['About Me', 'Blog', 'SQL', 'Python', 'Visualization', 'Recommender System', 'Diagrams'],
        icons=['person-hearts', 'quote', 'database', 'code-slash', 'bar-chart-line', 'controller', 'diagram-3'],
        default_index=0,)
    # 'Google Sheets' - 'table'

# ABOUT ME
if selected == 'About Me':
    st.markdown("<h1 style='text-align: center;'>Portfolio</h1>", unsafe_allow_html=True)
    with st.container():
        left_column, middle_column, right_column = st.columns((1,0.2,0.5))
        with left_column:
            st.header("Julie Shcherbyna")
            st.subheader("Aspiring Data Analyst/Product Manager")
            st.write("üëãüèª Hey, I'm Julie! I'm a data science and analytics almost undergraduate based in Ukraine. With prior relevant experience in tech, writing, sales, and social media management, I am constantly seeking unique internships to broaden my horizons before embarking on my data career upon graduation.")
            st.write("üìö With the COVID-19 pandemic behind us, I believe there is potential for data science to be applied in the retail industry. In response to the increasing demand for data analytics from both online and brick-and-mortar sales, I am thus aiming to enter this industry for my first full-time job.")
            st.write("üé∏ In addition, I'm into guitar playing, music, gym, running, video games, and books... A-a-and enjoy eating good food in my free time!")
            st.write("üë©üèª‚Äçüíª Academic interests: Data Visualization, Market Basket Analysis, Recommendation Systems, Natural Language Processing")
            st.write("üí≠ Ideal Career Prospects: Data Analyst, Data Scientist, Data Engineer, Business Intelligence Analyst, Product Manager")
            st.write("üìÑ [Resume ](https://drive.google.com/file/d/1x4FLEOxQfWFZnTcVMyO_2l1_Zzu_Ndn3/view?usp=sharing)")
        with middle_column:
            st.empty()
        with right_column:
            st.image(Image.open("images/me.jpg"))

    # --- CONTACT BUTTONS –Ω–∞ –±–æ–∫–æ–≤—ñ–π –ø–∞–Ω–µ–ª—ñ ---
    with st.sidebar:
        EMAIL = "juliafest05@gmail.com"
        SUBJECT = "Hello%20from%20your%20portfolio"
        BODY = "Hi!%20I%20saw%20your%20portfolio%20and..."
        LI_LINK = "https://www.linkedin.com/in/julie-shcherbyna-15a47a297/"
        TG_USERNAME = "ejikjulie"

        mailto = f"mailto:{EMAIL}?subject={SUBJECT}&body={BODY}"
        TG_APP = f"tg://resolve?domain={TG_USERNAME}"
        TG_WEB = f"https://t.me/{TG_USERNAME}"

        button_html = f"""
        <style>
        .contact-wrap {{
        display:flex; flex-direction:column; gap:8px;
        align-items:center; justify-content:center; margin-top:30px;
        }}
        .contact-btn {{
        --radius: 9999px;
        --pad: 12px 20px;
        --shadow: 0 6px 18px rgba(0,0,0,.15);
        --grad: linear-gradient(135deg,#7c3aed, #06b6d4);
        display:inline-flex; align-items:center; justify-content:center;
        padding: var(--pad);
        border-radius: var(--radius);
        font-weight: 600; letter-spacing:.2px;
        text-decoration:none; user-select:none;
        background: var(--grad);
        color: white !important;
        box-shadow: var(--shadow);
        transition: transform .12s ease, box-shadow .2s ease, filter .2s ease;
        }}
        .contact-btn:hover {{
        transform: translateY(-1px);
        box-shadow: 0 10px 22px rgba(0,0,0,.2);
        filter: brightness(1.05);
        }}
        .contact-btn .dot {{
        width:8px; height:8px; margin-right:8px; border-radius:9999px; background:white; opacity:.9;
        animation: pulse 1.8s infinite ease-in-out;
        }}
        @keyframes pulse {{
        0%,100% {{ transform: scale(1); opacity: .9; }}
        50% {{ transform: scale(1.35); opacity: .6; }}
        }}
        .chip {{
        padding:8px 14px; border-radius:9999px; font-weight:600;
        text-decoration:none; border:1px solid rgba(0,0,0,.08);
        background: rgba(255,255,255,.6); color: inherit !important;
        transition: background .2s ease, transform .12s ease;
        }}
        .chip:hover {{ background: rgba(255,255,255,.9); transform: translateY(-1px); }}
        </style>

        <div class="contact-wrap">
        <a class="contact-btn" href="{mailto}">
            <span class="dot"></span>Contact me
        </a>
        <a class="chip" href="{TG_WEB}"
            onclick="(function(){{window.location.href='{TG_APP}'; setTimeout(function(){{window.open('{TG_WEB}','_blank');}},800);}})(); return false;">
            Telegram
        </a>
        <a class="chip" href="{LI_LINK}">LinkedIn</a>
        </div>
        """

        st.markdown(button_html, unsafe_allow_html=True)

# BLOG
if selected == 'Blog':
    st.markdown("<h1 style='text-align: center;'>Blog</h1>", unsafe_allow_html=True)
    selected_options = ["Overview", "Article & Essay List", 
                        "Data Science Tools in Gaming Industry",
                        "Development of Game Recommender System",
                        "Consumer Segmentation to Study the Underconsumption Core Trend",
                        "Machine Learning in Healthcare", 
                        "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ Netflix",
                        "–ë–ª–æ–∫—á–µ–π–Ω —É –º–µ–¥–∏—Ü–∏–Ω—ñ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—Ü—ñ",
                        "–°–º–∞—Ä—Ç-–ø–æ–ª–∏—á–∫–∏: —è–∫ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Å—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤",
                        "Parental Leave Regulations"
                        ]
    selected = st.selectbox("Choose a section you would like to read:", options = selected_options)
    st.write("Current selection:", selected)
    if selected == "Overview":
        st.subheader("Overview")
        st.markdown("""
                    Welcome to my blog, a haven where words come alive and thoughts find their wings. 
                    
                    I must admit - from a young age, I have been enchanted by the magic of storytelling, 
                    a curious child with an insatiable appetite for reading and writing. 
                    Creativity has always been my constant companion, guiding me through a world brimming with endless possibilities. 
                    
                    Here, I joyfully share my articles, essays, and musings, hoping to spark a sense of wonder and reflection in you, dear reader. 
                    Join me on this journey, where every post is a piece of my heart and a glimpse into my ever-curious mind.
        """)
        st.write("For those looking forward to a good read, enjoy! üòä")
    
    elif selected == "Article & Essay List":
        st.subheader("Article & Essay List")
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/1.jpeg"), use_column_width=True)
            with text_column:
                st.subheader("Data Science Tools in Gaming Industry")
                st.write("April 13, 2023 | [Presentation](https://drive.google.com/file/d/15-XtkL5AiL4qU6rNgWchmZ572WUByuze/view?usp=sharing)")
                st.write("The application of data science tools on real life examples")
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/2.jpeg"), use_column_width=True)
            with text_column:
                st.subheader("Game Recommender System")
                st.write("April 18, 2024 | [Presentation](https://drive.google.com/file/d/1FRclX0J_qmyUhbJv_0orYQhJ42fxzaLW/view?usp=sharing)")
                st.write("Deatiled description of different filtering methods for recommender systems")  
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/8.jpg"), use_column_width=True)
            with text_column:
                st.subheader("Consumer Segmentation to Study the Underconsumption Core Trend")
                st.write("November 04, 2024 | [Article](https://docs.google.com/document/d/1v4Bao3t-z6RxaALdRvAvgJb3L-2cvZTd/edit?usp=sharing&ouid=107266174450085669622&rtpof=true&sd=true)")
                st.write("The Underconsumption core trend reflects a growing shift in consumer behavior toward doing more with less‚Äîprioritizing simplicity, " \
                "sustainability, and intentionality over excess. It highlights how individuals are increasingly rejecting overconsumption in favor of minimalism, " \
                "durability, and mindful purchasing decisions.")   
                st.markdown("**Think wiser, buy thrifted clothes**") 
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/3.webp"))
            with text_column:
                st.subheader("Machine Learning in Healthcare")
                st.write("April 16, 2024 | [Presentation](https://drive.google.com/file/d/1vEiC5A3XTRnR4t10oJxMhH0SWwbpSPTg/view)")
                st.markdown("Real life examples of machine learning in healthcare industry")       
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/4.jpeg"))
            with text_column:
                st.subheader("Application of Machine Learning on the example of Netflix")
                st.write("March 11-17, 2024 | [Article](https://conference.ikto.net/pub/akit_2024_11-17march_1.pdf#page=364)")
                st.markdown("Application of Machine Learning on the example of Netflix, how it started, how it works")       
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/5.jpeg"))
            with text_column:
                st.subheader("Blockcain in Healthcare")
                st.write("June 07, 2024 | [Article](https://www.lex-line.com.ua/?go=full_article&id=3877)")
                st.markdown("""Blockchain technology holds immense potential for revolutionizing the healthcare industry, particularly in the pharmaceutical field. 
                            In this article, several examples illustrate how blockchain can be utilized to enhance various aspects of pharmaceutical operations.""")
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/6.jpg"))
            with text_column:
                st.subheader("Smart shelves: how analytics create optimal product placement")
                st.write("November 07-08, 2024 | [Article](http://www.wayscience.com/wp-content/uploads/2024/11/Conference-Proceedings-November-7-8-2024.pdf#page=298)")
                st.markdown("""Example of big companies. Analytics, AI, IoT, and big data help retailers optimize inventory, forecast demand, and enhance customer experience.""")
        with st.container():
            text_column, image_column = st.columns((3,1))
            with image_column:
                st.image(Image.open("images/7.jpg"))
            with text_column:
                st.subheader("Parental Leave Regulations")
                st.write("January 13, 2025 | [Presentation](https://drive.google.com/file/d/15TADYJuA_eRI37Vho1NfcxmL4u7NoR9B/view?usp=sharing)")
                st.markdown("""The presentation compares national frameworks, highlighting how some states offer generous paid leave while others provide only 
                            limited unpaid options. This presentation was created with my colleagues from Erasmus program.""")
        
    elif selected == "Data Science Tools in Gaming Industry":
        st.subheader("Data Science Tools in Gaming Industry")
        st.write("April 13, 2023 | [Presentation](https://drive.google.com/file/d/15-XtkL5AiL4qU6rNgWchmZ572WUByuze/view?usp=sharing)")
        st.markdown("""
                    **~ Introduction**

                    Gaming has taken a massive leap from the days of Snake and Tetris. With technological advancements and innovations, gaming has become one 
                    of the most powerful industries in the world. As this market becomes more competitive, game creators are pushed to produce more innovative 
                    and generally better games, therefore data analysts are turning out to be really useful staff in gaming companies. The ultimate goal is to 
                    create an engaging experience for the player to spend more time playing the game. Analysts examine enormous surge of multi-source user data 
                    to gather and change into important insights. Thus, with these insights on hand, gaming companies can get a lot of benefits and produce more profits.

                    So, let‚Äôs consider on the particular examples most popular ways of how today‚Äôs gaming companies can augment their capabilities with data analytics, 
                    where analytics alongside data science is deployed for game development on.

                    **~ Three main ways**
                    
                    **1. Design**

                    The first one and the most exciting applications of data science in gaming is its use in the game development process. A game should be regarded 
                    as a kind mechanism which performance may be measured, and for example shows that a few levels may be excessively dull, some - excessively challenging, 
                    and some - essentially contain bugs that don't let users push ahead. 

                    Coincidentally, this is what happened to King Digital Entertainment. This well-known game designer once caught an unanticipated issue with its most popular 
                    game, Candy Crush Saga. Users were hugely abandoning level 65. ¬´We wanted to give our players a real challenge, as we didn‚Äôt know how long it would take before 
                    we did another update, thus, it required a little bit more effort and eyesight to win. So, the original version had 52 moves and 65 double jellies, which was 
                    impossible to complete without the strategy created mathematically. Plus, it has contained some bugs, so after the particular move the level was breaking without 
                    the possibility to be fixed by users¬ª, the creator says. The data analysts classified this level as a ‚ÄòNP-hard problem‚Äô. 

                    **2. Tracking key performance indicators**

                    In their endeavour to precisely measure a game's overall performance, makers definitely face the need to respond to some fairly pressing inquiries. (examples) 
                    By using these questions companies can more likely comprehend the explanations behind a game's high points and low points, and construct increasingly powerful 
                    strategies. For instance, if a game attracts new users every day, the likelihood that some of them will upgrade to a paid account ascents exponentially or make 
                    a profit for company by purchasing beautiful skins on frankly unattractive characters. Also, degrading monthly active users‚Äô rates may discuss approaching users 
                    wearing down, which is yet conceivable to fight off whenever recognised in time.

                    **3. Increasing monetisation**

                    Data analytics likewise enables gaming companies to perceive what precisely brings them more money and, consequently, modify their monetisation accordingly. 
                    To be sure, if a company uncovers those numerous users lean toward altering their armour or weapon, it's really sensible to offer them in-game armour and weapon 
                    upgrade.

                    Therefore, Zynga's example is here to demonstrate it. This gaming company's plan of action was allowed to play freely; however, it likewise offered a premium, 
                    ad-free account. The issue was that typically just **2%** of players really paid. Fortunately, the company discovered that in the first version of Farmville, users 
                    loved interacting with animals that were at first just decorations. A few users even began acquiring animals with real money, so in the accompanying version of 
                    the game, Zynga made animals a focal feature and even made 'uncommon species' to animate users additionally. Such a data driven methodology toward monetisation 
                    not just shows high return of investments for gaming companies; it likewise hits chord with users.

                    **~ Stocking Your Shop with Data Science**

                    Now to make the research more practical I would like to show on the instance how the data science tools work. Let‚Äôs talk about well-known company in gaming 
                    industry ¬´Riot Games¬ª and their masterpiece ¬´League of Legends¬ª. Firstly, I‚Äôll dive deep into one data science-fueled product - Your Shop. I‚Äôll explain in details 
                    on how the offers get to players in every region.

                    **1. Recommendation systems**

                    Recommendation systems fall under one of two categories, which, in general, are defined by the kind of data they‚Äôre built on. For **collaborative filtering methods**, 
                    user-item interactions are the underlying source. In contrast, **content-based** recommender systems focus on tagged attributes of the users and items.

                    For the latter system to work, a specific skin should have multiple tags that describe various features of the content; this could be anything from ‚Äúfunny‚Äù to ‚Äúcute‚Äù 
                    to ‚Äúedgelord.‚Äù. In the same light, champions would have labels as well, from basic role categorizations (‚ÄúADC,‚Äù ‚ÄúSupport,‚Äù ‚ÄúMid‚Äù) to damage type. As players start to 
                    build up a catalog of content that they own and play, they would start to inherit the traits of those items to create a unique player profile. This profile, along with 
                    item features, could be used to rank content that a player would prefer. 

                    **2. Utility matrix**

                    Regardless of the tags, the fundamental data structure fed into these algorithms is the same - a utility matrix of users and individual scores for champions they've 
                    interacted with or purchased. Here's an example: The obvious question is: How are these player-item scores calculated?

                    There are many random components, for instance game modes in League where champions are chosen at random, other may force a choice amongst a subset of heroes and skins, 
                    so, in these cases, the player is likely not explicitly expressing a deep interest for a particular champion or skin. But the similar players have similar preferences 
                    and that a piece of content is in the same ‚Äúneighborhood‚Äù as related items. Therefore, If two players, Tom and Jane, exhibit similar scores, then one can use Tom‚Äôs 
                    inferred ratings on the champion Zed to predict Jane‚Äôs unobserved rating on the same champion. Champions exist in neighborhoods, some closer than others, which inform what 
                    recommendations are made. One disadvantage is limited coverage due to sparsity; most players only interact with a small portion of champions and skins.

                    **3. Crucial metrics**

                    When launching any of the models, the company need high confidence in the recommendations it generates. In order to measure the effectiveness of a recommender, 
                    they evaluate a variety of metrics:

                    - **Coverage** encapsulates how many users will receive recommendations. If a player‚Äôs rating matrix is mostly empty because they are new to the game or haven‚Äôt 
                    played in quite some time, then their offers are usually replaced by a default set.

                    - **Novelty** evaluates the likelihood of a recommender system to provide offers that the player may not be aware of. This allows the player to discover 
                    additional information with their own likes and dislikes.

                    - **Serendipity** refers to recommendations that are unexpected. Imagine a player who likes to solo lane Riven. A recommendation to play Vayne might be seen 
                    as serendipitous, since it is somewhat unexpected by the player. This is because on the surface, Riven and Vayne don‚Äôt seem to share the same characteristics - 
                    but hidden underneath, their player bases are very similar.

                    - **Accuracy** has better short-term effects, as players are shown content they want now. The instinct here is that offering a skin on a champion that a player 
                    doesn‚Äôt currently play opens the door to a more diverse champion pool and a broader experience with *League*. 

                    **4. Loading of the shop**  

                    Ultimately, the whole process starts well before Your Shop opens for business and the icon shows up in players‚Äô clients. The algorithms are run a few days in 
                    advance so the analysts can ensure that all of evaluation metrics are satisfactory and that they have produced a solid set of recommendations for players. 
                    The crew stores the resulting datasets in a bucket which is then uploaded to the "Your Shop" services, where the company has deployments across the globe. 
                    Once Your Shop is activated, the service loads in the suggested recommendations as players turn over their cards. During this process, the eye is kept on the 
                    telemetry reflecting which recommendations were successful, pinpoint possible errors and measure the overall success of the system so the future iterations can 
                    be made with an even better experience.   

                    **~ Personality Quizzes**

                    Also, I would like to mention the iconic method: a **Personality Quiz**. Finding products most similar to the user based on their responses to a questionnaire. 
                    So how does this all apply to recommending a *League of Legends Champion* (playable character)?  

                    **1. Create a multi-dimensional space with features that help describe each Champion in the game (there‚Äôs currently 152 Champions to choose from).**

                    All of us are familiar with the 3-dimensional model, where each axis has different metrics, but what will happen then with n-dimensional one? Let‚Äôs be simple, 
                    we build a script that fetches games from the Riot API, from this extract the Champion played and in which lane, grab some features that will help describe the 
                    Champion, like how many kills they got, how much damage they did to objectives, how much damage they‚Äôve blocked, etc‚Ä¶ Then convert the dataframe into a matrix, 
                    where each row represents a Champion with 21 various statistics about their in-game performance on average across many samples.

                    **2. Fit the NN algorithm to this space.**

                    **Nearest Neighbours (NN)** is exactly what it says on the tin. For any point in a space, there are N-number of nearest neighbours. Literally, you just pick 
                    a point and find the N-closest points to that one. ‚ÄúN‚Äù can be any number, however the standard Python implementation defaults to 5. Hopefully, the visualisation 
                    below makes this crystal clear. Blue is the example point; Reds are the 5 nearest neighbours:

                    In the 2D space we can just press a ruler to our screen, however what happens in the unimaginable 4-dimensional space and beyond? We use formula Euclidean distance. 
                    On the slide 13 you can see the calculations.

                    **3. Create a survey that tries to place the person in that space.**

                    To do this, for each statistic, there needs to be a corresponding question and a way of translating their answer into a usable number. So, there is used a 
                    *‚ÄúHow much do you agree with the following statement‚Äù* style of questions. The reason it works well is because people respond on a scale. 
                    The first statistic in the dataset is: percentage of a Champion kills which were done without assistance. Artfully is translated into the following 
                    statement: *‚ÄúI am a lone wolf‚Äù* Why? Champions with a high % are people who get most of their kills when acting alone. Those with a low % rely on their team mates 
                    to help secure kills. Hence, lone wolf. This is the art of science part.Here you can see the following logic. Then continue creating questions, each time 
                    converting their answer to an actual value. A full list of the questions and their related statistics can be found on the slide 14.

                    Now, when someone completes the **Personality Quiz**, convert each answer to a statistic, normalise the values and boom ‚Äî we have converted their personality 
                    to a position in our 21-dimensional space and now have the exact results!

                    **~ Conclusion**
                    
                    The gaming industry has been growing exponentially. The number of active users tends to increase every minute and so does the overall income of the companies‚Äô 
                    developing games. Data science has entered various industries and improved the principles of their functioning forever. It has brought various businesses to a 
                    qualitatively new level of their development. The industry of gaming is no exception here. Moreover, data science techniques and methodologies have become integral 
                    parts of games development, design, operation, and many other stages of their functioning. 
                    """)
    
    elif selected == "Development of Game Recommender System":
        st.subheader("DEVELOPMENT OF GAME RECOMMENDER SYSTEM")
        st.write("April 18, 2024 | [Presentation](https://drive.google.com/file/d/1FRclX0J_qmyUhbJv_0orYQhJ42fxzaLW/view?usp=sharing)")
        st.markdown("""
                    As the gaming industry continues to experience rapid growth, driven by an estimated **2.96 billion** games available worldwide, the sheer abundance of options 
                    presents a challenge for gamers in selecting which game to play. This expansion, from the inception of the first computer game, Spacewar! in **1962** at MIT 
                    (Stephen Russell a.o.) [1], to today's landscape of over **50,000** titles spanning various genres, underscores the need for effective navigation and discovery 
                    mechanisms. 

                    While early games like *Spacewar!* laid the groundwork, advancements in technology have ushered in a new era of more advanced and interactive gaming experiences. 
                    In response to this, global giants would benefit if started creating powerful content-based game recommendation systems to fit the user's interests amidst the 
                    overwhelming array of choices. 

                    A recommender system is a type of information filtering system that predicts or suggests items, such as products, services, or content, that a user might be 
                    interested in. These systems are commonly used in e-commerce, entertainment, social media, and other online platforms to personalize and improve user experiences 
                    by providing relevant recommendations based on user data, such as past behavior, preferences, and demographics. [2]

                    Two common methods used in creating recommendation systems are Content-based Filtering and Collaborative Filtering. Content-based Filtering involves selecting 
                    items by evaluating the connection between item characteristics and user preferences, while Collaborative Filtering selects items based on the association between 
                    users with similar preferences [3]. Due to the capability of suggesting items that have not yet been rated by any user, future recommendation system will be based 
                    on Content-Based Filtering.

                    The main idea is simple: to create a user model, from collected data, and further use it to rate the dataset based on its compatibility to user.

                    The first stage of this research is to collect data of any game that is available on STEAM using dataset **‚Äú50 000 Steam Store Game‚Äù**, published by Nik Davis in **2019** 
                    [4]. The next step is to check data‚Äôs validity and utility to make sure that all data are useful and filled without any defected value through EDA method. 
                    Done the Feature Selection later, it is apparent that the most common and best parameters that could be used to build a suitable recommendation system are developer, 
                    categories, and genres.

                    The user's input of any STEAM game and given rating of that game is being converted into the user profile matrix using Content-based Filtering.
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,2,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/grs.1.png"), width=500)
                st.markdown("<p style='text-align: center;'><em>User Input and Rating</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    The following matrix is based on content which includes developer, genre, and categories since these three features are the most likely used tags for users to 
                    determine what game to their liking. On this basis, three broken down matrices will be created where condition of each attribute in each feature will be 
                    determined with One-Hot Encoding where ‚Äú0‚Äù assigned to attributes that is not belong to the game and ‚Äú1‚Äù assigned to attributes that is belong to the game. 
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1, 7, 1))
            with col1:
                st.empty()
            with col2:
                col2_1, col2_2 = st.columns(2)
                with col2_1:
                    st.image(Image.open("images/grs.2.png"), caption='Matrices Based on Categories', width=450)
                with col2_2:
                    st.image(Image.open("images/grs.3.png"), caption='Matrices Based on Genres', width=450)
            with col3:
                st.empty()
        with st.container():
            col1, col2, col3 = st.columns((1, 2, 1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/grs.4.png"), caption='Matrices Based on Developers', width=500)
            with col3:
                st.empty() 
        st.markdown("""
                    At the next stage, a weighted matrix is created, where the value in each column for the selected games is equal to the product of the One-Hot Encoding variables and 
                    the posted user rating, and the line Total is a sum of all values. 

                    Then, the total values in each table were normalized to ensure a proportional distribution of preferences. This normalization process facilitated fair comparison and 
                    interpretation of user preferences across different categories, genres, and developers.

                    The User Profile Matrix is the base to categorize which game to recommend to user. The step that needed is to multiply the User-Profile value on each game score. 
                    The result of the multiplication will become the level of similarity of the game to the user's preferences. The higher the result, the higher the chance that user would 
                    like to play the games. 

                    From the initial dataset, 5 users were picked randomly to test the method by asking them to input game that they have played or interested before and rate them based on their 
                    personal opinion. Using this method, the recommendation for 5 users have accuracy which can be calculated using formula (1).
                    """)
        st.latex(r'\text{Accuracy} = \frac{\text{Chosen game result}}{\text{Total result}}')
        st.markdown("""
                    The overall performance accuracy can be calculated, utilizing equation (2). Hence, the proposed method achieves an overall accuracy of 82%. In other words, out of every 
                    10 recommended games, 8 are deemed favorable for users to play. Based on the overall accuracy achieved by the proposed method, it can be concluded that the proposed mechanism 
                    successfully utilizes Content-based Filtering to give a game recommendation with user input as its base.
                    """)
        st.latex(r'''
                \text{Overall Accuracy} = \frac{\sum \text{(accuracy of all users)}}{\sum \text{(users)}}
                ''')

        st.markdown("""
                    From this project, it can be concluded that Content-based Filtering is suitable to be used on database that contain the attribute that used as comparing factor. For future work, 
                    this method can be improved by enhancing the database by inputting the demography data of user that chosen the same item and implementing Collaborative Filtering and Content-based 
                    Filtering at the same time, this way the result given can be more accurate and more personalized.

                    **References**

                    1.	A history of the computer game. Jesper Juul. URL: https://www.jesperjuul.net/thesis/2-historyofthecomputergame.html (April 1, 2024)
                    2.	J. Lee, M. Sun, and G. Lebanon, ‚ÄúA Comparative Study of Collaborative Filtering Algorithms,‚Äù 2012, [Online]. Available: http://arxiv.org/abs/1205.3193 (April 1, 2024)
                    3.	Van Meteren R., Van Someren M. Using Content-Based Filtering for Recommendation. URL: https://users.ics.forth.gr/~potamias/mlnia/paper_6.pdf (April 1, 2024)
                    4.	Davis, N. (2019) Steam store games (clean dataset), Kaggle. Available at: https://www.kaggle.com/datasets/nikdavis/steam-store-games (April 2, 2024)
                    """)

    elif selected == "Consumer Segmentation to Study the Underconsumption Core Trend":
        st.subheader("Consumer Segmentation to Study the Underconsumption Core Trend")
        st.write("Nov 04, 2024 | [Article](https://docs.google.com/document/d/1v4Bao3t-z6RxaALdRvAvgJb3L-2cvZTd/edit?usp=sharing&ouid=107266174450085669622&rtpof=true&sd=true)")
        with open("files/Consumer Segmentation to Study the Underconsumption Core Trend.docx", "rb") as f:
            st.download_button(
                label="üìÑ Download my coursework",
                data=f,
                file_name="Consumer Segmentation to Study the Underconsumption Core Trend.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

    elif selected == "Machine Learning in Healthcare":
        st.subheader("Machine Learning in Healthcare")
        st.write("May 16, 2024 | [Presentation](https://drive.google.com/file/d/1vEiC5A3XTRnR4t10oJxMhH0SWwbpSPTg/view?usp=sharing)")
        st.markdown("""
                    The Fourth Industrial Revolution and the processes associated with this social phenomenon affect virtually all areas, and it can be argued that 
                    there is no chance of not being involved in any field of human activity. The healthcare industry is also covered by automation processes in various areas - 
                    from diagnostics and patient records to the development of new drugs and surgical interventions using software applications, hardware and software robotic artificial intelligence systems [1]. 

                    Traditionally, the pharmaceutical industry has been rather slow to adopt new technologies due to strict regulations and complex supply chains. However, the coronavirus pandemic and full-scale
                    invasion have had a significant impact on the pharmaceutical industry, forcing it to quickly adapt and respond to the challenges posed by the spread of new "viruses."

                    One of the most prominent applications of Machine Learning in healthcare is medical imaging analysis. ML algorithms, particularly deep learning models, have shown remarkable performance 
                    in tasks such as image classification, segmentation, and disease detection. For example, convolutional neural networks **(CNNs)** have been used to automate the interpretation of radiological 
                    images, leading to faster and more accurate diagnosis of conditions such as cancer, neurological disorders, and cardiovascular diseases.

                    The next important issue is electronic health records **(EHRs)** which contain a wealth of information about patients' medical history, treatments, and outcomes. ML algorithms can 
                    analyse **EHR** data to identify disease risk factors, predict patient trajectories, and optimize treatment plans. Natural language processing **(NLP)** techniques enable the extraction of
                    valuable insights from unstructured clinical notes, facilitating clinical decision-making and improving healthcare delivery.

                    Machine Learning algorithms play a crucial role in accelerating drug discovery and development processes. By analysing molecular structures, genomic data, and clinical trial results, 
                    ML models can identify potential drug candidates, predict drug efficacy, and optimize treatment regimens. Today, a successful drug or vaccine requires several billion dollars in funding 
                    and takes several years to develop. By detecting patterns in data, artificial intelligence models can narrow down the resources needed to develop and find new drugs. In 2021, the biotech 
                    company Insilico Medicine announced the first AI-discovered drug to treat idiopathic pulmonary fibrosis [2]. Thanks to AI Insilico saved **90%** of the planned budget.

                    Personalized medicine approaches leverage ML algorithms to tailor treatments to individual patients based on their genetic makeup, medical history, and other personalized factors, 
                    leading to improved therapeutic outcomes, sleep, eating habits, nutrition supplement and reduced adverse effects. ML can also use big data and individualized data to deliver 
                    **"precision longevity"** by preparing personalized plans for nutrition, supplements, exercise, sleep, medications, and therapies. Rejuvenation biotechnology will no longer be 
                    limited to the rich, but will become available to everyone.

                    Internet of Things **(IoT)-enabled** infrastructure, such as smart hospitals and smart homes, integrates sensors, actuators, and ML-powered analytics to create intelligent 
                    environments that enhance patient care and operational efficiency. The IoT as part of ML is used to create smart rooms with temperature sensors, smart toilets, beds, and other 
                    types of invisible gadgets that will regularly analyse vital signs and other patient data to detect possible health crises. Aggregated data from wearable devices will allow us to 
                    accurately determine the status and course of serious illnesses, whether cardiovascular, fever, apnoea, pulmonary disease, asphyxiation, out-of-control falls, or fall injuries. 
                    Sudden changes in condition may trigger patient alerts, notification of next of kin, or an ambulance call.

                    Even complex surgeries that rely on sophisticated judgment and dexterous movements will become increasingly automated over time. Robotic surgeries grew from **1.8%** of all 
                    surgeries in **2012** to **15.1%** in **2018** [3]. Robots can already perform semi-autonomous surgical tasks such as colonoscopy, suturing, intestinal anastomosis, 
                    and dental implants under the supervision of a doctor. As ML learns from big data, robotic surgeries can move from a human surgeon controlling the robot to a surgeon 
                    supervising the robot and delegating some tasks. This could eventually lead to fully autonomous surgical robots. Finally, the emergence of medical nanorobots will offer numerous 
                    capabilities that surpass human surgeons. These miniature (1 to 10 nanometers) bots can repair damaged cells, fight cancer, correct genetic defects, and replace DNA molecules 
                    to eradicate disease.

                    A **2019** study [4] shows that ML healthcare markets will grow by **41.7%** annually to **$13 billion** by **2025** in areas such as hospital workflow, wearables, medical imaging 
                    and diagnostics, therapy planning, virtual assistants, and, most importantly, drug development. 

                    ML healthcare should not be viewed as just a market. In fact, it is a wave of transformations that will change the entire industry. ML-powered healthcare provides a perspective 
                    for human society to have a healthier and longer life. Global ML experts assessing the state and pace of ML development conclude that in 20 years, ML will be able to measure and 
                    improve human health, as well as help us get more happy moments in the life cycle of each person. At the same time, the further implementation of ML in the medical field has a 
                    number of urgent problems, including moral discomfort with machines making decisions affecting human health and life; legal and regulatory processes for doctors and surgeons in 
                    cases of death; investigation into responsibility in ML-related accidents or fatalities; potential parties responsible: hardware manufacturer, ML algorithm supplier, software company, 
                    supervising physician.

                    These problems are not short-term and will need to be solved in the long term. Analysis of research and practice shows that competition in the healthcare market, 
                    financial and other economic levers, new discoveries, and the accumulation of large amounts of medical data give optimistic estimates of the solution to these problems and the 
                    continued development of ML in healthcare.

                    References
                    
                    1. Ben Dickson. AI could help reduce the administrative costs of health car. VentureBeat. URL: https://venturebeat.com/ai/ai-could-help-reduce-the-administrative-costs-of-health-care/ (2024, March 27)
                    2. Conor Hale. Insilico Medicine begins first human trial of its AI-designed drug for pulmonary fibrosis. Nov 30, 2021. Fierce Biotech. URL: https://www.fiercebiotech.com/medtech/insilico-medicine-begins-first-human-trial-its-ai-designed-drug-for-pulmonary-fibrosis (2024, March 27)
                    3. Kyle H. Sheetz. Trends in the Adoption of Robotic Surgery for Common Surgical Procedures. Jan 10, 2020. JAMA Network Open. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6991252/#:~:text=From%20January%202012%20through%20June,laparoscopic%20and%20open%20surgery%20declined (2024, March 28)
                    4. Millicent Abadicio. AI in the Hospital Setting ‚Äì Challenges and Trends. Sep 8, 2020. Emerj. URL: https://emerj.com/ai-sector-overviews/ai-in-the-hospital-setting/ (2024, March 28)
                    """)

    elif selected == "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ Netflix":
        st.subheader("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ Netflix")
        st.write("March 11-17, 2024 | [Article](https://conference.ikto.net/pub/akit_2024_11-17march_1.pdf#page=364)")
        st.markdown("""
                    –ü–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å —Å—Ç—Ä—ñ–º—ñ–Ω–≥–æ–≤–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º —Ä—ñ–∑–∫–æ –∑—Ä–æ—Å–ª–∞ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ –∫—ñ–ª—å–∫–∞ —Ä–æ–∫—ñ–≤, —â–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É –ø–æ—è—Å–Ω—é—î—Ç—å—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ—é –ø–∞–Ω–¥–µ–º—ñ—î—é COVID-19, —è–∫–∞ —Å–ø—Ä–∏—á–∏–Ω–∏–ª–∞ –ø–æ–≤—Å—é–¥–Ω—ñ 
                    –ª–æ–∫–¥–∞—É–Ω–∏ —Ç–∞ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ü–∏—Ñ—Ä–æ–≤–∏—Ö —Ä–æ–∑–≤–∞–≥ —è–∫ –¥–ª—è –¥–æ–∑–≤—ñ–ª–ª—è, —Ç–∞–∫ —ñ –¥–ª—è –≤—ñ–¥–¥–∞–ª–µ–Ω–æ—ñÃà —Ä–æ–±–æ—Ç–∏. –ü–æ–ø–∏—Ç –Ω–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞ –∑–∞–ø–∏—Ç–æ–º —Å—Ç—Ä—ñ–º–∫–æ –∑—Ä—ñ—Å, 
                    –∑—Ä–æ–±–∏–≤—à–∏ –ø–æ—Å–ª—É–≥–∏ –Ω–∞–¥–∞–Ω–Ω—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ñ–¥–µ–æ –æ—Å–Ω–æ–≤–Ω–∏–º –¥–∂–µ—Ä–µ–ª–æ–º —Ä–æ–∑–≤–∞–≥, –≤—ñ–¥–ø–æ—á–∏–Ω–∫—É —Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ñÃà, —Ç–∏–º —Å–∞–º–∏–º –ø—Ä–∏—Å–∫–æ—Ä–∏–≤—à–∏ –≤–∂–µ –∑—Ä–æ—Å—Ç–∞—é—á—É —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—é —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É. 

                    –ù–∞—Ä–∞–∑—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –ø–æ—Ç–æ–∫–æ–≤–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º —î –Ω–∞–¥–∑–≤–∏—á–∞–∏ÃÜ–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–∏–º, –æ—Å–∫—ñ–ª—å–∫–∏ —Ü—ñ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ —Å—Ç–∞–ª–∏ –Ω–µ–≤—ñ–¥‚Äô—î–º–Ω–æ—é —á–∞—Å—Ç–∏–Ω–æ—é —Ç–æ–≥–æ, 
                    —è–∫ –ª—é–¥–∏ —Å–ø–æ–∂–∏–≤–∞—é—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Ç–∞ –≤–∑–∞—î–º–æ–¥—ñ—é—Ç—å —ñ–∑ –Ω–∏–º. –ó–º—ñ–Ω–∏ –≤–ø–æ–¥–æ–±–∞–Ω—å —ñ –æ—á—ñ–∫—É–≤–∞–Ω—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –≤–∏–º–∞–≥–∞—é—Ç—å –ø–æ—Å—Ç—ñ–∏ÃÜ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è, —â–æ–± –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ –±–µ–∑–ø–µ—Ä–µ–±—ñ–∏ÃÜ–Ω–µ 
                    –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è, –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–∏ÃÜ –¥–æ—Å–≤—ñ–¥ —ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω—É –∞–¥–∞–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å. –°–∞–º–µ —Ç–æ–º—É –≤ –¥–∞–Ω—ñ–∏ÃÜ —Å—Ç–∞—Ç—Ç—ñ –±—É–¥—É—Ç—å –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Å–∫–ª–∞–¥–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ —Ç–∞ —ó—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ 
                    —Å–∏—Å—Ç–µ–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ñÃà –∫–æ–º–ø–∞–Ω—ñ—ñÃà **¬´Netflix¬ª**. 

                    –ö—Ä–∏—Ö—ñ—Ç–Ω–∏–π —Å—Ç–∞—Ä—Ç–∞–ø, —â–æ –ø–æ—á–∏–Ω–∞–≤—Å—è –¥–≤–∞ –¥–µ—Å—è—Ç–∏–ª—ñ—Ç—Ç—è —Ç–æ–º—É, —Ç–∞ –ø—Ä–æ—Å–∏–≤ Blockbuster –ø—Ä–∏–¥–±–∞—Ç–∏ –π–æ–≥–æ –≤—Å—å–æ–≥–æ –∑–∞ **$50 –º–ª–Ω** –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏–≤—Å—è –Ω–∞ –Ω–∞–π–±—ñ–ª—å—à—É —É —Å–≤—ñ—Ç—ñ –ø–æ—Ç–æ–∫–æ–≤—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É. 
                    –ê–ª–µ —è–∫ Netflix —Ü–µ –≤–¥–∞–ª–æ—Å—è? –£ –ø–µ—Ä—à—ñ –¥–Ω—ñ —Å–≤–æ–≥–æ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–º–æ–¥–µ–ª—å Netflix –±–∞–∑—É–≤–∞–ª–∞—Å—è –ª–∏—à–µ –Ω–∞ –ø—Ä–æ–∫–∞—Ç—ñ —Ñ—ñ–ª—å–º—ñ–≤ –Ω–∞ DVD, –ø—Ä–æ–ø–æ–Ω—É—é—á–∏ –Ω–µ–æ–±–º–µ–∂–µ–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ü–∏—Ö —Å–∞–º–∏—Ö 
                    DVD –±–µ–∑ –¥–∞—Ç –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è, –∫–æ–º—ñ—Å—ñ–π –∑–∞ –ø—Ä–æ—Å—Ç—Ä–æ—á–µ–Ω–Ω—è –∞–±–æ –º—ñ—Å—è—á–Ω–∏—Ö –æ–±–º–µ–∂–µ–Ω—å –Ω–∞ –æ—Ä–µ–Ω–¥—É. –ö–æ–º–ø–∞–Ω—ñ—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞ –ø—Ä–æ—Å—Ç—É —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ 5-–∑—ñ—Ä–∫–æ–≤–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥—É. 
                    –õ—é–¥—è–º –±—É–ª–æ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–æ –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏ —Ñ—ñ–ª—å–º–∏, —è–∫—ñ –≤–æ–Ω–∏ –±—Ä–∞–ª–∏ –Ω–∞–ø—Ä–æ–∫–∞—Ç –ø—ñ—Å–ª—è —ó—Ö –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è. –ù–∞ —Ç–æ–π —á–∞—Å –æ—Ü—ñ–Ω–∫–∏ –±—É–ª–∏ —î–¥–∏–Ω–∏–º —Ç–æ—á–Ω–∏–º —Å–ø–æ—Å–æ–±–æ–º –¥—ñ–∑–Ω–∞—Ç–∏—Å—è, —â–æ —Ö—Ç–æ—Å—å –¥—ñ–π—Å–Ω–æ –¥–∏–≤–∏–≤—Å—è DVD. 
                    5-–∑—ñ—Ä–∫–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–æ–ø–æ–º–æ–≥–ª–∞ Netflix –∑—ñ–±—Ä–∞—Ç–∏ –≤–∞–∂–ª–∏–≤—ñ –¥–∞–Ω—ñ. –©–æ –≤–∏—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥, —Ç–æ –∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å DVD-–¥–∏—Å–∫—ñ–≤, —ñ —Ç–∏–º –±—ñ–ª—å—à–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –≤—ñ–Ω —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è —ñ–Ω—à–∏–º –∫–ª—ñ—î–Ω—Ç–∞–º. 

                    –ö–æ–ª–∏ –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç –ø–æ—á–∞–≤ —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è, –†—ñ–¥ –ì–∞—Å—Ç—ñ–Ω–≥—Å, –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä Netflix, –≤–∏–∑–Ω–∞—á–∏–≤ —É–Ω—ñ–∫–∞–ª—å–Ω—É –ø–µ—Ä–µ–≤–∞–≥—É Netflix —É –ø–æ—î–¥–Ω–∞–Ω–Ω—ñ –≤–º—ñ–ª–æ–≥–æ –æ–ø–æ–≤—ñ–¥–∞–Ω–Ω—è —ñ —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂—É–≤–∞–Ω–æ—ó –ø–æ—Ç—É–∂–Ω–æ—Å—Ç—ñ 
                    –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç—É, —â–æ–± –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—ñ –≤—Ä–∞–∂–µ–Ω–Ω—è –≤—ñ–¥ –ø–µ—Ä–µ–≥–ª—è–¥—É –¥–ª—è —Å–≤–æ—ó—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤. –£ **2007** —Ä–æ—Ü—ñ Netflix –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–≤ —Å–≤–æ—é –ø–µ—Ä—à—É –ø–æ—Å–ª—É–≥—É –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–µ—Ä–µ–¥–∞–≤–∞–Ω–Ω—è Watch Now, 
                    –∞ –≤–∂–µ —á–µ—Ä–µ–∑ –¥–≤–∞ —Ä–æ–∫–∏ –∑–∞–ø–æ—á–∞—Ç–∫—É–≤–∞–≤ Netflix Prize –∑ –º–µ—Ç–æ—é –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫–∏–π –¥–æ—Å–≤—ñ–¥ –Ω–∞ **‚â• 10%**. –ö–æ–º–∞–Ω–¥–∞, —è–∫–∞ –æ—Ç—Ä–∏–º–∞–ª–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É —É —Ä–æ–∑–º—ñ—Ä—ñ **$1 –º–ª–Ω**, —Ä–æ–∑—Ä–æ–±–∏–ª–∞ —Å–∫–ª–∞–¥–Ω—É 
                    —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π, —â–æ –±—É–¥—É–≤–∞–ª–∞—Å—è –Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó —Ç–∞ —Å–∫–ª–∞–¥–∞–ª–∞—Å—è –∑ –ø–æ–Ω–∞–¥ 100 —Ä—ñ–∑–Ω–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤. 

                    –°–ø—ñ–ª—å–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è ‚Äî —Ü–µ –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –¥–∞–Ω—ñ –º–∏–Ω—É–ª–æ—ó –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑–≤‚Äô—è–∑–∫—ñ–≤ –º—ñ–∂ ¬´–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏ —Ç–∞ –≤–∑–∞—î–º–æ–∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ 
                    –º—ñ–∂ –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –∞—Å–æ—Ü—ñ–∞—Ü—ñ–π –º—ñ–∂ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏ —Ç–∞ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏¬ª. –Ü—Å–Ω—É—î –¥–≤–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ —Å–ø—ñ–ª—å–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ **(UBCF)** —ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ **(IBCF)**. 
                    **UBCF** —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ –ø—Ä–∏–ø—É—Å–∫–∞—î, —â–æ –ª—é–¥–∏, —è–∫—ñ –º–∞–ª–∏ –ø–æ–¥—ñ–±–Ω—ñ –¥—É–º–∫–∏ –≤ –º–∏–Ω—É–ª–æ–º—É, —à–≤–∏–¥—à–µ –∑–∞ –≤—Å–µ, –±—É–¥—É—Ç—å –ø–æ–¥—ñ–ª—è—Ç–∏ —Ç—ñ –∂ –¥—É–º–∫–∏ –∑–Ω–æ–≤—É –≤ –º–∞–π–±—É—Ç–Ω—å–æ–º—É, —Ç–æ–¥—ñ —è–∫ **IBCF** –ø—Ä–∏–ø—É—Å–∫–∞—î, —â–æ –ª—é–¥–∏ –∑–∞–≤–∂–¥–∏ —Ö–æ—á—É—Ç—å –º–∞—Ç–∏ —Å—Ö–æ–∂—ñ —Ä–µ—á—ñ. 
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,3,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/netflix.1.webp"))
                st.markdown("<p style='text-align: center;'><em>UBCF vs IBCF</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    –Ø–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ –º–∞–ª—é–Ω–∫—É –≤–∏—â–µ, –≤ **UBCF** –¢—ñ–º —ñ –î–∂–æ–Ω –æ–±–æ—î –ª—é–±–ª—è—Ç—å —à–æ–∫–æ–ª–∞–¥ —ñ –º–æ—Ä–æ–∑–∏–≤–æ, —è–∫ –ø–æ–∫–∞–∑–∞–Ω–æ —Å—Ç—Ä—ñ–ª–∫–∞–º–∏, —Ç–æ–º—É –≤–æ–Ω–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—é—Ç—å—Å—è —è–∫ —Ç–∞–∫—ñ, —â–æ –º–∞—é—Ç—å –ø–æ–¥—ñ–±–Ω—ñ –¥—É–º–∫–∏. 
                    –¢—ñ–º —Ç–∞–∫–æ–∂ –ª—é–±–∏—Ç—å –º–æ—Ä–æ–∑–∏–≤–æ —Ç–∞ –ø–æ–Ω—á–∏–∫–∏, –æ—Ç–∂–µ, –º–æ–∂–Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏, —â–æ –î–∂–æ–Ω –±—É–¥–µ –º–∞—Ç–∏ –≤–∏—Å–æ–∫—É –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞—Å–æ–ª–æ–¥–∏—Ç–∏—Å—è –º–æ—Ä–æ–∑–∏–≤–æ–º —Ç–∞ –ø–æ–Ω—á–∏–∫–∞–º–∏, –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –º–∞—é—Ç—å —Å—Ö–æ–∂—ñ —ñ–Ω—Ç–µ—Ä–µ—Å–∏. 
                    –î–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è, –≤ IBCF –º–æ—Ä–æ–∑–∏–≤–æ –≤ —Ä—ñ–∂–∫–∞—Ö —ñ –ø–æ—Ä—Ü—ñ–π–Ω–µ –º–æ—Ä–æ–∑–∏–≤–æ —î –¥–≤–æ–º–∞ –≤–∏–¥–∞–º–∏ –º–æ—Ä–æ–∑–∏–≤–∞. –Ø–∫—â–æ –î–∂–æ–Ω –ª—é–±–∏—Ç—å –º–æ—Ä–æ–∑–∏–≤–æ –≤ —Ä—ñ–∂–∫—É, –≤—ñ–Ω, —à–≤–∏–¥—à–µ –∑–∞ –≤—Å–µ, –ø–æ–ª—é–±–∏—Ç—å –π –ø–æ—Ä—Ü—ñ–π–Ω–µ –º–æ—Ä–æ–∑–∏–≤–æ.

                    –î–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–µ—Ç–æ–¥—ñ–≤ –∫–æ–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞ –∑–∞—Å–Ω–æ–≤–∞–Ω–æ–≥–æ –Ω–∞ –∑–º—ñ—Å—Ç—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—ñ–≤ **(UBCF —Ç–∞ IBCF –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ)** –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ —Ç–∞ –µ–ª–µ–º–µ–Ω—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ —É –≤–∏–≥–ª—è–¥—ñ –≤–µ–∫—Ç–æ—Ä—ñ–≤
                    –Ω–∞ –æ—Å–Ω–æ–≤—ñ —à–∞–±–ª–æ–Ω—ñ–≤ –æ—Ü—ñ–Ω–∫–∏, –¥–æ —è–∫–∏—Ö –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è –º–µ—Ç–æ–¥–∏ –º–∞—Ç—Ä–∏—á–Ω–æ—ó —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó. –î–ª—è –µ–ª–µ–º–µ–Ω—Ç–∞ *i*, *qi* –≤–∏–º—ñ—Ä—é—î, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ —Å–∏–ª—å–Ω–æ (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ) —á–∏ —Å–ª–∞–±–∫–æ (–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ) –µ–ª–µ–º–µ–Ω—Ç 
                    –≤–æ–ª–æ–¥—ñ—î –ø–µ—Ä–µ–ª—ñ–∫–æ–º —Ñ–∞–∫—Ç–æ—Ä—ñ–≤, —Ç–∞–∫–∏—Ö —è–∫ —Ä–æ–∑–≤–∏—Ç–æ–∫ —Ö–∞—Ä–∞–∫—Ç–µ—Ä—É —á–∏ —Ä–æ–º–∞–Ω—Ç–∏—á–Ω–∏–π –∫—ñ–Ω–µ—Ü—å. –î–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ *u*, *Pu* –≤–∏–º—ñ—Ä—é—î —Å—Ç—É–ø—ñ–Ω—å –∑–∞—Ü—ñ–∫–∞–≤–ª–µ–Ω–æ—Å—Ç—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–∏—Ö –∂–µ 
                    —Ñ–∞–∫—Ç–æ—Ä—ñ–≤. –û–±—á–∏—Å–ª—é—é—á–∏ —Å–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ –¥–≤–æ—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤, –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ —á–∏—Å–ª–æ *Rui*, —è–∫–µ –æ—Ü—ñ–Ω—é—î —ñ–Ω—Ç–µ—Ä–µ—Å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –¥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –µ–ª–µ–º–µ–Ω—Ç–∞.
                    """)
        st.latex(r"""
                R_{ui} = q_i * P_u
                 """)
        st.markdown("""
                    –û—Å–Ω–æ–≤–Ω–∞ –ø–µ—Ä–µ–≤–∞–≥–∞ –º–∞—Ç—Ä–∏—á–Ω–æ—ó —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó –ø–æ–ª—è–≥–∞—î –≤ —ó—ó –≥–Ω—É—á–∫–æ—Å—Ç—ñ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –≤–µ–ª–∏–∫–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö. –î–ª—è Netflix –∞—Å–ø–µ–∫—Ç–∏ –¥–∞–Ω–∏—Ö —â–æ–¥–æ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –º–æ–∂—É—Ç—å –≤–∫–ª—é—á–∞—Ç–∏ –∂–∞–Ω—Ä–∏, 
                    –∞–∫—Ç–æ—Ä—Å—å–∫–∏–π —Å–∫–ª–∞–¥, —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Å—Ç—Ä—ñ—á–∫–∏ —Ç–æ—â–æ. –Ü–Ω—Ç–µ—Ä–µ—Å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –º–æ–∂–Ω–∞ –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–∞–∫–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤, —è–∫ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–≥–ª—è–¥—ñ–≤, –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–∏—Ö –ø–µ—Ä–µ–≥–ª—è–¥—ñ–≤ 
                    —Ç–∞ —á–∞—Å –¥–æ–±–∏, –∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑–∞–∑–≤–∏—á–∞–π –ø–µ—Ä–µ–±—É–≤–∞—î –Ω–∞ Netflix. –¢–æ–∂, —Ü—ñ –º–µ—Ç–æ–¥–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó –≤–º—ñ—Å—Ç—É –¥–ª—è –æ–∫—Ä–µ–º–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, 
                    –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–º—ñ—Å—Ç—É —Ç–∞ –∑–≤–∏—á–∫–∏ –ø–µ—Ä–µ–≥–ª—è–¥—É.

                    –°–∏—Å—Ç–µ–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π —Å–ø—Ä–æ—â—É—é—Ç—å –ª—é–¥—è–º –ø—Ä–æ—Ü–µ—Å –≤–∏–±–æ—Ä—É. –ë—Ä–∏—Ç–∞–Ω—Å—å–∫–∏–π —Ç–µ–ª–µ–ø—Ä–æ–¥—é—Å–µ—Ä —ñ –ø—Ä–æ—Ñ–µ—Å–æ—Ä –î–∂–æ–Ω –ï–ª–ª—ñ—Å —É —Å–≤–æ—ó–π –∫–Ω–∏–∑—ñ ¬´Seeing Things: Television in the Age of Uncertainty¬ª 
                    –ø–∏—à–µ: ¬´–í–∏–±–∏—Ä–∞—Ç–∏ –æ–∑–Ω–∞—á–∞—î —É—Å–≤—ñ–¥–æ–º–ª—é–≤–∞—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, —è–∫—ñ –≤—Ç—Ä–∞—á–∞—é—Ç—å—Å—è¬ª. –°–∞–º–µ —Ü–µ–π —Å—Ç—Ä–∞—Ö –≤—Ç—Ä–∞—Ç–∏ —à–≤–∏–¥–∫–æ –æ—Ö–æ–ø–ª—é—î –ª—é–¥–µ–π, –∑–º—É—à—É—é—á–∏ —ó—Ö —Å—É–º–Ω—ñ–≤–∞—Ç–∏—Å—è. Netflix, 
                    –∞–Ω–∞–ª—ñ–∑—É—é—á–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É —Å–≤–æ—ó—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, –∑—Ä–æ–∑—É–º—ñ–≤, —â–æ –±–∞–≥–∞—Ç–æ –ª—é–¥–µ–π –≥—É–±–ª—è—Ç—å —ñ–Ω—Ç–µ—Ä–µ—Å –¥–æ –≤–∏–±–æ—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –ø—Ä–æ—Ç—è–≥–æ–º 60-90 —Å–µ–∫—É–Ω–¥, –ø–µ—Ä–µ–≥–ª—è–Ω—É–≤—à–∏ –≤—ñ–¥ **10 –¥–æ 20** –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤. 
                    –¢–æ–∂ —ó—Ö–Ω—å–æ—é –º–µ—Ç–æ—é —Å—Ç–∞–ª–æ –∑–∞–ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏ —Ü—ñ–∫–∞–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–≤—ñ –º–µ–Ω—à–µ, –Ω—ñ–∂ –∑–∞ **1,5 —Ö–≤–∏–ª–∏–Ω–∏**, —â–æ–± –∑–±–µ—Ä–µ–≥—Ç–∏ —É–≤–∞–≥—É —Ç–∞ —É—Ç—Ä–∏–º–∞—Ç–∏ —é–∑–µ—Ä–∞ –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ñ.

                    –ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, –Ω–∞ —è–∫–æ–º—É –ø—Ä–∏—Å—Ç—Ä–æ—ó –≤–∏ –¥–∏–≤–∏—Ç–µ—Å—è Netflix, –≤–∏ –∑–∞–≤–∂–¥–∏ –ø–µ—Ä—à–∏–º –ø–æ—Ç—Ä–∞–ø–ª—è—î—Ç–µ –Ω–∞ –¥–æ–º–∞—à–Ω—é —Å—Ç–æ—Ä—ñ–Ω–∫—É. –ó–≥—ñ–¥–Ω–æ –∑ –¥–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫–æ—é —Å—Ç–∞—Ç—Ç–µ—é, –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ—é Netflix –Ω–∞ **ACM**, 
                    80% –≥–æ–¥–∏–Ω –ø–æ—Ç–æ–∫–æ–≤–æ—ó —Ç—Ä–∞–Ω—Å–ª—è—Ü—ñ—ó —î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Å–∏—Å—Ç–µ–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –¥–æ–º–∞—à–Ω—ñ–π —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ, –∞ —Ä–µ—à—Ç–∞ **20%** ‚Äì —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø–æ—à—É–∫—É. –î–ª—è –ª–µ–≥–∫–æ—ó —Ç–∞ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ –∑—Ä–æ–∑—É–º—ñ–ª–æ—ó –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó 
                    Netflix –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–∞–∫–µ—Ç —Å—ñ—Ç–∫–∏. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º –≥–ª–∏–±—à–µ –∑–∞–Ω—É—Ä–∏—Ç–∏—Å—è –≤ –ø–µ–≤–Ω–∏–π –∂–∞–Ω—Ä –∞–±–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Ä—è–¥–∫–∏, —è–∫—ñ —ó—Ö –Ω–µ —Ü—ñ–∫–∞–≤–ª—è—Ç—å.
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,2,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/netflix.2.png"))
                st.markdown("<p style='text-align: center;'><em>Netflix Main Screen</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    –î–ª—è Netflix –¥—É–∂–µ –≤–∞–∂–ª–∏–≤–æ —Ä–æ–∑–º—ñ—â—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, —è–∫–∏–π –Ω–∞–π–±—ñ–ª—å—à–µ —Ü—ñ–∫–∞–≤–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, —É –≤–µ—Ä—Ö–Ω—å–æ–º—É –ª—ñ–≤–æ–º—É –∫—É—Ç—ñ. –î–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—ñ—Ç–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫—ñ–ª—å–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤. 
                    –ü–æ-–ø–µ—Ä—à–µ, —Ü–µ ‚Äì **Personalized Video Ranker (PVR)**, —è–∫–∏–π —Å—Ç–≤–æ—Ä—é—î —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫ –∫–∞—Ç–∞–ª–æ–≥—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –æ–±–ª—ñ–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∑–≤–∏—á–æ–∫ –ø–µ—Ä–µ–≥–ª—è–¥—É —É—á–∞—Å–Ω–∏–∫–∞. 
                    –ó–≥–æ–¥–æ–º, –æ—Ç—Ä–∏–º–∞–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫ —É –∫–∞—Ç–∞–ª–æ–∑—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∂–∞–Ω—Ä—É —Ç–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Ä—è–¥–∫—É –≤–ø–æ—Ä—è–¥–∫—É–≤–∞–Ω–Ω—è –≤–º—ñ—Å—Ç—É –≤ –æ–¥–Ω–æ–º—É —Ä—è–¥–∫—É. 
                    –¢–∞–∫–∏–º —á–∏–Ω–æ–º, –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –º–æ–∂—É—Ç—å –º–∞—Ç–∏ –∞–±—Å–æ–ª—é—Ç–Ω–æ —Ä—ñ–∑–Ω–∏–π –≤–º—ñ—Å—Ç —É —Ä—è–¥–∫–∞—Ö, —è–∫—ñ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤—É –Ω–∞–∑–≤—É –∂–∞–Ω—Ä—É, —è–∫-–æ—Ç –∂–∞—Ö–∏.

                    –ü—ñ—Å–ª—è —Ç–æ–≥–æ, —è–∫ **PVR** –∑–≤—É–∂—É—î –∫–∞—Ç–∞–ª–æ–≥, Top N video ranker, –∞–ª–≥–æ—Ä–∏—Ç–º, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤, —è–∫—ñ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å—Å—è –≤ —Ä—è–¥–∫—É **Top Picks** 
                    –Ω–∞ –¥–æ–º–∞—à–Ω—ñ–π —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ, –≤–∏–∑–Ω–∞—á–∞—î –Ω–∞–π–±—ñ–ª—å—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –≤—ñ–¥–µ–æ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ —Ç–∞ –ø–µ—Ä–µ—Ä–∞—Ö–æ–≤—É—î —ó—Ö —É –Ω–æ–≤–æ–º—É —Ä—è–¥–∫—É. –Ü–Ω—à—ñ —Ä—è–¥–∫–∏, —è–∫-–æ—Ç ¬´–ü–æ–ø—É–ª—è—Ä–Ω—ñ –∑–∞—Ä–∞–∑¬ª, 
                    ¬´–ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –ø–µ—Ä–µ–≥–ª—è–¥¬ª —ñ ¬´–û—Å–∫—ñ–ª—å–∫–∏ –≤–∏ –¥–∏–≤–∏–ª–∏—Å—è¬ª **(BYW)**, —Ç–∞–∫–æ–∂ —Å—Ç–≤–æ—Ä—é—é—Ç—å—Å—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤. –ó–æ–∫—Ä–µ–º–∞, **BYW** –≥–µ–Ω–µ—Ä—É—î—Ç—å—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ –≤—ñ–¥–µ–æ –¥–æ –≤—ñ–¥–µ–æ, 
                    —Ç–∏–ø–æ–º —Å–ø—ñ–ª—å–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ **(IBCF)**. –†–µ–π—Ç–∏–Ω–≥–æ–≤–∏–π —Å–ø–∏—Å–æ–∫ –≤—ñ–¥–µ–æ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—ñ–¥–µ–æ –≤ –∫–∞—Ç–∞–ª–æ–∑—ñ, –∞ –ø–æ—Ç—ñ–º –¥–æ –Ω—å–æ–≥–æ –¥–æ–¥–∞—î—Ç—å—Å—è –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ —Å–ø–∏—Å–∫—É 
                    –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ —É–ø–æ–¥–æ–±–∞–Ω—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞. –ê–ª–≥–æ—Ä–∏—Ç–º –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à—ñ —Ä—è–¥–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞, —É–Ω–∏–∫–∞—é—á–∏ –¥—É–±–ª—é–≤–∞–Ω–Ω—è –≤–º—ñ—Å—Ç—É 
                    —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫–∏. 

                    –ù–∞—Å—Ç—É–ø–Ω–∏–º –∫—Ä–æ–∫–æ–º —î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–µ–ª–µ—à–æ—É —Ç–∞ —Ñ—ñ–ª—å–º—É, —Ç–∞–∫–∏—Ö —è–∫ –æ–±–∫–ª–∞–¥–∏–Ω–∫–∞, –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å, –∞–∫—Ç–æ—Ä—Å—å–∫–∏–π —Å–∫–ª–∞–¥ —ñ –ø—ñ–¥–∂–∞–Ω—Ä–∏. –ó–∞ –¥–∞–Ω–∏–º–∏ –Ω–µ–π—Ä–æ–±—ñ–æ–ª–æ–≥—ñ–≤ 
                    –∑ –ú–∞—Å—Å–∞—á—É—Å–µ—Ç—Å—å–∫–æ–≥–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ —ñ–Ω—Å—Ç–∏—Ç—É—Ç—É, –ª—é–¥—Å—å–∫–æ–º—É –º–æ–∑–∫—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ—Ç—Ä—ñ–±–Ω–æ –ª–∏—à–µ **13 –º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥**. –û—Å–∫—ñ–ª—å–∫–∏ –æ–±–∫–ª–∞–¥–∏–Ω–∫–∞ –∑–∞–π–º–∞—î –Ω–∞–π–±—ñ–ª—å—à–µ –º—ñ—Å—Ü—è –Ω–∞ –µ–∫—Ä–∞–Ω—ñ, 
                    –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–µ –º–æ–∂–µ –π–æ–≥–æ –ø—Ä–æ—ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏. –î–ª—è **Stranger Things**, –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–æ–≥–æ —à–æ—É –Ω–∞ Netflix —É **2019** —Ä–æ—Ü—ñ, –æ–±–∫–ª–∞–¥–∏–Ω–∫–∏, –ø–æ–∫–∞–∑–∞–Ω—ñ –Ω–∞ –º–∞–ª—é–Ω–∫—É –Ω–∏–∂—á–µ, –±—É–ª–∏ —Å—Ç–≤–æ—Ä–µ–Ω—ñ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ 
                    –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —ñ–∑ —Ä—ñ–∑–Ω–∏–º–∏ –∫–∞–¥—Ä–∞–º–∏, –∫–æ–ª—å–æ—Ä–∞–º–∏ —à—Ä–∏—Ñ—Ç—ñ–≤, —Ä–æ–∑–º—ñ—Ä–∞–º–∏ —Ç–∞ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è–º.
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,2,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/netflix.3.png"), width=500)
                st.markdown("<p style='text-align: center;'><em>Stranger Things' Example</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    –ñ–æ–¥–Ω–∞ —ñ–Ω—à–∞ –ø–æ—Ç–æ–∫–æ–≤–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –Ω–∞—Ä–∞–∑—ñ –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –æ–±–∫–ª–∞–¥–∏–Ω–∫–∏ —Ç–∞–∫, —è–∫ —Ü–µ —Ä–æ–±–∏—Ç—å Netflix. –ù–∞–π—Å–∏–ª—å–Ω—ñ—à–æ—é –ø–µ—Ä–µ–≤–∞–≥–æ—é Netflix –ø–µ—Ä–µ–¥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ —î –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è. 
                    –£ –±–ª–æ–∑—ñ Netflix: ¬´—É –Ω–∞—Å —î –Ω–µ –æ–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç, –∞ –ø–æ–Ω–∞–¥ **100 –º—ñ–ª—å–π–æ–Ω—ñ–≤** —Ä—ñ–∑–Ω–∏—Ö –ø—Ä–æ–¥—É–∫—Ç—ñ–≤, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑ –Ω–∞—à–∏—Ö —É—á–∞—Å–Ω–∏–∫—ñ–≤ —ñ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è–º–∏ —Ç–∞ 
                    –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –≤—ñ–∑—É–∞–ª—å–Ω–∏–º–∏ –µ—Ñ–µ–∫—Ç–∞–º–∏.¬ª

                    –í—Ä–∞—Ö–æ–≤—É—é—á–∏ –≤–µ–ª–∏—á–µ–∑–Ω—É —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å —É —Å–º–∞–∫–∞—Ö —ñ –≤–ø–æ–¥–æ–±–∞–Ω–Ω—è—Ö, —á–∏ –Ω–µ –±—É–ª–æ –± –∫—Ä–∞—â–µ, —è–∫–±–∏ Netflix –º—ñ–≥ –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–∏–π —Ç–≤—ñ—Ä –º–∏—Å—Ç–µ—Ü—Ç–≤–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, 
                    —â–æ–± –ø—ñ–¥–∫—Ä–µ—Å–ª–∏—Ç–∏ –∞—Å–ø–µ–∫—Ç–∏ –Ω–∞–∑–≤–∏, —è–∫—ñ –º–∞—é—Ç—å –¥–ª—è –Ω–∏—Ö –æ—Å–æ–±–ª–∏–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è?

                    –†–æ–∑–≥–ª—è–Ω–µ–º–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏, –∫–æ–ª–∏ —Ä—ñ–∑–Ω—ñ —É—á–∞—Å–Ω–∏–∫–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é –ø–µ—Ä–µ–≥–ª—è–¥—É. –õ—ñ–≤–æ—Ä—É—á —Ç—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —è–∫—ñ —É—á–∞—Å–Ω–∏–∫ –ø–µ—Ä–µ–≥–ª—è–¥–∞–≤ —É –º–∏–Ω—É–ª–æ–º—É. –ü—Ä–∞–≤–æ—Ä—É—á –≤—ñ–¥ —Å—Ç—Ä—ñ–ª–∫–∏ 
                    –ø–æ–∫–∞–∑–∞–Ω–æ –æ–±–∫–ª–∞–¥–∏–Ω–∫—É, —è–∫—É —É—á–∞—Å–Ω–∏–∫ –æ—Ç—Ä–∏–º–∞–≤ –±–∏ –¥–ª—è –ø–µ–≤–Ω–æ–≥–æ —Ñ—ñ–ª—å–º—É –≤—ñ–¥—à—Ç–æ–≤—Ö—É—é—á–∏—Å—å –≤—ñ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó.

                    –ó–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤—ñ–¥–¥–∞—î –ø–µ—Ä–µ–≤–∞–≥—É —Ä—ñ–∑–Ω–∏–º –∂–∞–Ω—Ä–∞–º —ñ —Ç–µ–º–∞–º, –ª–µ–≥–∫–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ñ—ñ–ª—å–º—É ¬´–î–æ–±—Ä–∏–π –í—ñ–ª–ª –•–∞–Ω—Ç—ñ–Ω–≥¬ª. –ì–ª—è–¥–∞—á, —è–∫–∏–π –ø–µ—Ä–µ–≥–ª—è–¥–∞–≤ 
                    –±–∞–≥–∞—Ç–æ —Ä–æ–º–∞–Ω—Ç–∏—á–Ω–∏—Ö —Ñ—ñ–ª—å–º—ñ–≤, –º–æ–∂–µ –∑–∞—Ü—ñ–∫–∞–≤–∏—Ç–∏—Å—è ¬´–î–æ–±—Ä–∏–º –í—ñ–ª–ª–æ–º –•–∞–Ω—Ç—ñ–Ω–≥–æ–º¬ª, —è–∫—â–æ –Ω–∞ –æ–±–∫–ª–∞–¥–∏–Ω—Ü—ñ –±—É–¥–µ –∑–æ–±—Ä–∞–∂–µ–Ω–æ –∑–∞–∫–æ—Ö–∞–Ω–∏—Ö –ú–µ—Ç–∞ –î–µ–π–º–æ–Ω–∞ —Ç–∞ –ú—ñ–Ω—ñ –î—Ä–∞–π–≤–µ—Ä, —Ç–æ–¥—ñ —è–∫ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á, 
                    —è–∫–∏–π –¥–∏–≤–∏–≤—Å—è –±–∞–≥–∞—Ç–æ –∫–æ–º–µ–¥—ñ–π, –∑–∞—Ü—ñ–∫–∞–≤–∏—Ç—å—Å—è —Ñ—ñ–ª—å–º–æ–º, —è–∫—â–æ –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –†–æ–±—ñ–Ω–∞ –í—ñ–ª—å—è–º—Å–∞, –≤—ñ–¥–æ–º–æ–≥–æ –∫–æ–º—ñ–∫–∞.
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,2,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/netflix.4.png"), width=600)
                st.markdown("<p style='text-align: center;'><em>Movies Recommendations</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    –ó–∞ —ñ–Ω—à–∏–º —Å—Ü–µ–Ω–∞—Ä—ñ—î–º, —Ä—ñ–∑–Ω—ñ —É–ø–æ–¥–æ–±–∞–Ω–Ω—è —á–ª–µ–Ω—ñ–≤ –∞–∫—Ç–æ—Ä—Å—å–∫–æ–≥–æ —Å–∫–ª–∞–¥—É –º–æ–∂—É—Ç—å –≤–ø–ª–∏–Ω—É—Ç–∏ –Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—é —ñ–ª—é—Å—Ç—Ä–∞—Ü—ñ—ó –¥–æ —Ñ—ñ–ª—å–º—É ¬´–ö—Ä–∏–º—ñ–Ω–∞–ª—å–Ω–µ —á—Ç–∏–≤–æ¬ª. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á, 
                    —è–∫–∏–π –¥–∏–≤–∏—Ç—å—Å—è –±–∞–≥–∞—Ç–æ —Ñ—ñ–ª—å–º—ñ–≤ –∑–∞ —É—á–∞—Å—Ç—é –£–º–∏ –¢—É—Ä–º–∞–Ω, —à–≤–∏–¥—à–µ –∑–∞ –≤—Å–µ, –ø–æ–∑–∏—Ç–∏–≤–Ω–æ –≤—ñ–¥—Ä–µ–∞–≥—É—î –Ω–∞ –æ–±–∫–ª–∞–¥–∏–Ω–∫—É –¥–ª—è ¬´–ö—Ä–∏–º—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –ß—Ç–∏–≤–∞¬ª, —è–∫–∞ –º—ñ—Å—Ç–∏—Ç—å –£–º—É. 
                    –¢–∏–º —á–∞—Å–æ–º —à–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫ –î–∂–æ–Ω–∞ –¢—Ä–∞–≤–æ–ª—Ç–∏ –º–æ–∂–µ –±—É—Ç–∏ –±—ñ–ª—å—à –∑–∞—Ü—ñ–∫–∞–≤–ª–µ–Ω–∏–º —É –ø–µ—Ä–µ–≥–ª—è–¥—ñ —Ñ—ñ–ª—å–º—É, —è–∫—â–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –î–∂–æ–Ω.
                    """)
        with st.container():
            col1, col2, col3 = st.columns((1,2,1))
            with col1:
                st.empty()
            with col2:
                st.image(Image.open("images/netflix.5.png"), width=600)
                st.markdown("<p style='text-align: center;'><em>Movies Recommendations</em></p>", unsafe_allow_html=True)
            with col3:
                st.empty()
        st.markdown("""
                    **–õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞:**
                    
                    1.	Gomez-Uribe C. A., Hunt N. The Netflix Recommender System: Algorithms, Business Value, and Innovation. 2015. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ] URL: https://dl.acm.org/doi/10.1145/2843948 
                    2.	How Netflix Became a $100 Billion Company in 20 Years. Product Habits. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ] URL: https://producthabits.com/how-netflix-became-a-100-billion-company-in-20-years/
                    3.	Koren Y., Bell R., Volinsky C. Matrix Factorization Techniques for Recommender Systems. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ]. 2009. Vol. 42, no. 8. P. 30‚Äì37. URL: https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf
                    4.	Item-Based Collaborative Filtering in Python ‚Äì Predictive Hacks. Predictive Hacks. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ] URL: https://predictivehacks.com/item-based-collaborative-filtering-in-python/ 
                    5.	Blog N. T. Learning a Personalized Homepage. Medium. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ] URL: https://predictivehacks.com/item-based-collaborative-filtering-in-python/#google_vignette 
                    6.	Chandrashekar A. Amat F. Basilico J. Jebera T. Artwork Personalization at Netflix. Medium. [–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–µ –î–∂–µ—Ä–µ–ª–æ] URL: https://netflixtechblog.com/artwork-personalization-c589f074ad76
                    """)

    elif selected == "–ë–ª–æ–∫—á–µ–π–Ω —É –º–µ–¥–∏—Ü–∏–Ω—ñ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—Ü—ñ":
        st.subheader("–ë–ª–æ–∫—á–µ–π–Ω —É –º–µ–¥–∏—Ü–∏–Ω—ñ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—Ü—ñ")
        st.write("June 07, 2024 | [Article](https://www.lex-line.com.ua/?go=full_article&id=3877)")
        st.markdown("""
                    –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—è –±–ª–æ–∫—á–µ–π–Ω –ø—Ä–∏–≤–µ—Ä—Ç–∞—î –≤—Å–µ –±—ñ–ª—å—à–µ —É–≤–∞–≥–∏ –∑–∞–≤–¥—è–∫–∏ –∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ –∑–∞–±–µ–∑–ø–µ—á—É–≤–∞—Ç–∏ –ø—Ä–æ–∑–æ—Ä–µ —Ç–∞ –±–µ–∑–ø–µ—á–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —ñ –ø–µ—Ä–µ–¥–∞—á—É –¥–∞–Ω–∏—Ö. 
                    –•–æ—á–∞ –±–ª–æ–∫—á–µ–π–Ω –≤—ñ–¥–æ–º–∏–π —Å–≤–æ—î—é —Ä–æ–ª–ª—é —É —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ—è—Ö —ñ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞—Ö, –π–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª –≤–∏—Ö–æ–¥–∏—Ç—å –∑–∞ –º–µ–∂—ñ —Ü–∏—Ö —Å—Ñ–µ—Ä. 
                    –ú–µ–¥–∏—á–Ω–∞ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–Ω–∞ –≥–∞–ª—É–∑—ñ —Å—Ç–∏–∫–∞—é—Ç—å—Å—è –∑ –ø—ñ–¥—Ä–æ–±–∫–æ—é –ª—ñ–∫—ñ–≤, —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –ª–∞–Ω—Ü—é–≥—ñ–≤ –ø–æ—Å—Ç–∞–≤–æ–∫, –ø–æ—Ç—Ä–µ–±–æ—é —É –±–µ–∑–ø–µ—Ü—ñ —Ç–∞ –∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω–æ—Å—Ç—ñ 
                    –º–µ–¥–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –∞ —Ç–∞–∫–æ–∂ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ—Å—Ç—é –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –ø—Ä–æ–∑–æ—Ä–æ—Å—Ç—ñ. –ë–ª–æ–∫—á–µ–π–Ω –º–æ–∂–µ —Å—Ç–∞—Ç–∏ –∫–ª—é—á–æ–≤–∏–º —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è —Ü–∏—Ö –ø—Ä–æ–±–ª–µ–º.
                    
                    –ë–ª–æ–∫—á–µ–π–Ω (–≤—ñ–¥ –∞–Ω–≥–ª. ‚Äúblock‚Äù ‚Äì‚Äù–±–ª–æ–∫‚Äù, ‚Äúchain‚Äù ‚Äì ‚Äú–ª–∞–Ω—Ü—é–≥‚Äù) ‚Äî —Ü–µ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö, —è–∫–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è —á–∏—Å–ª–µ–Ω–Ω–∏–º–∏ –∫–æ–º–ø'—é—Ç–µ—Ä–∞–º–∏, —Ä–æ–∑–º—ñ—â–µ–Ω–∏–º–∏ 
                    –ø–æ –≤—Å—å–æ–º—É —Å–≤—ñ—Ç—É. –î–∞–Ω—ñ –≤ —Ü—ñ–π –±–∞–∑—ñ –æ—Ä–≥–∞–Ω—ñ–∑–æ–≤–∞–Ω—ñ –≤ –±–ª–æ–∫–∏, —è–∫—ñ —Ä–æ–∑—Ç–∞—à–æ–≤–∞–Ω—ñ –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥—ñ—á–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É —ñ –∑–∞—Ö–∏—â–µ–Ω—ñ –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ—ñ—î—é, –¥–µ –∫–æ–∂–µ–Ω –Ω–æ–≤–∏–π –±–ª–æ–∫ –º—ñ—Å—Ç–∏—Ç—å 
                    —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π —Ç–∞ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω—ñ –∑–∞ –ø–µ–≤–Ω–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –¥–∞–Ω—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –±–ª–æ–∫—É [1]. –¶–µ –∑–∞–±–µ–∑–ø–µ—á—É—î –≤–∏—Å–æ–∫–∏–π —Ä—ñ–≤–µ–Ω—å –±–µ–∑–ø–µ–∫–∏ —Ç–∞ –Ω–µ–∑–º—ñ–Ω–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö.
                    
                    –ë–ª–æ–∫—á–µ–π–Ω —î —Ü–∏—Ñ—Ä–æ–≤–∏–º —Ä–µ—î—Å—Ç—Ä–æ–º, —è–∫–∏–π –Ω–∞–¥—ñ–π–Ω–æ –∑–∞–ø–∏—Å—É—î —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –º—ñ–∂ —Å—Ç–æ—Ä–æ–Ω–∞–º–∏, –∑–∞—Ö–∏—â–∞—é—á–∏ —ó—Ö –≤—ñ–¥ –Ω–µ—Å–∞–Ω–∫—Ü—ñ–æ–Ω–æ–≤–∞–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É. –î–∞–Ω—ñ –∑–∞–ø–∏—Å—É—é—Ç—å—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ—é —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ—é
                    –º–µ—Ä–µ–∂–µ—é —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö –∫–æ–º–ø'—é—Ç–µ—Ä—ñ–≤ (–Ω–æ–¥). –ü—Ä–∏ —ñ–Ω—ñ—Ü—ñ—é–≤–∞–Ω–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –ø–µ—Ä–µ–∫–∞–∑—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏, –≤–æ–Ω–∞ —Ç—Ä–∞–Ω—Å–ª—é—î—Ç—å—Å—è –≤ –º–µ—Ä–µ–∂—É, –¥–µ –Ω–æ–¥–∏ –ø—Ä–æ–≤–æ–¥—è—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é, 
                    –ø–µ—Ä–µ–≤—ñ—Ä—è—é—á–∏ —Ü–∏—Ñ—Ä–æ–≤—ñ –ø—ñ–¥–ø–∏—Å–∏ —Ç–∞ —ñ–Ω—à—ñ –¥–∞–Ω—ñ. –ü—ñ—Å–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è –¥–æ–¥–∞—î—Ç—å—Å—è –¥–æ –±–ª–æ–∫—É, —è–∫–∏–π –ø–æ—î–¥–Ω—É—î—Ç—å—Å—è –∑ —ñ–Ω—à–∏–º–∏ –≤ –±–ª–æ–∫—á–µ–π–Ω. –ó–∞–≤–¥—è–∫–∏ –±–µ–∑–ø–µ—Ü—ñ, –ø—Ä–æ–∑–æ—Ä–æ—Å—Ç—ñ —Ç–∞ 
                    –Ω–µ–∑–º—ñ–Ω–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö, –±–ª–æ–∫—á–µ–π–Ω –ø—ñ–¥–≤–∏—â—É—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —ñ –Ω–∞–¥—ñ–π–Ω—ñ—Å—Ç—å —É –º–µ–¥–∏—Ü–∏–Ω—ñ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—Ü—ñ. 
                    
                    –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω-—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —É –º–µ–¥–∏—Ü–∏–Ω—ñ –∑–Ω–∞—á–Ω–æ –∑—Ä–æ—Å–ª–æ –ø—Ä–æ—Ç—è–≥–æ–º –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ä–æ–∫—ñ–≤. –£ 2017 —Ä–æ—Ü—ñ –≤—ñ–¥—Å–æ—Ç–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ —Å—Ç–∞–Ω–æ–≤–∏–≤ –ª–∏—à–µ 6%. 
                    –ü—Ä–æ—Ç–µ –≤–∂–µ –≤ 2018 —Ä–æ—Ü—ñ —Ü–µ–π –ø–æ–∫–∞–∑–Ω–∏–∫ –∑—Ä—ñ—Å –¥–æ 18%, –∞ —É 2019 —Ä–æ—Ü—ñ –¥–æ—Å—è–≥–Ω—É–≤ 29%. –ù–∞–π–±—ñ–ª—å—à–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –≤—ñ–¥–±—É–ª–æ—Å—è —É 2020 —Ä–æ—Ü—ñ, –∫–æ–ª–∏ –≤—ñ–¥—Å–æ—Ç–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –≤ –º–µ–¥–∏—Ü–∏–Ω—ñ –¥–æ—Å—è–≥–Ω—É–≤ 46%.[2]
                    
                    **–£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏ –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤** —î –æ–¥–Ω–∏–º –∑ –Ω–∞–π–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å –±–ª–æ–∫—á–µ–π–Ω—É –≤ –æ—Ö–æ—Ä–æ–Ω—ñ –∑–¥–æ—Ä–æ–≤'—è. –®–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –∑–∞–±–µ–∑–ø–µ—á—É—î –±–µ–∑–ø–µ—á–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –º–µ–¥–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤, 
                    –Ω–æ–º–µ—Ä—ñ–≤ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è —Ç–∞ –±–∞–Ω–∫—ñ–≤—Å—å–∫–∏—Ö —Ä–µ–∫–≤—ñ–∑–∏—Ç—ñ–≤. –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π —Ä–µ—î—Å—Ç—Ä –≥–∞—Ä–∞–Ω—Ç—É—î –±–µ–∑–ø–µ—á–Ω—É –ø–µ—Ä–µ–¥–∞—á—É –¥–∞–Ω–∏—Ö, –∑–∞–ø–æ–±—ñ–≥–∞—é—á–∏ –Ω–µ—Å–∞–Ω–∫—Ü—ñ–æ–Ω–æ–≤–∞–Ω–æ–º—É –¥–æ—Å—Ç—É–ø—É. –ë–ª–æ–∫—á–µ–π–Ω 
                    –∑–∞–±–µ–∑–ø–µ—á—É—î –Ω–µ–∑–º—ñ–Ω–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö, —Ç–æ–±—Ç–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –º–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏ –±–µ–∑ –∑–≥–æ–¥–∏ –º–µ—Ä–µ–∂—ñ, —â–æ –≥–∞—Ä–∞–Ω—Ç—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å —ñ –¥–æ—Å—Ç–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤ –∑ —á–∞—Å–æ–º.
                    
                    **–ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∑–∞–ø–∏—Å—ñ–≤ –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω—å.** –î–æ—Å–ª—ñ–¥–Ω–∏–∫–∏ —á–∞—Å—Ç–æ –º–∞—é—Ç—å –∑–∞–ø–∏—Å—É–≤–∞—Ç–∏ –≤–µ–ª–∏–∫—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –ø—ñ–¥ —á–∞—Å –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω—å –Ω–æ–≤–∏—Ö –ª—ñ–∫—ñ–≤ –∞–±–æ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è 
                    –Ω–æ–≤–∏—Ö –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω—å. –§—É–Ω–∫—Ü—ñ—è –Ω–µ–∑–º—ñ–Ω–Ω–æ—Å—Ç—ñ –±–ª–æ–∫—á–µ–π–Ω—É –∑–∞–±–µ–∑–ø–µ—á—É—î —Ü—ñ–ª—ñ—Å–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω—å, –∑–∞–ø–æ–±—ñ–≥–∞—é—á–∏ —ó—Ö –ø—ñ–¥—Ä–æ–±—Ü—ñ –∞–±–æ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—è–º, –¥–æ–ø–æ–º–∞–≥–∞—î —Ñ—ñ–∫—Å—É–≤–∞—Ç–∏ –∫–æ–∂–Ω—É –∑–º—ñ–Ω—É 
                    –≤ –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω–Ω—è—Ö —ñ –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ –¥–µ—Ç–∞–ª—ñ –≤–Ω–µ—Å–µ–Ω–∏—Ö –∑–º—ñ–Ω. –ö—Ä—ñ–º —Ç–æ–≥–æ, –º–µ—Ö–∞–Ω—ñ–∑–º –∫–æ–Ω—Å–µ–Ω—Å—É—Å—É –±–ª–æ–∫—á–µ–π–Ω—É –∑–∞–±–µ–∑–ø–µ—á—É—î, —â–æ –±—É–¥—å-—è–∫—ñ –∑–º—ñ–Ω–∏ —É –∑–∞–ø–∏—Å–∞—Ö –∫–ª—ñ–Ω—ñ—á–Ω–∏—Ö –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω—å 
                    –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∑–≥–æ–¥–∏ –∫—ñ–ª—å–∫–æ—Ö —Å—Ç–æ—Ä—ñ–Ω, –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—é—á–∏ –∫–æ–∂–Ω—É –º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—é —Ç–∞ –ø–æ–∑–Ω–∞—á–∞—é—á–∏ —ó—ó —è–∫ –¥–æ—Å—Ç–æ–≤—ñ—Ä–Ω—É, —â–æ —Å–ø—Ä–∏—è—î –¥–æ–≤—ñ—Ä—ñ —Ç–∞ –Ω–∞–¥—ñ–π–Ω–æ—Å—Ç—ñ. 
                    
                    **–£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ª–∞–Ω—Ü—é–≥–æ–º –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ü—ñ—ó.** –§–∞–ª—å—à–∏–≤—ñ –ª—ñ–∫–∏ —î –∑–Ω–∞—á–Ω–æ—é –ø—Ä–æ–±–ª–µ–º–æ—é –≤ —Å–∏—Å—Ç–µ–º—ñ –æ—Ö–æ—Ä–æ–Ω–∏ –∑–¥–æ—Ä–æ–≤'—è, —è–∫–∞ –Ω–µ —Ç—ñ–ª—å–∫–∏ –ø–æ—à–∏—Ä–µ–Ω–∞, –∞–ª–µ –π –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–±–µ–∑–ø–µ—á–Ω–æ—é. 
                    –ü—ñ–¥—Ä–æ–±–ª–µ–Ω—ñ –ª—ñ–∫–∏ –º–æ–∂—É—Ç—å –º—ñ—Å—Ç–∏—Ç–∏ —Ç–æ–∫—Å–∏—á–Ω—ñ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∏, —â–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –∑–¥–æ—Ä–æ–≤'—è –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤. –Ü–Ω–æ–¥—ñ –Ω–µ–ª–µ–≥–∞–ª—å–Ω—ñ –ª—ñ–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ —É–ø–∞–∫–æ–≤—É—é—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è —Ç–µ—Ä–º—ñ–Ω—É –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ 
                    –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø—ñ–¥—Ä–æ–±–ª–µ–Ω–∏—Ö –µ—Ç–∏–∫–µ—Ç–æ–∫. –ó–∞ –¥–∞–Ω–∏–º–∏ –ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—ó —Ä–∞–¥–∏ –∑ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –∑–ª–æ—á–∏–Ω–Ω–æ—Å—Ç—ñ, –ø–æ–Ω–∞–¥ 10% –ª—ñ–∫—ñ–≤ —É –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –ª–∞–Ω—Ü—é–≥—É –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è —î –ø—ñ–¥—Ä–æ–±–ª–µ–Ω–∏–º–∏. 
                    –£ –±–∞–≥–∞—Ç—å–æ—Ö –∫—Ä–∞—ó–Ω–∞—Ö —Ñ–∞–ª—å—à–∏–≤—ñ –ª—ñ–∫–∏ —Å—Ç–∞–Ω–æ–≤–ª—è—Ç—å –¥–æ 70% —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ü—ñ—ó –≤ –ª–∞–Ω—Ü—é–≥—É –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è. 
                    
                    –í–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –¥–∞—Å—Ç—å –∑–º–æ–≥—É –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ —à–ª—è—Ö –ª—ñ–∫—ñ–≤ –≤—ñ–¥ –∑–∞–≤–æ–¥—É –¥–æ –∞–ø—Ç–µ–∫–∏, –≤–∫–ª—é—á–Ω–æ –∑ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—î—é –∑ –≤–∏—Ä–æ–±–Ω–∏–∫–∞–º–∏ —Ç–∞ –≥—É—Ä—Ç–æ–≤–∏–º–∏ –ø—Ä–æ–¥–∞–∂–∞–º–∏, —â–æ –ø–æ–ª–µ–≥—à–∏—Ç—å –≤–∏—è–≤–ª–µ–Ω–Ω—è —Ç–∞ 
                    –≤–∏–ª—É—á–µ–Ω–Ω—è –ø—ñ–¥—Ä–æ–±–ª–µ–Ω–∏—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤. –ö—Ä—ñ–º —Ç–æ–≥–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –º–æ–∂–µ –¥–æ–ø–æ–º–æ–≥—Ç–∏ –≤ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –∂–∏—Ç—Ç—î–≤–æ–≥–æ —Ü–∏–∫–ª—É –ª—ñ–∫—ñ–≤, –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—ñ –ø–æ–ø–∏—Ç—É —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ —Ü—å–æ–≥–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ–π.
                    
                    **–û–±–º—ñ–Ω –º–µ–¥–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏.** –ü–∞—Ü—ñ—î–Ω—Ç–∞–º —á–∞—Å—Ç–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è –∫—ñ–ª—å–∫–æ—Ö —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç—ñ–≤ —Ç–∞ –¥–æ—Å—Ç—É–ø –¥–æ –ø–æ–≤–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó —Å–≤–æ–≥–æ –∑–¥–æ—Ä–æ–≤'—è. –û–¥–Ω–∞–∫ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –æ–±–º—ñ–Ω –º–µ–¥–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏ 
                    –º—ñ–∂ –ª—ñ–∫–∞—Ä—è–º–∏ —î –∑–Ω–∞—á–Ω–æ—é –ø—Ä–æ–±–ª–µ–º–æ—é. –ë–ª–æ–∫—á–µ–π–Ω –¥–æ–ø–æ–º–∞–≥–∞—î —É —Ü—å–æ–º—É, —É—Å—É–≤–∞—é—á–∏ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –Ω–µ—Å—É–º—ñ—Å–Ω—ñ—Å—Ç—é —Ñ–æ—Ä–º–∞—Ç—ñ–≤, –∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω—ñ—Å—Ç—é —Ç–∞ –±–µ–∑–ø–µ–∫–æ—é. –î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–æ–∑–≤–æ–ª—è—î 
                    –±–∞–≥–∞—Ç—å–æ–º —É—á–∞—Å–Ω–∏–∫–∞–º –º–∞—Ç–∏ –¥–æ—Å—Ç—É–ø –¥–æ –æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö, —â–æ –ø–æ–ª–µ–≥—à—É—î —Å–ø—ñ–ª—å–Ω—É —Ä–æ–±–æ—Ç—É –ª—ñ–∫–∞—Ä—ñ–≤ —Ç–∞ –Ω–∞–¥–∞—î –ø–∞—Ü—ñ—î–Ω—Ç–∞–º –¥–æ—Å—Ç—É–ø –¥–æ –ø–æ–≤–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó —ó—Ö –∑–¥–æ—Ä–æ–≤'—è.
                    
                    **–í—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ–π –º–µ–¥–∏—á–Ω–∏—Ö –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤.** –ö–æ–ª–∏ –ø–∞—Ü—ñ—î–Ω—Ç–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤—ñ–¥–≤—ñ–¥–∞—Ç–∏ –ª—ñ–∫–∞—Ä—è, –≤–æ–Ω–∏ —Ö–æ—á—É—Ç—å –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –π–æ–≥–æ –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ—ó –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å–æ–º. 
                    –ë–ª–æ–∫—á–µ–π–Ω –∑–∞–±–µ–∑–ø–µ—á—É—î –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–∞ –Ω–µ–∑–º—ñ–Ω–Ω–∏–π —Ä–µ—î—Å—Ç—Ä –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ–π –ª—ñ–∫–∞—Ä—ñ–≤, —É–Ω–µ–º–æ–∂–ª–∏–≤–ª—é—é—á–∏ –Ω–µ—Å–∞–Ω–∫—Ü—ñ–æ–Ω–æ–≤–∞–Ω—ñ –∑–º—ñ–Ω–∏. –¶–µ –¥–æ–ø–æ–º–∞–≥–∞—î –ø–∞—Ü—ñ—î–Ω—Ç–∞–º –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ—ó 
                    –ø–µ—Ä–µ–¥ –≤—ñ–∑–∏—Ç–æ–º –¥–æ –ª—ñ–∫–∞—Ä—è, –∞ –ª—ñ–∫–∞—Ä–Ω—è–º —É –Ω–∞–π–º—ñ –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–∏—Ö –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —Ç–∞ –¥–æ—Å—Ç–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –≤—Å—ñ—Ö –∑–∞—Ü—ñ–∫–∞–≤–ª–µ–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω.
                    
                    **–ü–ª–∞—Ç–µ–∂—ñ –≤ –æ—Ö–æ—Ä–æ–Ω—ñ –∑–¥–æ—Ä–æ–≤'—è.** –õ—ñ–∫–∞—Ä–Ω—ñ, –∫–ª—ñ–Ω—ñ–∫–∏, –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ñ —Ü–µ–Ω—Ç—Ä–∏ —Ç–∞ —Å—Ç—Ä–∞—Ö–æ–≤—ñ –∫–æ–º–ø–∞–Ω—ñ—ó —â–æ–¥–Ω—è –æ–±—Ä–æ–±–ª—è—é—Ç—å –±–µ–∑–ª—ñ—á —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π —ñ –∑–∞—è–≤–æ–∫, –≤–∫–ª—é—á–∞—é—á–∏ –æ–±–º—ñ–Ω –∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, 
                    –∞ —Ç–æ–º—É –∑–∞—Ö–∏—Å—Ç –ø–ª–∞—Ç—ñ–∂–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Å—Ç–∞–≤ —â–µ –±—ñ–ª—å—à –≤–∞–∂–ª–∏–≤–∏–º. –ë–ª–æ–∫—á–µ–π–Ω –∑–∞–±–µ–∑–ø–µ—á—É—î –±–µ–∑–ø–µ—á–Ω–µ —ñ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏–º–∏ –æ–ø–µ—Ä–∞—Ü—ñ—è–º–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ—ñ—é –¥–ª—è 
                    –∑–∞—Ö–∏—Å—Ç—É –¥–∞–Ω–∏—Ö —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—ñ–π–∫–æ–≥–æ –¥–æ –∑–ª–æ–º—É –ª–∞–Ω—Ü—é–≥–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π. –ö—Ä—ñ–º —Ç–æ–≥–æ, –≤—ñ–Ω –æ–ø—Ç–∏–º—ñ–∑–æ–≤—É—î –ø–ª–∞—Ç–µ–∂—ñ, —É—Å—É–≤–∞—é—á–∏ –ø–æ—Å–µ—Ä–µ–¥–Ω–∏–∫—ñ–≤, —â–æ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ —à–≤–∏–¥—à–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π —ñ –∑–Ω–∏–∂–µ–Ω–Ω—è –≤–∏—Ç—Ä–∞—Ç. 
                    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—É—î, —â–æ 10% —Å—Ç—Ä–∞—Ö–æ–≤–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤ –∑–∞–ø–µ—Ä–µ—á—É—é—Ç—å—Å—è, –∞ 17% –≤–∏–º–æ–≥ –≤—ñ–¥—Ö–∏–ª—è—é—Ç—å—Å—è —á–µ—Ä–µ–∑ –Ω–µ–ø–æ–≤–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—é —Ç–æ—â–æ [3]. 
                    –¢–æ–∂ –∫–æ–ª–∏ –ø–∞—Ü—ñ—î–Ω—Ç –≤—ñ–¥–≤—ñ–¥—É—î –ª—ñ–∫–∞—Ä—è, –ø–æ–¥—ñ—è –∑–∞–ø–∏—Å—É—î—Ç—å—Å—è –≤ —Ä–µ—î—Å—Ç—Ä –±–ª–æ–∫—á–µ–π–Ω—É, –∞ —Å—Ç—Ä–∞—Ö–æ–≤–∞ –æ—Ç—Ä–∏–º—É—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —â–æ –≤–∏—Ä—ñ—à—É—î –≤–∏–Ω–∏–∫–Ω–µ–Ω–Ω—è –±—É–¥—å-—è–∫–∏—Ö —Å–ø–æ—Ä—ñ–≤.  
                    
                    **–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ñ –º–µ–¥–∏—á–Ω—ñ –∫–∞—Ä—Ç–∏.** –£ 2016 —Ä–æ—Ü—ñ –£–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç –î–∂–æ–Ω–∞ –•–æ–ø–∫—ñ–Ω—Å–∞ –æ–ø—É–±–ª—ñ–∫—É–≤–∞–≤ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è, —Ç—Ä–µ—Ç—å–æ—é –∑–∞ –∑–Ω–∞—á—É—â—ñ—Å—Ç—é –ø—Ä–∏—á–∏–Ω–æ—é —Å–º–µ—Ä—Ç—ñ –≤ –°–®–ê —Å—Ç–∞–ª–∏ –ø–æ–º–∏–ª–∫–∏, 
                    –¥–æ–ø—É—â–µ–Ω—ñ –≤–Ω–∞—Å–ª—ñ–¥–æ–∫ —É–ø—É—â–µ–Ω—å –≤ —ñ—Å—Ç–æ—Ä—ñ—ó —Ö–≤–æ—Ä–æ–±–∏. –ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ñ –º–µ–¥–∏—á–Ω—ñ –∫–∞—Ä—Ç–∏ –Ω–∞ –±–∞–∑—ñ –±–ª–æ–∫—á–µ–π–Ω –º–æ–∂—É—Ç—å –ª–µ–≥–∫–æ —Ä–æ–∑–≤‚Äô—è–∑–∞—Ç–∏ —Ü—é –ø—Ä–æ–±–ª–µ–º—É. –ö–æ–∂–µ–Ω –∑–∞–ø–∏—Å —É —Å–∏—Å—Ç–µ–º—ñ ‚Äî –∑–∞–ø–∏—Å–∫–∞ –ª—ñ–∫–∞—Ä—è, 
                    —Ä–µ—Ü–µ–ø—Ç —á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—ñ–≤ ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ —É–Ω—ñ–∫–∞–ª—å–Ω—É —Ö–µ—à-—Ñ—É–Ω–∫—Ü—ñ—é, —è–∫—É –º–æ–∂–Ω–∞ —Ä–æ–∑—à–∏—Ñ—Ä—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –∑ –¥–æ–∑–≤–æ–ª—É –ø–∞—Ü—ñ—î–Ω—Ç–∞. –ö–æ–∂–Ω–∞ –ø–æ–ø—Ä–∞–≤–∫–∞ –∞–±–æ –ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–∏—Ö —Ä–µ—î—Å—Ç—Ä—É—î—Ç—å—Å—è —è–∫ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è. 
                    –¶–µ –∑–∞–±–µ–∑–ø–µ—á—É—î –∑—Ä—É—á–Ω—ñ—Å—Ç—å –¥–ª—è –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤ —ñ –º–µ–¥–∏—á–Ω–∏—Ö –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤, —Å–ø—Ä–æ—â—É—î –≤–∑–∞—î–º–æ–¥—ñ—é –∑—ñ —Å—Ç—Ä–∞—Ö–æ–≤–∏–º–∏ –∫–æ–º–ø–∞–Ω—ñ—è–º–∏ —Ç–∞ –¥–∞—î –ø–∞—Ü—ñ—î–Ω—Ç–∞–º –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Å–≤–æ—ó–º–∏ –¥–∞–Ω–∏–º–∏. 
                    
                    Bisresearch –ø—Ä–æ–≥–Ω–æ–∑—É—î, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω—É –≤ –æ—Ö–æ—Ä–æ–Ω—ñ –∑–¥–æ—Ä–æ–≤'—è –ø—ñ–¥–≤–∏—â–∏—Ç—å —Ä—ñ–≤–µ–Ω—å –¥–æ–≤—ñ—Ä–∏ —Ç–∞ –¥–æ–∑–≤–æ–ª–∏—Ç—å –∑–∞–æ—â–∞–¥–∏—Ç–∏ –¥–æ 100 –º—ñ–ª—å—è—Ä–¥—ñ–≤ –¥–æ–ª–∞—Ä—ñ–≤ –¥–æ 2025 —Ä–æ–∫—É –∑–∞–≤–¥—è–∫–∏ –∑–Ω–∏–∂–µ–Ω–Ω—é –≤–∏—Ç—Ä–∞—Ç 
                    —ñ –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—é —à–∞—Ö—Ä–∞–π—Å—Ç–≤—É. –û—Ç–∂–µ, —É –Ω–∞–π–±–ª–∏–∂—á—ñ –∫—ñ–ª—å–∫–∞ —Ä–æ–∫—ñ–≤ –æ—á—ñ–∫—É—î—Ç—å—Å—è —â–µ –∞–∫—Ç–∏–≤–Ω—ñ—à–µ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –±–ª–æ–∫—á–µ–π–Ω-—Ä—ñ—à–µ–Ω—å —É –º–µ–¥–∏—Ü–∏–Ω—É.

                    –õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∞:

                    1.	Binance Academy | –©–æ —Ç–∞–∫–µ –±–ª–æ–∫—á–µ–π–Ω —ñ —è–∫ –≤—ñ–Ω –ø—Ä–∞—Ü—é—î?. URL: https://academy.binance.com/uk/articles/what-is-blockchain-and-how-does-it-work (2024, May 31)
                    2.	PubMed Central (PMC) | Blockchain technology in the pharmaceutical industry: a systematic review. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9137953/ (2024, May 31)
                    3.	WhiteBIT Blog | –ë–ª–æ–∫—á–µ–π–Ω —É –º–µ–¥–∏—Ü–∏–Ω—ñ —Ç–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—Ü—ñ: –æ–≥–ª—è–¥ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏. URL: https://blog.whitebit.com/uk/blockchain-in-medicine/ (2024, May 31)
                    """)
    
    elif selected == "–°–º–∞—Ä—Ç-–ø–æ–ª–∏—á–∫–∏: —è–∫ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Å—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤":
        st.subheader("–°–º–∞—Ä—Ç-–ø–æ–ª–∏—á–∫–∏: —è–∫ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Å—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤")
        st.write("Nov 07-08, 2024 | [Article](http://www.wayscience.com/wp-content/uploads/2024/11/Conference-Proceedings-November-7-8-2024.pdf#page=298)")
        st.markdown("""
                    –£ —Å—É—á–∞—Å–Ω–∏—Ö —É–º–æ–≤–∞—Ö —Ä–æ–∑–¥—Ä—ñ–±–Ω–∞ —Ç–æ—Ä–≥—ñ–≤–ª—è –∑–∞–∑–Ω–∞—î –∑–Ω–∞—á–Ω–∏—Ö –∑–º—ñ–Ω –∑–∞–≤–¥—è–∫–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—é —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π. –°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç–∏ —Ç–∞ —Ä—ñ—Ç–µ–π–ª–µ—Ä–∏ –æ—Ç—Ä–∏–º—É—é—Ç—å –Ω–æ–≤—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –ø–æ–ª—ñ–ø—à–µ–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç—Å—å–∫–æ–≥–æ –¥–æ—Å–≤—ñ–¥—É, 
                    –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø—Ä–æ–¥–∞–∂—ñ–≤ —ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ—Ö –ø—Ä–æ—Ü–µ—Å—ñ–≤. –í–∞–∂–ª–∏–≤–∏–º —á–∏–Ω–Ω–∏–∫–æ–º —É—Å–ø—ñ—Ö—É —Ä—ñ—Ç–µ–π–ª—É —î –≥—Ä–∞–º–æ—Ç–Ω–µ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏, —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É (–®–Ü), —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É 
                    —Ä–µ—á–µ–π (IoT) —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö (Big Data) –¥–æ–∑–≤–æ–ª—è—î –∫–æ–º–ø–∞–Ω—ñ—è–º –∫—Ä–∞—â–µ —Ä–æ–∑—É–º—ñ—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –ø–æ–∫—É–ø—Ü—ñ–≤ —ñ –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –¥–ª—è –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø—Ä–∏–±—É—Ç–∫—ñ–≤. 
                    
                    –£ —Ü—ñ–π —Ä–æ–±–æ—Ç—ñ –±—É–¥—É—Ç—å —Ä–æ–∑–≥–ª—è–Ω—É—Ç—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ —Ç–æ–≥–æ, —è–∫ –ª—ñ–¥–µ—Ä–∏ —Ä–∏–Ω–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Å—É—á–∞—Å–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Å–≤–æ—ó—Ö –±—ñ–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—ñ–≤. –ë—É–¥–µ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ, —è–∫ —Ü—ñ –∫–æ–º–ø–∞–Ω—ñ—ó –≤–ø—Ä–æ–≤–∞–¥–∂—É—é—Ç—å —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω—ñ 
                    —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤–∏–∫–ª–∞–¥–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤, –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è –≤–∏–≤—á–µ–Ω–Ω—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å IoT –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∑–∞–ø–∞—Å–∞–º–∏ —Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑—É—é—Ç—å —Å–≤–æ—ó –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –ø–æ—Ç—Ä–µ–± –∫–ª—ñ—î–Ω—Ç—ñ–≤. 
                    
                    **–ê–Ω–∞–ª—ñ—Ç–∏–∫–∞** —Å—Ç–∞–ª–∞ –∫–ª—é—á–æ–≤–∏–º –µ–ª–µ–º–µ–Ω—Ç–æ–º —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —É —Ä–æ–∑–¥—Ä—ñ–±–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ, –Ω–∞–¥–∞—é—á–∏ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≥–ª–∏–±–æ–∫–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤, –≤–∏–∑–Ω–∞—á–∞—Ç–∏ —Ç—Ä–µ–Ω–¥–∏ —Ç–∞ –∞–¥–∞–ø—Ç—É–≤–∞—Ç–∏ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó –¥–æ –≤–∏–º–æ–≥ —Ä–∏–Ω–∫—É. 
                    –í–∞–∂–ª–∏–≤–∏–º –ø—Ä–∏–∫–ª–∞–¥–æ–º —î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –∫–æ–º–ø–∞–Ω—ñ—î—é **Target**, —è–∫–∞ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î —Å–∫–ª–∞–¥–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –ø–æ–∫—É–ø—Ü—ñ–≤. –ó—ñ–±—Ä–∞–Ω—ñ –¥–∞–Ω—ñ –ø—Ä–æ –ø–æ–∫—É–ø–∫–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–≤–∏—á–∫–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤ —ñ –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ —ó—Ö–Ω—ñ –ø–æ—Ç—Ä–µ–±–∏.
                    
                    –û–¥–Ω–∏–º —ñ–∑ –Ω–∞–π–≤—ñ–¥–æ–º—ñ—à–∏—Ö –ø—ñ–¥—Ö–æ–¥—ñ–≤ —î —Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ—Å—Ç—å –Ω–∞ –º–æ–ª–æ–¥–∏—Ö –º–∞—Ç–µ—Ä—ñ–≤. –ó–∞–≤–¥—è–∫–∏ –∞–Ω–∞–ª—ñ–∑—É –ø–æ–∫—É–ø–æ–∫, —Ç–∞–∫–∏—Ö —è–∫ –±–µ–∑—Ä–µ—Ü–µ–ø—Ç—É—Ä–Ω—ñ –ª—ñ–∫–∏, –ø–µ–≤–Ω—ñ –≤—ñ—Ç–∞–º—ñ–Ω–∏ —Ç–∞ —ñ–Ω—à—ñ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ç–æ–≤–∞—Ä–∏, –∫–æ–º–ø–∞–Ω—ñ—è –∑–º–æ–≥–ª–∞ –≤–∏—è–≤–∏—Ç–∏ —Ä–∞–Ω–Ω—ñ –æ–∑–Ω–∞–∫–∏ –≤–∞–≥—ñ—Ç–Ω–æ—Å—Ç—ñ —É —Å–≤–æ—ó—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤. 
                    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ü—ñ –¥–∞–Ω—ñ, **Target** —Ä–æ–∑—Ä–æ–±–∏–ª–∞ —Å–∏—Å—Ç–µ–º—É, —è–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –Ω–∞–¥—Å–∏–ª–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —Ä–µ–∫–ª–∞–º–Ω—ñ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∫—É–ø–æ–Ω–∏ –Ω–∞ –¥–∏—Ç—è—á—ñ —Ç–æ–≤–∞—Ä–∏ –∞–±–æ —Ç–æ–≤–∞—Ä–∏ –¥–ª—è –Ω–æ–≤–æ–Ω–∞—Ä–æ–¥–∂–µ–Ω–∏—Ö. –¶–µ –¥–æ–∑–≤–æ–ª–∏–ª–æ –Ω–µ —Ç—ñ–ª—å–∫–∏ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ —Ä—ñ–≤–µ–Ω—å –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó, 
                    –∞ –π —É—Ç—Ä–∏–º–∞—Ç–∏ –ª–æ—è–ª—å–Ω—ñ—Å—Ç—å –∫–ª—ñ—î–Ω—Ç—ñ–≤. [1]
                    
                    –ù–∞—Å—Ç—É–ø–Ω–∏–º –≤–∞–∂–ª–∏–≤–∏–º —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –º–æ–∂–Ω–∞ –≤–≤–∞–∂–∞—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è **—à—Ç—É—á–Ω–æ–≥–æ  —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É (–®–Ü)** –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –ø–æ–ø–∏—Ç—É —Ç–∞ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è. –û–¥–∏–Ω —ñ–∑ –ø—Ä–æ–≤—ñ–¥–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –π–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î –º–µ—Ä–µ–∂–∞ 
                    —Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç—ñ–≤ **Walmart**. –ó–∞–≤–¥—è–∫–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—é –®–Ü **Walmart** –º–æ–∂–µ –∑–¥—ñ–π—Å–Ω—é–≤–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑ –≤–µ–ª–∏—á–µ–∑–Ω–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö –ø—Ä–æ –ø—Ä–æ–¥–∞–∂—ñ, —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—é —Ç–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –ø–æ–∫—É–ø—Ü—ñ–≤, —â–æ –¥–æ–∑–≤–æ–ª—è—î –±—ñ–ª—å—à —Ç–æ—á–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏ –ø–æ–ø–∏—Ç –Ω–∞ —Ç–æ–≤–∞—Ä–∏.
                    
                    –¶—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ, —è–∫–µ –≤—Ä–∞—Ö–æ–≤—É—î —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ, —Å–µ–∑–æ–Ω–Ω—ñ –∫–æ–ª–∏–≤–∞–Ω–Ω—è —Ç–∞ –ª–æ–∫–∞–ª—å–Ω—ñ —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—ó. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –ø—ñ–¥ —á–∞—Å —Å–≤—è—Ç –∞–±–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö –∞–∫—Ü—ñ–π **Walmart** –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –®–Ü –º–æ–∂–µ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–æ–ø–∏—Ç—É –Ω–∞ –ø–µ–≤–Ω—ñ —Ç–æ–≤–∞—Ä–∏, 
                    —è–∫-–æ—Ç –ø—Ä–æ–¥—É–∫—Ç–∏ —Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è, –µ–ª–µ–∫—Ç—Ä–æ–Ω—ñ–∫—É —á–∏ —ñ–≥—Ä–∞—à–∫–∏. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –∑–±—ñ–ª—å—à–∏—Ç–∏ –æ–±—Å—è–≥–∏ –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è —Ü–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤, –∑–º–µ–Ω—à–∏—Ç–∏ –≤–∏—Ç—Ä–∞—Ç–∏ –Ω–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ —É–Ω–∏–∫–Ω—É—Ç–∏ –¥–µ—Ñ—ñ—Ü–∏—Ç—É —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö, —â–æ –æ—Å–æ–±–ª–∏–≤–æ –≤–∞–∂–ª–∏–≤–æ –ø—ñ–¥ —á–∞—Å –ø—ñ–∫–æ–≤–∏—Ö –ø–µ—Ä—ñ–æ–¥—ñ–≤. 
                    –ö—Ä—ñ–º —Ç–æ–≥–æ, **Walmart** –∞–∫—Ç–∏–≤–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —á–∞—Ç-–±–æ—Ç–∏, —è–∫—ñ –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞ –±–∞–∑—ñ –®–Ü, –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç—ñ–≤. –¶—ñ –±–æ—Ç–∏ –Ω–∞–¥–∞—é—Ç—å –ø–æ–∫—É–ø—Ü—è–º –º–∏—Ç—Ç—î–≤—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–æ—à–∏—Ä–µ–Ω—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è, –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –∑ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—è–º –∑–∞–º–æ–≤–ª–µ–Ω—å, –Ω–∞–¥–∞—é—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é 
                    –ø—Ä–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤ —Ç–∞ —Å—Ç–∞—Ç—É—Å –¥–æ—Å—Ç–∞–≤–∫–∏. [2]
                    
                    **–Ü–Ω—Ç–µ—Ä–Ω–µ—Ç —Ä–µ—á–µ–π (IoT)** –¥–æ–∑–≤–æ–ª—è—î —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –¥–µ —Ä—ñ–∑–Ω—ñ –ø—Ä–∏—Å—Ç—Ä–æ—ó, —è–∫-–æ—Ç –¥–∞—Ç—á–∏–∫–∏, RFID-–º—ñ—Ç–∫–∏ —Ç–∞ –∫–∞–º–µ—Ä–∏, –ø—ñ–¥–∫–ª—é—á–µ–Ω—ñ –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É —ñ –∑–¥–∞—Ç–Ω—ñ –∑–±–∏—Ä–∞—Ç–∏ —Ç–∞ –æ–±–º—ñ–Ω—é–≤–∞—Ç–∏—Å—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –≤ —Ä–µ–∂–∏–º—ñ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É. –¶–µ –¥–æ–∑–≤–æ–ª—è—î 
                    —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ –∑–∞–ø–∞—Å–∏ —Ç–æ–≤–∞—Ä—ñ–≤, –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω—ñ –ø—Ä–æ—Ü–µ—Å–∏ —Ç–∞ —Ä–µ–∞–≥—É–≤–∞—Ç–∏ –Ω–∞ –∑–º—ñ–Ω–∏ –≤ –ø–æ—Ç—Ä–µ–±–∞—Ö –ø–æ–∫—É–ø—Ü—ñ–≤. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –¥–∞—Ç—á–∏–∫–∏ –º–æ–∂—É—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Å–∏–≥–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ—Å—Ç—å –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –ø–æ–ª–∏—Ü—å –∞–±–æ –≤–∏—è–≤–ª—è—Ç–∏ —Ç–æ–≤–∞—Ä–∏, —è–∫—ñ –Ω–µ 
                    –∫–æ—Ä–∏—Å—Ç—É—é—Ç—å—Å—è –ø–æ–ø–∏—Ç–æ–º, —â–æ –¥–æ–ø–æ–º–∞–≥–∞—î –∫—Ä–∞—â–µ –ø–ª–∞–Ω—É–≤–∞—Ç–∏ –ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è. **Big Data**, –≤ —Å–≤–æ—é —á–µ—Ä–≥—É, –∑–∞–±–µ–∑–ø–µ—á—É—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –∞–Ω–∞–ª—ñ–∑—É –≤–µ–ª–∏—á–µ–∑–Ω–∏—Ö –º–∞—Å–∏–≤—ñ–≤ –¥–∞–Ω–∏—Ö, —â–æ –Ω–∞–¥—Ö–æ–¥—è—Ç—å –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª, —è–∫-–æ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó, –≤—ñ–¥–≥—É–∫–∏ –ø–æ–∫—É–ø—Ü—ñ–≤ —Ç–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ 
                    –Ω–∞ —Å–∞–π—Ç–∞—Ö. –ó–∞–≤–¥—è–∫–∏ –∞–Ω–∞–ª—ñ—Ç–∏—Ü—ñ **Big Data** –º–æ–∂–Ω–∞ –≤–∏–∑–Ω–∞—á–∞—Ç–∏ —Ç—Ä–µ–Ω–¥–∏, –≤–∏–≤—á–∞—Ç–∏ —Å–ø–æ–∂–∏–≤—á—ñ –≤–ø–æ–¥–æ–±–∞–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ —Ü—ñ–Ω–æ—É—Ç–≤–æ—Ä–µ–Ω–Ω—è. –ö—Ä—ñ–º —Ç–æ–≥–æ, —Ü—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏ –º–∞–π–±—É—Ç–Ω—ñ —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—ó, –ø—ñ–¥–≤–∏—â—É–≤–∞—Ç–∏ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∏—Ö –∫–∞–º–ø–∞–Ω—ñ–π 
                    —Ç–∞ –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ç–æ–≤–∞—Ä–Ω–∏–º–∏ –∑–∞–ø–∞—Å–∞–º–∏, —â–æ –≤ –∫—ñ–Ω—Ü–µ–≤–æ–º—É –ø—ñ–¥—Å—É–º–∫—É —Å–ø—Ä–∏—è—î –∑—Ä–æ—Å—Ç–∞–Ω–Ω—é –ø—Ä–æ–¥–∞–∂—ñ–≤ —ñ –ø–æ–ª—ñ–ø—à–µ–Ω–Ω—é –∫–ª—ñ—î–Ω—Ç—Å—å–∫–æ–≥–æ –¥–æ—Å–≤—ñ–¥—É.
                    
                    **–¢–µ–ø–ª–æ–≤—ñ –∫–∞—Ä—Ç–∏** –∞–∫—Ç–∏–≤–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –≤ —Ä—ñ—Ç–µ–π–ª—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ø–æ–∫—É–ø—Ü—ñ–≤, —ñ –æ–¥–Ω–∞ –∑ –∫–æ–º–ø–∞–Ω—ñ–π, —è–∫–∞ —É—Å–ø—ñ—à–Ω–æ –≤–ø—Ä–æ–≤–∞–¥–∏–ª–∞ —Ü–µ–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, ‚Äî —Ü–µ **IKEA**. –£ —Å–≤–æ—ó—Ö –º–∞–≥–∞–∑–∏–Ω–∞—Ö **IKEA** –∑–∞—Å—Ç–æ—Å–æ–≤—É—î —Ç–µ–ø–ª–æ–≤—ñ –∫–∞—Ä—Ç–∏ –¥–ª—è –≤–∏–≤—á–µ–Ω–Ω—è —Ç–æ–≥–æ, —è–∫ –ø–æ–∫—É–ø—Ü—ñ 
                    –ø–µ—Ä–µ—Å—É–≤–∞—é—Ç—å—Å—è –ø–æ —Ç–æ—Ä–≥–æ–≤–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—É, —è–∫—ñ –¥—ñ–ª—è–Ω–∫–∏ –≤—ñ–¥–≤—ñ–¥—É—é—Ç—å –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ —Ç–∞ –¥–µ –∑–∞—Ç—Ä–∏–º—É—é—Ç—å—Å—è –Ω–∞–π–¥–æ–≤—à–µ.
                    
                    –ù–∞ –æ—Å–Ω–æ–≤—ñ —Ç–µ–ø–ª–æ–≤–∏—Ö –∫–∞—Ä—Ç **IKEA** –≤–∏—è–≤–∏–ª–∞, —â–æ –¥–µ—è–∫—ñ –ø–æ–ø—É–ª—è—Ä–Ω—ñ –∑–æ–Ω–∏, —Ç–∞–∫—ñ —è–∫ –∫—É—Ö–æ–Ω–Ω—ñ –≤—ñ–¥–¥—ñ–ª–∏ –∞–±–æ –≤—ñ–¥–¥—ñ–ª–∏ –∑—ñ —Å–ø–∞–ª—å–Ω—è–º–∏, –ø—Ä–∏–≤–µ—Ä—Ç–∞—é—Ç—å –Ω–∞–π–±—ñ–ª—å—à–µ –ø–æ–∫—É–ø—Ü—ñ–≤. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ü—ñ –¥–∞–Ω—ñ, –∫–æ–º–ø–∞–Ω—ñ—è –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞–ª–∞ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤, —Å—Ç–≤–æ—Ä–∏–≤—à–∏ 
                    —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –º–∞—Ä—à—Ä—É—Ç–∏ —á–µ—Ä–µ–∑ –º–µ–Ω—à –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω—ñ —á–∞—Å—Ç–∏–Ω–∏ –º–∞–≥–∞–∑–∏–Ω—É, —â–æ–± –Ω–∞–ø—Ä–∞–≤–∏—Ç–∏ –ø–æ–∫—É–ø—Ü—ñ–≤ –¥–æ –Ω–æ–≤–∏—Ö –∞–±–æ –∞–∫—Ü—ñ–π–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤. –¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ–∑–≤–æ–ª–∏–≤ IKEA –∑–±—ñ–ª—å—à–∏—Ç–∏ –ø—Ä–æ–¥–∞–∂—ñ —Ç–æ–≤–∞—Ä—ñ–≤, —è–∫—ñ –Ω–µ –ø–æ—Ç—Ä–∞–ø–ª—è–ª–∏ —É –ø–æ–ª–µ –∑–æ—Ä—É –ø–æ–∫—É–ø—Ü—ñ–≤ —Ä–∞–Ω—ñ—à–µ. 
                    
                    –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—è –ø–æ–∫—É–ø—Ü—ñ–≤ ‚Äî —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è, —è–∫–∞ –¥–æ–∑–≤–æ–ª—è—î —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏, —è–∫ –ø–æ–∫—É–ø—Ü—ñ –ø–µ—Ä–µ–º—ñ—â—É—é—Ç—å—Å—è –ø–æ –º–∞–≥–∞–∑–∏–Ω—É, –∑ –º–µ—Ç–æ—é –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ —Ç–∞ –ø–æ–ª—ñ–ø—à–µ–Ω–Ω—è –¥–æ—Å–≤—ñ–¥—É —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤ ‚Äì –¥–æ—Å—è–≥–∞—î—Ç—å—Å—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö 
                    –ø—Ä–æ –º–∞—Ä—à—Ä—É—Ç–∏, —è–∫—ñ –ø—Ä–æ—Ö–æ–¥—è—Ç—å —Å–ø–æ–∂–∏–≤–∞—á—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó, —Ç–∞–∫—ñ —è–∫ –∫–∞–º–µ—Ä–∏, –¥–∞—Ç—á–∏–∫–∏ —Ç–∞ –º–æ–±—ñ–ª—å–Ω—ñ –¥–æ–¥–∞—Ç–∫–∏.
                    
                    –ö–æ–º–ø–∞–Ω—ñ—è Tesco, –æ–¥–∏–Ω —ñ–∑ –Ω–∞–π–±—ñ–ª—å—à–∏—Ö —Ä—ñ—Ç–µ–π–ª–µ—Ä—ñ–≤ —É –í–µ–ª–∏–∫—ñ–π –ë—Ä–∏—Ç–∞–Ω—ñ—ó, –∞–∫—Ç–∏–≤–Ω–æ –≤–ø—Ä–æ–≤–∞–¥–∂—É—î —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó –ø–æ–∫—É–ø—Ü—ñ–≤ —É —Å–≤–æ—ó—Ö —Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç–∞—Ö. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∞–Ω–∞–ª—ñ—Ç–∏–∫—É —Ç–∞ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ, Tesco –º–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤—ñ–¥—Å—Ç–µ–∂—É–≤–∞—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏ –ø–æ–∫—É–ø—Ü—ñ–≤ 
                    —É –º–∞–≥–∞–∑–∏–Ω–∞—Ö —ñ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏, —è–∫—ñ –¥—ñ–ª—è–Ω–∫–∏ —î –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–º–∏, –∞ —è–∫—ñ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –±–µ–∑ —É–≤–∞–≥–∏. –¶—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –¥–æ–∑–≤–æ–ª—è—î Tesco –Ω–µ –ª–∏—à–µ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö, –∞–ª–µ –π —Ä–æ–∑—Ä–æ–±–ª—è—Ç–∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó, —è–∫—ñ –∑–∞–ª—É—á–∞—é—Ç—å —É–≤–∞–≥—É –ø–æ–∫—É–ø—Ü—ñ–≤. 
                    –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —è–∫—â–æ –ø–µ–≤–Ω—ñ —Ç–æ–≤–∞—Ä–∏, —Ç–∞–∫—ñ —è–∫ —Å–≤—ñ–∂—ñ –æ–≤–æ—á—ñ —Ç–∞ —Ñ—Ä—É–∫—Ç–∏, –Ω–µ –æ—Ç—Ä–∏–º—É—é—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —É–≤–∞–≥–∏, Tesco –º–æ–∂–µ –ø–æ–º—ñ—Å—Ç–∏—Ç–∏ —ó—Ö —É –∑–æ–Ω–∏, –¥–µ –ø–æ–∫—É–ø—Ü—ñ –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –ø—Ä–æ—Ö–æ–¥—è—Ç—å, –∞–±–æ –≤–ª–∞—à—Ç—É–≤–∞—Ç–∏ –∞–∫—Ü—ñ—ó, —â–æ–± –ø—Ä–∏–≤–µ—Ä–Ω—É—Ç–∏ —ó—Ö–Ω—é —É–≤–∞–≥—É. –ó–∞–≤–¥—è–∫–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—é —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó –ø–æ–∫—É–ø—Ü—ñ–≤, 
                    –∫–æ–º–ø–∞–Ω—ñ—è –∑–∞—Ñ—ñ–∫—Å—É–≤–∞–ª–∞ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –ø—Ä–æ–¥–∞–∂—ñ–≤ —Å–≤—ñ–∂–∏—Ö –ø—Ä–æ–¥—É–∫—Ç—ñ–≤ –Ω–∞ 20% —É –º–∞–≥–∞–∑–∏–Ω–∞—Ö, –¥–µ –±—É–ª–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞–Ω–∏—Ö –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó. [3]
                    
                    –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –ø–æ–ø–∏—Ç—É —î –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏–º –∞—Å–ø–µ–∫—Ç–æ–º —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ä–æ–∑–¥—Ä—ñ–±–Ω–æ—é —Ç–æ—Ä–≥—ñ–≤–ª–µ—é, –æ—Å–∫—ñ–ª—å–∫–∏ –¥–æ–∑–≤–æ–ª—è—î —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –∑–∞–ø–∞—Å–∏, –∑–º–µ–Ω—à–∏—Ç–∏ –≤–∏—Ç—Ä–∞—Ç–∏ —ñ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç—ñ–≤. –¶–µ –ø—Ä–æ—Ü–µ—Å, –≤ —è–∫–æ–º—É –∞–Ω–∞–ª—ñ–∑—É—é—Ç—å—Å—è –¥–∞–Ω—ñ –ø—Ä–æ –ø—Ä–æ–¥–∞–∂—ñ, —Å–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å, 
                    —Ä–∏–Ω–∫–æ–≤—ñ —Ç—Ä–µ–Ω–¥–∏ —Ç–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫—É —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–∞–π–±—É—Ç–Ω—å–æ–≥–æ –ø–æ–ø–∏—Ç—É –Ω–∞ –ø–µ–≤–Ω—ñ —Ç–æ–≤–∞—Ä–∏. –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –ø–æ–ø–∏—Ç—É “ë—Ä—É–Ω—Ç—É—î—Ç—å—Å—è –Ω–∞ –∞–Ω–∞–ª—ñ–∑—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö, —è–∫—ñ –º–æ–∂—É—Ç—å –≤–∫–ª—é—á–∞—Ç–∏: –æ–±—Å—è–≥–∏ –ø—Ä–æ–¥–∞–∂—ñ–≤ —É —Ä—ñ–∑–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏ —á–∞—Å—É, –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–∞—Ç–µ—Ä–Ω—ñ–≤ —Ç–∞ —Ç—Ä–µ–Ω–¥—ñ–≤; —Å–µ–∑–æ–Ω–Ω—ñ –∫–æ–ª–∏–≤–∞–Ω–Ω—è, 
                    —Ç–∞–∫—ñ —è–∫ —Å–≤—è—Ç–∫–æ–≤—ñ –ø–µ—Ä—ñ–æ–¥–∏ —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –ø–æ–¥—ñ—ó, —â–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –ø–æ–ø–∏—Ç; –º–∞–∫—Ä–æ–µ–∫–æ–Ω–æ–º—ñ—á–Ω—ñ —Ñ–∞–∫—Ç–æ—Ä–∏, —è–∫—ñ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –∫—É–ø—ñ–≤–µ–ª—å–Ω—É —Å–ø—Ä–æ–º–æ–∂–Ω—ñ—Å—Ç—å —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤; –¥–∞–Ω—ñ –ø—Ä–æ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –ø–æ–∫—É–ø—Ü—ñ–≤, –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è, —è–∫—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —ó—Ö–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è —â–æ–¥–æ –ø–æ–∫—É–ø–∫–∏.
                    
                    A/B —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —î —â–µ –æ–¥–Ω–∏–º –ø–æ—Ç—É–∂–Ω–∏–º —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤. –¶–µ–π –º–µ—Ç–æ–¥ –ø–æ–ª—è–≥–∞—î —É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –¥–≤–æ—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —è–∫–∏–π –∑ –Ω–∏—Ö —î –±—ñ–ª—å—à –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–º —É –∑–∞–ª—É—á–µ–Ω–Ω—ñ –ø–æ–∫—É–ø—Ü—ñ–≤.
                    
                    –û–¥–∏–Ω –∑ —è—Å–∫—Ä–∞–≤–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ A/B —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —î Amazon. –ö–æ–º–ø–∞–Ω—ñ—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–µ—Å—Ç–∏, –∑–º—ñ–Ω—é—é—á–∏ –ø–æ—Ä—è–¥–æ–∫ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö, —â–æ–± –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —è–∫–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç –∫—Ä–∞—â–µ –∫–æ–Ω–≤–µ—Ä—Ç—É—î –≤—ñ–¥–≤—ñ–¥—É–≤–∞—á—ñ–≤ —É –ø–æ–∫—É–ø—Ü—ñ–≤. –û–¥–Ω–∏–º —ñ–∑ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —î —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö 
                    –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–Ω–æ–ø–∫–∏ "–î–æ–¥–∞—Ç–∏ –¥–æ –∫–æ—à–∏–∫–∞". Amazon –∑–º—ñ–Ω—é–≤–∞–ª–∞ –∫–æ–ª—å–æ—Ä–∏, —Ä–æ–∑–º—ñ—Ä–∏ —Ç–∞ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è –∫–Ω–æ–ø–∫–∏, —â–æ–± –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —è–∫–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç —Å–ø—Ä–∏—è—î –Ω–∞–π–±—ñ–ª—å—à—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª—ñ–∫—ñ–≤ —Ç–∞ –∫–æ–Ω–≤–µ—Ä—Å—ñ–π. –£ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ –æ–¥–Ω–æ–≥–æ –∑ —Ç–∞–∫–∏—Ö —Ç–µ—Å—Ç—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ—è –≤–∏—è–≤–∏–ª–∞, —â–æ –∑–º—ñ–Ω–∞ –∫–æ–ª—å–æ—Ä—É –∫–Ω–æ–ø–∫–∏ –Ω–∞ —è—Å–∫—Ä–∞–≤—ñ—à–∏–π –≤—ñ–¥—Ç—ñ–Ω–æ–∫ –ø—Ä–∏–∑–≤–µ–ª–∞ –¥–æ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø—Ä–æ–¥–∞–∂—ñ–≤ –Ω–∞ 3%. –•–æ—á–∞ —Ü–µ –∑–¥–∞—î—Ç—å—Å—è –Ω–µ–∑–Ω–∞—á–Ω–∏–º, –¥–ª—è –∫–æ–º–ø–∞–Ω—ñ—ó –∑ —Ç–∞–∫–∏–º–∏ –æ–±—Å—è–≥–∞–º–∏ –ø—Ä–æ–¥–∞–∂—ñ–≤, —è–∫ Amazon, –Ω–∞–≤—ñ—Ç—å –º–∞–ª–µ–Ω—å–∫–µ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –∫–æ–Ω–≤–µ—Ä—Å—ñ—ó –º–æ–∂–µ –æ–∑–Ω–∞—á–∞—Ç–∏ –º—ñ–ª—å–π–æ–Ω–∏ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–æ–ª–∞—Ä—ñ–≤ —É –ø—Ä–∏–±—É—Ç–∫–∞—Ö. [4]
                    
                    –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø–∞—Å—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç—É —Ä–µ—á–µ–π (IoT) —î –Ω–µ –º–µ–Ω—à –≤–∞–∂–ª–∏–≤–∏–º –∞—Å–ø–µ–∫—Ç–æ–º —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ä—ñ—Ç–µ–π–ª–æ–º, —è–∫–∏–π –¥–æ–∑–≤–æ–ª—è—î –∫–æ–º–ø–∞–Ω—ñ—è–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞—Ç–∏ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ—Ü–µ—Å–∏ –∫–æ–Ω—Ç—Ä–æ–ª—é –∑–∞ —Ç–æ–≤–∞—Ä–Ω–∏–º–∏ –∑–∞–ø–∞—Å–∞–º–∏. IoT –≤–∫–ª—é—á–∞—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Å–µ–Ω—Å–æ—Ä—ñ–≤, –ø—ñ–¥–∫–ª—é—á–µ–Ω–∏—Ö –¥–æ –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç—É, –¥–ª—è –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö –ø—Ä–æ —Å—Ç–∞–Ω —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å–∫–ª–∞–¥–∞—Ö —ñ –≤ –º–∞–≥–∞–∑–∏–Ω–∞—Ö. –¶–µ –∑–∞–±–µ–∑–ø–µ—á—É—î —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ä—ñ–≤–µ–Ω—å –∑–∞–ø–∞—Å—ñ–≤, —Ç–µ—Ä–º—ñ–Ω–∏ –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä—ñ–≤ —Ç–∞ —ñ–Ω—à—ñ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏. 
                    
                    –°–µ–Ω—Å–æ—Ä–∏: –†—ñ—Ç–µ–π–ª–µ—Ä–∏ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—é—Ç—å —Å–µ–Ω—Å–æ—Ä–∏ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö, —É —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–∞—Ö —Ç–∞ –Ω–∞ —Å–∫–ª–∞–¥—ñ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä—ñ–≤. –¶—ñ —Å–µ–Ω—Å–æ—Ä–∏ –º–æ–∂—É—Ç—å –≤–∏—è–≤–ª—è—Ç–∏, –∫–æ–ª–∏ –∑–∞–ø–∞—Å–∏ –∑–Ω–∏–∂—É—é—Ç—å—Å—è –¥–æ –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è. –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö: –î–∞–Ω—ñ –∑ —Å–µ–Ω—Å–æ—Ä—ñ–≤ –ø–µ—Ä–µ–¥–∞—é—Ç—å—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –Ω–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É, –¥–µ –≤–æ–Ω–∏ –∞–Ω–∞–ª—ñ–∑—É—é—Ç—å—Å—è. –¶–µ –¥–æ–∑–≤–æ–ª—è—î —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω –∑–∞–ø–∞—Å—ñ–≤ –±–µ–∑ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –ø—Ä–æ–≤–æ–¥–∏—Ç–∏ —Ñ—ñ–∑–∏—á–Ω—É —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—é. –ê–Ω–∞–ª—ñ–∑ —ñ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è: –ù–∞ –æ—Å–Ω–æ–≤—ñ –∑—ñ–±—Ä–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Ä—ñ—Ç–µ–π–ª–µ—Ä–∏ –º–æ–∂—É—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏ –ø–æ–ø–∏—Ç –Ω–∞ —Ç–æ–≤–∞—Ä–∏, –≤–∏—è–≤–ª—è—Ç–∏ –Ω–µ–¥–æ–ª—ñ–∫–∏ —É –∑–∞–ø–∞—Å–∞—Ö —ñ —Å–≤–æ—î—á–∞—Å–Ω–æ –ø–æ–ø–æ–≤–Ω—é–≤–∞—Ç–∏ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è: –£ —Ä–∞–∑—ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –∑–∞–ø–∞—Å—ñ–≤ —Å–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ –ø–æ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤, —â–æ –∑–Ω–∏–∂—É—î —Ä–∏–∑–∏–∫ –¥–µ—Ñ—ñ—Ü–∏—Ç—É.
                    
                    Carrefour ‚Äì –æ–¥–Ω–∞ –∑ –Ω–∞–π–±—ñ–ª—å—à–∏—Ö –º–µ—Ä–µ–∂ —Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç—ñ–≤ —É —Å–≤—ñ—Ç—ñ ‚Äì –∞–∫—Ç–∏–≤–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó IoT –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∑–∞–ø–∞—Å–∞–º–∏. Carrefour –≤–ø—Ä–æ–≤–∞–¥–∏–ª–∞ —Ä—ñ—à–µ–Ω–Ω—è, —è–∫—ñ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ —Å–µ–Ω—Å–æ—Ä–∞—Ö, —â–æ –≤—ñ–¥—Å—Ç–µ–∂—É—é—Ç—å –∑–∞–ø–∞—Å–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö –º–∞–≥–∞–∑–∏–Ω—ñ–≤. –î–æ –ø—Ä–∏–∫–ª–∞–¥—É, –∫–æ–º–ø–∞–Ω—ñ—è —Ä–µ–∞–ª—ñ–∑—É–≤–∞–ª–∞ –ø—Ä–æ–µ–∫—Ç —É –§—Ä–∞–Ω—Ü—ñ—ó, –¥–µ —Å–µ–Ω—Å–æ—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ñ—ñ–∫—Å—É–≤–∞–ª–∏, –∫–æ–ª–∏ —Ä—ñ–≤–µ–Ω—å —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø–æ–ª–∏—Ü—è—Ö –∑–Ω–∏–∂—É—î—Ç—å—Å—è –¥–æ –ø–µ–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É. –¶—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –Ω–∞–¥—Ö–æ–¥–∏–ª–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É —Å–∏—Å—Ç–µ–º—É —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è, —è–∫–∞ –≤ —Ä–µ–∂–∏–º—ñ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É –º–æ–Ω—ñ—Ç–æ—Ä–∏–ª–∞ –∑–∞–ø–∞—Å–∏ –ø–æ –≤—Å—ñ–π –º–µ—Ä–µ–∂—ñ –º–∞–≥–∞–∑–∏–Ω—ñ–≤. –î–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É —Å–≤—ñ–∂–æ—Å—Ç—ñ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤, –æ—Å–æ–±–ª–∏–≤–æ –≤ —Å–µ–∫—Ç–æ—Ä–∞—Ö —Å–≤—ñ–∂–∏—Ö –æ–≤–æ—á—ñ–≤ —ñ —Ñ—Ä—É–∫—Ç—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞ –¥–∞—Ç—á–∏–∫–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ —ñ –≤–æ–ª–æ–≥–æ—Å—Ç—ñ, —â–æ–± –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —É–º–æ–≤–∏ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤ –ó–∞–≤–¥—è–∫–∏ —Ü—å–æ–º—É –ø—ñ–¥—Ö–æ–¥—É Carrefour –∑–º–æ–≥–ª–∞ –∑–º–µ–Ω—à–∏—Ç–∏ –≤–∏—Ç—Ä–∞—Ç–∏, –∑–±—ñ–ª—å—à–∏—Ç–∏ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤ —Ç–∞ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å. [5]
                    
                    –í–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —É —Ä–æ–∑–¥—Ä—ñ–±–Ω—ñ–π —Ç–æ—Ä–≥—ñ–≤–ª—ñ —î –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ—Å—Ç—é –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó –¥–æ —Å—É—á–∞—Å–Ω–∏—Ö —É–º–æ–≤ —Ä–∏–Ω–∫—É. –¶–µ –Ω–µ –ª–∏—à–µ –∑–º—ñ–Ω—é—î –ø—Ä–∞–≤–∏–ª–∞ –≥—Ä–∏, –∞–ª–µ –π –≤—ñ–¥–∫—Ä–∏–≤–∞—î –Ω–æ–≤—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –±—ñ–∑–Ω–µ—Å—É —Ç–∞ –≥–ª–∏–±—à–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç—ñ–≤. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É, –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö, –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ —Ç–∞ IoT –¥–æ–ø–æ–º–∞–≥–∞—î —Ä—ñ—Ç–µ–π–ª–µ—Ä–∞–º –Ω–µ –ª–∏—à–µ –∑–∞–¥–æ–≤–æ–ª—å–Ω—è—Ç–∏ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤, –∞–ª–µ –π —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –¥–æ—Å–≤—ñ–¥, —è–∫–∏–π —î –∫–ª—é—á–æ–≤–∏–º –¥–ª—è –ª–æ—è–ª—å–Ω–æ—Å—Ç—ñ —Ç–∞ —É—Å–ø—ñ—Ö—É –≤ —É–º–æ–≤–∞—Ö –≤–∏—Å–æ–∫–æ—ó –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó.

                    –õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ñ –¥–∂–µ—Ä–æ–µ–ª–∞:
                    1.	Duhigg C. How Companies Learn Your Secrets. The New York Time Magazine. 2012. URL: https://doi.org/10.7312/star16075-025
                    2.	de Mattos C. A., Correia F. C., Kissimoto K. O. Artificial Intelligence Capabilities for Demand Planning Process. Logistics. 2024. Vol. 8, no. 2. P. 53. URL: https://doi.org/10.3390/logistics8020053 
                    3.	Smith D. Logistics in Tesco: Past, Present and Future. Logistics And Retail Management insights Into Current Practice And Trends From Leading Experts. Boca Raton, 2023. P. 154‚Äì183. URL: https://doi.org/10.4324/9780429271144-8 
                    4.	The impacts of learning analytics and A/B testing research: a case study in differential scientometrics / R. S. Baker et al. International Journal of STEM Education. 2022. Vol. 9, no. 1. URL: https://doi.org/10.1186/s40594-022-00330-6 
                    5.	CarrefourSA's Rapid Expansion Strategy: Leveraging IoT | IoT ONE Digital Transformation Advisors. IoT ONE. URL: https://www.iotone.com/case-study/carrefoursa-s-rapid-expansion-strategy-leveraging-iot-for-retail-growth/c3169 
                    """)

# SQL
if selected == 'SQL':
    st.markdown("<h1 style='text-align: center;'>SQL</h1>", unsafe_allow_html=True)
    selected_options = ["Overview", 
                        "PostgreSQL",
                        "SQLite"]
    selected = st.selectbox("Choose a topic you would like to check me on:", options = selected_options)
    st.write("Current selection:", selected)
    if selected == "Overview":
        st.subheader("Overview")
        st.markdown("""
                    Here, I share my journey and projects involving some of the most powerful and widely used database technologies: PostgreSQL, SQLite and MongoDB. 
                    
                    Each project showcases unique features, practical applications, and best practices for using these databases effectively.
        """)
        st.subheader("What You'll Find Here")
        st.markdown("""
                    - **PostgreSQL Projects:** 

                    - **SQLite Projects:** 
                    """)

        st.write("Let's dive deep into my practical realm... üîé")
   
    elif selected == "PostgreSQL":
        st.subheader("PostgreSQL")
        selected_options = ["#1. Titanic-Dataset", 
                            "#2 Tennis country club"]
        selected = st.selectbox("Choose a project:", options = selected_options)
        if selected == "#1. Titanic-Dataset":
            st.subheader("Titanic-Dataset")
            df = pd.read_csv('datasets/Titanic-Dataset.csv')
            st.write(df)
            st.write("**Here is a description of each column in the Titanic dataset:**")
            st.write("üìå **PassengerId** - Unique identifier for each passenger.",
                 "<br>üìå **Survived** - Survival status (0 = No, 1 = Yes).",
                 "<br>üìå **Pclass** - Passenger class (1st, 2nd, 3rd)",
                 "<br>üìå **Name** - Full name of the passenger",
                 "<br>üìå **Sex** - Gender of the passenger",
                 "<br>üìå **Age** - Age of the passenger in years",
                 "<br>üìå **SibSp** - Number of siblings/spouses aboard the Titanic",
                 "<br>üìå **Parch** - Number of parents/children aboard the Titanic",
                 "<br>üìå **Ticket** - Ticket number",
                 "<br>üìå **Fare** - Passenger fare",
                 "<br>üìå **Cabin** - Cabin number (if known)",
                 "<br>üìå **Embarked** - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)",
                 unsafe_allow_html=True)
            # 1
            st.write("**1. A list of the first 5 passangers of the Titanic.**")
            code = '''SELECT * FROM titanic LIMIT 5'''
            st.code(code, language='sql')
            subset_df = df.head(5)  
            st.write(subset_df)
            # 2
            st.write("**2. A list of passengers of the Titanic whose sex is female.**")
            code = '''SELECT * FROM titanic WHERE sex = 'female' '''
            st.code(code, language='sql')
            subset_df = df[df['Sex'] == 'female']
            st.write(subset_df)
            # 3
            st.write("**3. A list of passengers who were younger than 10 years old, did not survive on sinking Titanic, and were in class 1.**")
            code = """SELECT * FROM titanic WHERE age < 10 AND survived = 0 AND pclass = 1"""
            st.code(code, language='sql')
            subset_df = df[(df['Age'] < 10) & (df['Survived'] == 0) & (df['Pclass'] == 1)]
            st.write(subset_df)
            # 4
            st.write("**4. A list of passengers who are either male or younger than 5 years old.**")
            code = """SELECT age, sex WHERE age < 5 OR age = 'female' """
            st.code(code, language='sql')
            subset_df = df[(df['Sex'] == 'male') | (df['Age'] < 5)][['Sex', 'Age']] 
            st.write(subset_df)
            # 5
            st.write("**5. A list of unique values for the sex of the passengers from the Titanic.**")
            code = """SELECT DISTINCT sex AS unique_sex FROM titanic"""
            st.code(code, language='sql')
            unique_sex = pd.DataFrame(df['Sex'].dropna().unique(), columns=['unique_sex']) 
            st.write(unique_sex)
            # 6
            st.write("**6. A list of passengers from the Titanic whose cabins have a name.**")
            code = """SELECT * FROM titanic WHERE cabin IS NOT NULL"""
            st.code(code, language='sql')
            non_null_cabin_df = df[df['Cabin'].notna()] 
            st.write(non_null_cabin_df)
            # 7
            st.write("**7. A list of passengers from the Titanic showing their survival status, port of embarkation, and the number of siblings/spouses aboard, "
                     "where the port of embarkation is either 'S' (Southampton) or 'C' (Cherbourg) and the number of siblings/spouses aboard is either 0, 1, or 3.**")
            code = """SELECT survived, embarked, sibsp FROM titanic 
                      WHERE embarked IN ('S', 'C') AND
                      sibsp IN (0, 1, 3)"""
            st.code(code, language='sql')
            filtered_df = df[(df['Embarked'].isin(['S', 'C'])) & (df['SibSp'].isin([0, 1, 3]))][['Survived', 'Embarked', 'SibSp']]
            st.write(filtered_df)
            # 8
            st.write("**8. A list of passengers from the Titanic whose number of siblings/spouses aboard is between 2 and 4 (inclusive) and the port of embarkation "
                     "is between 'A' and 'Q' (alphabetically).**")
            code = """SELECT survived, embarked, sibsp FROM titanic 
                      WHERE (sibsp BETWEEN 2 AND 4) AND 
                      (embarked BETWEEN 'A' AND 'Q')"""
            st.code(code, language='sql')
            filtered_df = df[(df['SibSp'].between(2, 4)) & (df['Embarked'].between('A', 'Q'))][['Survived', 'Embarked', 'SibSp']]
            st.write(filtered_df)
            # 9
            st.write("**9. A list of passengers from the Titanic showing their names and cabin values, where the passenger's name contains the letter 'a' "
                      "(case insensitive) and the cabin starts with the letter 'G' followed by any single character.**")
            code = """SELECT name, cabin FROM titanic WHERE 
                      LOWER(name) LIKE '%a%' AND
                      LOWER(cabin) LIKE 'g_'"""
            st.code(code, language='sql')
            df['name_lower'] = df['Name'].str.lower()
            df['cabin_lower'] = df['Cabin'].str.lower()
            filtered_df = df[df['name_lower'].str.contains('a') & df['cabin_lower'].str.match('g.')][['Name', 'Cabin']]
            st.write(filtered_df)
            # 10
            st.write("**10. Calculate and display the following statistics for the 'age' column in the Titanic: count, sum, mean, minimum, maximum, variance, "
                     "and standard deviation.**")
            code = """SELECT COUNT(age) AS "age_count",
                      SUM(age) AS "age_sum",
                      AVG(age) AS "age_mean",
                      MIN(age) AS "age_min",
                      MAX(age) AS "age_max",
                      VARIANCE(age) AS "age_variance",
                      STDDEV(age) AS "age_stdev"                      
                      FROM titanic"""
            st.code(code, language='sql')
            age_count = df['Age'].count()
            age_sum = df['Age'].sum()
            age_mean = df['Age'].mean()
            age_min = df['Age'].min()
            age_max = df['Age'].max()
            age_variance = df['Age'].var()
            age_stdev = df['Age'].std()
            age_stats = pd.DataFrame({
                "Count": [age_count],
                "Sum": [age_sum],
                "Mean": [age_mean],
                "Min": [age_min],
                "Max": [age_max],
                "Variance": [age_variance],
                "StDev": [age_stdev]
                })
            st.write(age_stats)
            # 11
            st.write("**11. Calculate and display the correlation and sample covariance between the 'age' and 'fare' columns.**")
            code = """SELECT CORR(age, fare),
                      COVAR_SAMP(age, fare) 
                      FROM titanic"""
            st.code(code, language='sql')
            corr_age_fare = df['Age'].corr(df['Fare'])
            covar_samp_age_fare = df[['Age', 'Fare']].cov().iloc[0, 1]
            stats_df = pd.DataFrame({
                "Pearson‚Äô correlation coefficient": [corr_age_fare],
                "Sample covariance": [covar_samp_age_fare]
            })
            st.write(stats_df)
            # 12
            st.write("**12. A list of passengers in the Titanic, showing their age and survival status, where the age is not null, ordered by age in descending order.**")
            code = """SELECT age, survived FROM titanic 
                      WHERE age IN NOT NULL 
                      ORDER BY age DESC"""
            st.code(code, language='sql')
            filtered_df = df[df['Age'].notnull()][['Age', 'Survived']].sort_values(by='Age', ascending=False)
            st.write(filtered_df)
            # 13
            st.write("**13. Count the number of passengers for each sex in the Titanic and display the results ordered by count in descending order.**")
            code = """SELECT sex, COUNT(sex) FROM titanic
                      GROUP BY sex
                      ORDER BY COUNT(sex) DESC"""
            st.code(code, language='sql')
            sex_counts = df.groupby('Sex').size().reset_index(name='Count').sort_values(by='Count', ascending=False)
            st.write(sex_counts)
            # 14
            st.write("**14. Count the number of passengers for each 'sibsp' value in the Titanic dataset, showing only those with a count greater than 10, and display the "
                     "results ordered by count in descending order.**")
            code = """SELECT sibsp, COUNT(*) FROM titanic 
                      GROUP BY sibsp 
                      HAVING COUNT(*) > 10
                      ORDER BY COUNT(*) DESC"""
            st.code(code, language='sql')
            sibsp_counts = df.groupby('SibSp').size().reset_index(name='Count')
            sibsp_counts_filtered = sibsp_counts[sibsp_counts['Count'] > 10].sort_values(by='Count', ascending=False)
            st.write(sibsp_counts_filtered)
            # 15
            st.write("**15. Calculate and display several mathematical transformations of the fare values. Include rounding to one decimal place, square root, cube root, floor, ceiling, truncation, logarithm (with a slight adjustment for values close to zero), exponential, and power of 2 transformations.**")
            code = """SELECT fare, 
                      ROUND(fare::NUMERIC, 1) AS round,
                      ROUND(SQRT(fare)::NUMERIC, 1) AS sqrt, 
                      ROUND(CBRT(fare)::NUMERIC, 1) AS cbrt,
                      ROUND(FLOOR(fare)::NUMERIC, 1) AS floor,
                      ROUND(CEIL(fare)::NUMERIC, 1) AS ceil,
                      TRUNC(fare),
                      ROUND(LOG(fare + 0.0000001)::NUMERIC, 1) AS log,
                      ROUND(EXP(fare)::NUMERIC, 1) AS exp,
                      ROUND(POWER(fare, 2)::NUMERIC, 1) AS power_2
                      FROM titanic"""
            st.code(code, language='sql')
            def apply_transformations(df):
                df['round'] = df['Fare'].round(1)
                df['sqrt'] = np.sqrt(df['Fare']).round(1)  # Square root of fare
                df['cbrt'] = np.cbrt(df['Fare']).round(1)  # Cube root of fare
                df['floor'] = np.floor(df['Fare']).round(1)  # Floor value of fare
                df['ceil'] = np.ceil(df['Fare']).round(1)  # Ceiling value of fare
                df['trunc'] = df['Fare'].apply(int)  # Truncate fare to integer
                df['log'] = np.log(df['Fare'] + 0.0000001).round(1)  # Logarithm of fare (adjusted for values close to zero)
                df['exp'] = np.exp(df['Fare']).round(1)  # Exponential of fare
                df['power_2'] = np.power(df['Fare'], 2).round(1)  # Power of 2 of fare

                df = df[['Fare', 'round', 'sqrt', 'cbrt', 'floor', 'ceil', 'trunc', 'log', 'exp', 'power_2']]

                df = df.rename(columns={
                                        'round': 'Rounded Fare (1 decimal)',
                                        'sqrt': 'Square Root',        
                                        'cbrt': 'Cube Root',
                                        'floor': 'Floor Value',
                                        'ceil': 'Ceiling Value',
                                        'trunc': 'Truncated Fare',
                                        'log': 'Logarithm',
                                        'exp': 'Exponential',
                                        'power_2': 'Fare Squared'
                                        })
                return df 
            transformed_df = apply_transformations(df)
            st.write(transformed_df)
            # 16
            st.write("**16. Perform various string operations on the 'name' column in the Titanic dataset and order by length.**")
            code = """SELECT name, 
                      LOWER(name),
                      UPPER(name),
                      INITCAP(lLOWER(name)),
                      TRIM(name) AS trim,
                      LEFT(name, 2),
                      RIGHT(name, 2),
                      LENGTH(name),
                      SUBSTR(name, 2, 5),
                      POSITION('r' IN name),
                      SUBSTRING(name FROM 2 FOR 3),
                      SUBSTRING(LOWER(name), '^l.*i$'),
                      SUBSTRING(LOWER(name), 'l', 'lll'),
                      REPEAT(LEFT(name, 2), 3),
                      REVERSE(name),
                      FROM titanic
                      ORDER BY length"""
            st.code(code, language='sql')
            df['Lower name'] = df['Name'].str.lower() # Converts letters to Lowercase
            df['Upper name'] = df['Name'].str.upper() # Converts letters to Uppercase
            df['First letter'] = df['Name'].str.title() # Capitalizes the first letter of each word
            df['Spaces'] = df['Name'].str.strip() # Removes extra spaces at the beginning and end
            df['Left 2'] = df['Name'].str[:2] # Extracts the first two characters
            df['Right 2'] = df['Name'].str[-2:] # Extracts the last two characters.
            df['Length'] = df['Name'].str.len() # Calculates the length of each name
            df['Substr'] = df['Name'].str[1:6] # Extracts 5 characters from 2nd position (starting fro 1)
            df['Position r'] = df['Name'].str.find('r') # Finds the position of the first occurrence of the letter "r"
            df['Substring'] = df['Name'].str[1:4] # Extracts 3 characters from index 2
            df['Substring Regex'] = df['Name'].str.lower().str.extract(r'^(l.*i)$', expand=False) # Finds names starting with 'l' and ending with 'i'
            df['Replace l with lll'] = df['Name'].str.lower().str.replace('l', 'lll', regex=True) # Replaces every "l" with "lll"
            df['Repeat left 2'] = df['Name'] * 3  # Repeat the first 2 chars of name 3 times
            df['Reverse name'] = df['Name'].apply(lambda x: x[::-1] if isinstance(x, str) else x) # Reverses the name 
            
            df_sorted = df[['Name', 'Lower name', 'Upper name', 'First letter', 'Spaces', 'Left 2', 'Right 2', 
                'Length', 'Substr', 'Position r', 'Substring', 'Substring Regex', 'Replace l with lll', 'Repeat left 2', 'Reverse name']].sort_values(by='Length')
            st.write(df_sorted)
            # 17
            st.write("**17. What percentage of males and females survived?**")
            code = """SELECT sex,
                      ROUND(COUNT(sex) / AVG(c.count)) * 100, 1) AS survived_percentage
                      FROM titanic
                      LEFT JOIN
                      (SELECT sex, COUNT(sex) FROM titanic GROUP BY sex) AS c USING(sex)
                      WHERE survived = 1
                      GROUP BY sex, survived 
                      ORDER BY survived_percentage DESC"""
            st.code(code, language='sql')
            survival_percentage = (df[df['Survived'] == 1]['Sex'].value_counts() / df['Sex'].value_counts()) * 100
            survival_percentage_df = survival_percentage.reset_index()
            survival_percentage_df.columns = ['Sex', 'Survived Percentage']
            survival_percentage_df['Survived Percentage'] = survival_percentage_df['Survived Percentage'].round(1)
            survival_percentage_df = survival_percentage_df.sort_values(by='Survived Percentage', ascending=False)
            st.write(survival_percentage_df) 
            # 18
            st.write("**18. What percentage of passengers survived per class?**")
            code = """SELECT pclass,
                      ROUND((COUNT(*) / AVG(c.count)) * 100, 1) AS percent_survived
                      FROM titanic
                      LEFT JOIN (SELECT pclass, COUNT(pclass) FROM titanic GROUP BY pclass) AS c
                      USING(pclass)
                      GROUP BY pclass, survived
                      HAVING survived = 1
                      ORDER BY pclass"""
            st.code(code, language='sql')
            survival_percentage = (df[df['Survived'] == 1]['Pclass'].value_counts() / df['Pclass'].value_counts()) * 100
            survival_percentage_df = survival_percentage.reset_index()
            survival_percentage_df.columns = ['Pclass', 'Survived Percentage']
            survival_percentage_df['Survived Percentage'] = survival_percentage_df['Survived Percentage'].round(1)
            survival_percentage_df = survival_percentage_df.sort_values(by='Pclass')
            st.write(survival_percentage_df)
            # 19
            st.write("**19. Count the number of passengers by their title.**")
            code = """WITH total AS (SELECT COUNT(*) FROM titianic)
                      SELECT REPLACE(SUBSTRING(name, ', [a-zA-Z]*'), ', ', '') AS title,
                      COUNT(*) AS frequency, 
                      CONCAT(ROUND((COUNT(*) / avg(total.count)) * 100, 2), '%') AS relative_frequency
                      FROM titanic, total
                      GROUP BY title
                      ORDER BY frequency DESC"""
            st.code(code, language='sql')
            df['Title'] = df['Name'].apply(lambda x: re.search(r', (\w+)', x).group(1) if re.search(r', (\w+)', x) else 'Unknown')
            title_counts = df['Title'].value_counts().reset_index()
            title_counts.columns = ['Title', 'Frequency']
            total_passengers = len(df) 
            title_counts['Relative_Frequency'] = title_counts['Frequency'].apply(lambda x: f"{round((x / total_passengers) * 100, 2)}%")
            st.write(title_counts)
            # 20
            st.write("**20. What is the average fare, number of passengers and total fare earned per class?**")
            code = """SELECT pclass,
            ROUND(AVG(fare)::DECIMAL, 2) AS average_fare,
            TRUNC(AVG(c.count)) AS passenger_count,
            ROUND(SUM(fare)::DECIMAL, 2) AS total_fare
            FROM titanic
            LEFT JOIN (SELECT pclass, COUNT(pclass) FROM titanic GROUP BY pclass) AS c 
            USING(pclass)
            GROUP BY pclass
            ORDER BY pclass"""
            st.code(code, language='sql')
            pclass_stats = df.groupby('Pclass').agg(
                average_fare=('Fare', lambda x: round(x.mean(), 2)),
                passenger_count=('Pclass', 'count'),
                total_fare=('Fare', lambda x: round(x.sum(), 2))
                ).reset_index()
            pclass_stats = pclass_stats.sort_values(by='Pclass')
            st.write(pclass_stats)
            # 21
            st.write("**21. Did having a sibling/spouse aboard help the passengers survive?**")
            code = """SELECT sibsp, 
            ROUND((COUNT(*) / AVG(c.count)) * 100, 1) AS percent_survived
            FROM titanic
            LEFT JOIN (SELECT sibsp, COUNT(sibsp) FROM titanic GROUP BY sibsp) AS c
            USING(sibsp)
            GROUP BY sibsp, survived
            HAVING survived = 1
            ORDER BY percent_survived DESC"""
            st.code(code, language='sql')
            sibsp_counts = df.groupby('SibSp').size().reset_index(name='Total_Count')
            sibsp_survivors = df[df['Survived'] == 1].groupby('SibSp').size().reset_index(name='Survived_Count')
            sibsp_stats = sibsp_counts.merge(sibsp_survivors, on='SibSp', how='left').fillna(0)
            sibsp_stats['Percent Survived'] = round((sibsp_stats['Survived_Count'] / sibsp_stats['Total_Count']) * 100, 1)
            sibsp_stats = sibsp_stats[['SibSp', 'Percent Survived']].sort_values(by='Percent Survived', ascending=False)
            st.write(sibsp_stats)
            # 22
            st.write("**22. What is the count of passengers per cabin code (first character in the cabin string)?**")
            code = """SELECT LEFT(cabin, 1) AS cabin_code,
                      COUNT(*)
                      FROM titanic
                      WHERE cabin IS NOT NULL
                      GROUP BY cabin_code
                      ORDER BY count DESC"""
            st.code(code, language='sql')
            df_cabin = df[df['Cabin'].notna()]
            df_cabin['Cabin Code'] = df_cabin['Cabin'].astype(str).str[0] 
            cabin_counts = df_cabin['Cabin Code'].value_counts().reset_index()
            cabin_counts.columns = ['Cabin Code', 'Count']
            cabin_counts = cabin_counts.sort_values(by='Count', ascending=False)
            st.write(cabin_counts)
        elif selected == "#2 Tennis country club":
            st.write("The datasets are for a newly created country club, "
                     "with a set of members, facilities such as "
                     "tennis courts, and booking history for those facilities. Amongst other things, the club wants to understand "
                     "how they can use their information to analyse facility usage/demand.")
            st.subheader("Members table:")
            df_m = pd.read_csv('datasets/members.csv')
            st.write(df_m)
            st.write("I'll start off with a look at the Members table. Each member has an ID, basic address information, a reference to the "
                     "member that recommended them (if any), and a timestamp for when they joined. The addresses in the "
                     "dataset are entirely (and unrealistically) fabricated.")
            st.subheader("Facilities table:")
            df_f = pd.read_csv('datasets/facilities.csv')
            st.write(df_f)
            st.write("The facilities table lists all the bookable facilities that the country club possesses. The club stores id/name "
                     "information, the cost to book both members and guests, the initial cost to build the facility, and estimated "
                     "monthly upkeep costs. The main purpose of this table is to track how financially worthwhile each facility is.")
            st.subheader("Table tracking bookings of facilities:")
            df_b = pd.read_csv('datasets/bookings.csv')
            st.write(df_b)
            st.write("Finally, there's a table tracking bookings of facilities. This stores the facility id, the member who made the "
                     "booking, the start of the booking, and how many half hour 'slots' the booking was made for.")
            selected_options = ["Simple SQL Queries",
                                "Joins and Subqueries",
                                "Modifying Data",
                                "Aggregation",
                                "Working with Timestamps",
                                "String Operations"]
            selected = st.selectbox("Choose the topic you are intered in:", options = selected_options)
            if selected == "Simple SQL Queries":
                st.subheader("Simple SQL Queries B")
                # 1
                st.write("**1. Retrieve everything from a facilities table.**")
                code = '''SELECT * FROM cd.facilities'''
                st.code(code, language='sql')
                st.write(df_f)
                # 2
                st.write("**2. Print out a list of all of the facilities and their cost to members.**")
                code = '''SELECT name, membercost FROM cd.facilities'''
                st.code(code, language='sql')
                st.write(df_f[['name', 'membercost']])
                # 3
                st.write("**3. Produce a list of facilities that charge a fee to members?**")
                code = '''SELECT * FROM cd.facilities WHERE membercost != 0'''
                st.code(code, language='sql')
                st.write(df_f[df_f['membercost'] != 0])
                # 4
                st.write("**4. Produce a list of facilities that charge a fee to members, and that fee is less than 1/50th "
                     "of the monthly maintenance cost? Return the facid, facility name, member cost, and monthly maintenance of "
                     "the facilities in question.**")
                code = '''SELECT facid, name, membercost, monthlymaintenance 
                FROM cd.facilities 
                WHERE membercost > 0 AND membercost < monthlymaintenance/50.0'''
                st.code(code, language='sql')
                st.write(df_f[(df_f['membercost'] > 0) & (df_f["membercost"] < df_f["monthlymaintenance"]/50)])
                # 5
                st.write("**5. Produce a list of all facilities with the word 'Tennis' in their name?**")
                code = '''SELECT * FROM cd.facilities WHERE name LIKE '%Tennis%'''
                st.code(code, language='sql')
                st.write(df_f[df_f['name'].str.contains('Tennis', case=False)])
                # 6
                st.write("**6. Retrieve the details of facilities with ID 1 and 5. Do it without using the OR operator.**")
                code = '''SELECT * FROM cd.facilities WHERE facid IN (1, 5)'''
                st.code(code, language='sql')
                st.write(df_f[df_f['facid'].isin([1, 5])])
                # 7
                st.write("**7. Produce a list of facilities, with each labelled as 'cheap' or 'expensive' depending on if their monthly "
                     "maintenance cost is more than $100? Return the name and monthly maintenance of the facilities in question.**")
                code = '''SELECT name, 
                CASE 
                WHEN monthlymaintenance > 100 THEN 'expensive' 
                ELSE 'cheap' 
                END AS cost     
                FROM cd.facilities'''
                st.code(code, language='sql')
                df_filtered = df_f.assign(cost=df_f['monthlymaintenance'].apply(lambda x: 'expensive' if x > 100 else 'cheap'))[['name', 'cost']]
                st.write(df_filtered)
                # 8
                st.write("**8. Produce a list of members who joined after the start of September 2012. Return the memid, surname, firstname, "
                     "and joindate of the members in question.**")
                code = """SELECT memid, surname, firstname, joindate 
                FROM cd.members
                WHERE joindate >= '2012-09-01'"""
                st.code(code, language='sql')
                df_filtered = df_m[df_m['joindate'] >= '9/1/2012']
                st.write(df_filtered[['memid', 'surname', 'firstname', 'joindate']])
                # 9
                st.write("**9.  Produce an ordered list of the first 10 surnames in the members table. The list must not contain duplicates.**")
                code = """SELECT DISTINCT surname 
                FROM cd.members 
                ORDER BY surname ASC 
                LIMIT 10"""
                st.code(code, language='sql')
                st.write(df_m['surname'].drop_duplicates().sort_values().head(10))
                # 10
                st.write("**10. You, for some reason, want a combined list of all surnames and all facility names. Produce that list! :)**")
                code = """SELECT surname
                FROM cd.members
                UNION
                SELECT name
                FROM cd.facilities"""
                st.code(code, language='sql')
                st.write(pd.concat([df_m['surname'], df_f['name']]).drop_duplicates())
                # 11
                st.write("**11. Get the signup date of your last member.**")
                code = """SELECT max(joindate) AS latest FROM cd.members"""
                st.code(code, language='sql')
                latest_signup = pd.DataFrame({'joindate': ['9/26/2012 18:08']})
                st.table(latest_signup)
                # 12
                st.write("**12. Get the first and last name of the last member(s) who signed up - not just the date.**")
                code = """SELECT firstname, surname, joindate
                FROM cd.members
                WHERE joindate = (SELECT MAX(joindate) FROM cd.members)"""
                st.code(code, language='sql')
                latest_signup_name = pd.DataFrame({
                'firstname': ['Drarren'],
                'surname': ['Smith'], 
                'joindate': ['9/26/2012 18:08']})
                st.table(latest_signup_name)
            
            elif selected == "Joins and Subqueries":
                st.subheader("Joins and Subqueries")
                # 1
                st.write("**1. Produce a list of the start times for bookings by members named 'David Farrell'?**")
                code = """SELECT bks.starttime 
                FROM cd.bookings bks
                INNER JOIN cd.members mems
                ON bks.memid = mems.memid
                WHERE mems.firstname = 'David' AND mems.surname = 'Farrell'"""
                st.code(code, language='sql')
                df_merged = pd.merge(df_b, df_m, on='memid')
                df_result = df_merged[(df_merged['firstname'] == 'David') & (df_merged['surname'] == 'Farrell')]
                st.write(df_result[['starttime']])
                # 2
                st.write("**2. Produce a list of the start times for bookings for tennis courts, for the date '2012-09-21'. "
                     "Return a list of start time and facility name pairings, ordered by the time.**")
                code = ("""SELECT bks.starttime as start, facs.name as name
                FROM 
                cd.facilities facs
                INNER JOIN cd.bookings bks
                ON facs.facid = bks.facid
                WHERE bks.starttime >= '2012-09-21' 
                AND bks.starttime < '2012-09-22'
                AND facs.name LIKE 'Tennis Court%'
                ORDER BY bks.starttime
                """)
                st.code(code, language='sql')
                df_merged = pd.merge(df_b, df_f, on='facid')
                df_merged['starttime'] = pd.to_datetime(df_merged['starttime'])
                df_result = df_merged[
                (df_merged['starttime'] >= '2012-09-21') & 
                (df_merged['starttime'] < '2012-09-22') & 
                (df_merged['name'].str.contains('Tennis Court'))]
                df_result = df_result.sort_values(by='starttime')
                st.table(df_result[['starttime', 'name']])
                # 3
                st.write("**3. Output a list of all members who have recommended another member. Ensure that there are "
                     "no duplicates in the list, and that results are ordered by (surname, firstname).**")
                code = """SELECT DISTINCT recs.firstname as firstname, recs.surname as surname
                FROM	
                cd.members mems
                INNER JOIN cd.members recs
                ON recs.memid = mems.recommendedby
                ORDER BY surname, firstname"""
                st.code(code, language='sql')
                df_recommended = df_m.merge(df_m, left_on='recommendedby', right_on='memid', suffixes=('', '_recommender'))
                df_recommended = df_recommended[['firstname_recommender', 'surname_recommender']].drop_duplicates()
                df_recommended.columns = ['firstname', 'surname']
                df_recommended = df_recommended.sort_values(by=['surname', 'firstname'])
                st.table(df_recommended)
                # 4
                st.write("**4. Output a list of all members, including the individual who recommended them (if any)."
                     "Ensure that results are ordered by (surname, firstname).**")
                code = """SELECT mems.firstname as memfname, mems.surname as memsname, 
                recs.firstname as recfnaem, recs.surname as recsname
                FROM 
                cd.members mems
                LEFT OUTER JOIN cd.members recs
                ON recs.memid = mems.recommendedby
                ORDER BY memsname, memfname"""
                st.code(code, language='sql')
                df_joined = df_m.merge(df_m, how='left', left_on='recommendedby', right_on='memid', suffixes=('_member', '_recommender'))
                df_result = df_joined[['firstname_member', 'surname_member', 'firstname_recommender', 'surname_recommender']]
                df_result.columns = ['memfname', 'memsname', 'recfname', 'recsname'] 
                df_result = df_result.sort_values(by=['memsname', 'memfname'])
                st.table(df_result)
                # 5
                st.write("**5. Produce a list of all members who have used a tennis court. Include in your output the name "
                     "of the court, and the name of the member formatted as a single column. Ensure no duplicate data, and "
                     "order by the member name.**")
                code = """SELECT DISTINCT mems.firstname || ' ' || mems.surname as member, facs.name as facility
                FROM
                cd.members mems
                INNER JOIN cd.bookings bks
                ON mems.memid = bks.memid
                INNER JOIN cd.facilities facs
                ON bks.facid = facs.facid
                WHERE bks.facid IN (0,1)
                ORDER BY member, facility"""
                st.code(code, language='sql')
                df_merged = df_m.merge(df_b, on='memid').merge(df_f, on='facid')
                df_tennis = df_merged[df_merged['facid'].isin([0, 1])]
                df_tennis['member'] = df_tennis['firstname'] + ' ' + df_tennis['surname']
                df_result = df_tennis[['member', 'name']].drop_duplicates().sort_values(by=['member', 'name'])
                df_result.columns = ['member', 'facilities']
                st.table(df_result)
                # 6
                st.write("**6. Produce a list of bookings on the day of 2012-09-14 which will cost the member (or guest) "
                     "more than $30? Remember that guests have different costs to members (the listed costs are per half-hour "
                     "'slot'), and the guest user is always ID 0. Include in your output the name of the facility, the name of the "
                     "member formatted as a single column, and the cost. Order by descending cost, and do not use any subqueries.**")
                code = """SELECT mems.firstname || ' ' || mems.surname as member, facs.name as facility, 
                CASE 
                WHEN mems.memid = 0 THEN
                    bks.slots*facs.guestcost
                ELSE
                    bks.slots*facs.membercost
                END AS cost
                FROM
                cd.members mems                
                INNER JOIN cd.bookings bks
                        ON mems.memid = bks.memid
                INNER JOIN cd.facilities facs
                        ON bks.facid = facs.facid
                WHERE
		        bks.starttime >= '2012-09-14' AND
		        bks.starttime < '2012-09-15' AND (
			        (mems.memid = 0 and bks.slots*facs.guestcost > 30) OR
			        (mems.memid != 0 and bks.slots*facs.membercost > 30)
                )
                ORDER BY cost DESC"""
                st.code(code, language='sql') 
                df_merged = df_b.merge(df_m, on='memid').merge(df_f, on='facid')
                df_merged['starttime'] = pd.to_datetime(df_merged['starttime'])
                df_filtered = df_merged[(df_merged['starttime'].dt.date == pd.to_datetime('2012-09-14').date())]
                df_filtered['cost'] = df_filtered.apply(
                    lambda row: row['slots'] * row['guestcost'] if row['memid'] == 0 else row['slots'] * row['membercost'], axis=1)
                df_result = df_filtered[df_filtered['cost'] > 30]
                df_result['member'] = df_result['firstname'] + ' ' + df_result['surname']
                df_result = df_result[['member', 'name', 'cost']].rename(columns={'name': 'facility'}).sort_values(by='cost', ascending=False)
                df_result['cost'] = df_result['cost'].astype(int)
                st.table(df_result)
                # 7
                st.write("**7. Output a list of all members, including the individual who recommended them (if any), without using any joins? "
                     "Ensure that there are no duplicates in the list, and that each firstname + surname pairing is formatted as a column and ordered.**")
                code = """SELECT DISTINCT mems.firstname || ' ' ||  mems.surname as member,
                (SELECT recs.firstname || ' ' ||  recs.surname as recommender
                FROM cd.members recs 
                WHERE recs.memid = mems.recommendedby)
                FROM cd.members mems
                ORDER BY member"""
                st.code(code, language='sql')
                member_dict = df_m.set_index('memid').apply(lambda row: f"{row['firstname']} {row['surname']}", axis=1).to_dict()
                df_m['recommender'] = df_m['recommendedby'].map(member_dict)
                df_m['member'] = df_m['firstname'] + ' ' + df_m['surname']
                df_result = df_m[['member', 'recommender']].drop_duplicates().sort_values(by='member')
                st.table(df_result)
                # 8
                st.write("**8. Produce a list of bookings on the day of 2012-09-14 which will cost the member (or guest) more than $30? Remember that guests "
                     "have different costs to members (the listed costs are per half-hour 'slot'), and the guest user is always ID 0. Include in your output "
                     "the name of the facility, the name of the member formatted as a single column, and the cost. Order by descending cost.**")
                code = """SELECT member, facility, cost FROM (
                SELECT
  		        mems.firstname || ' ' || mems.surname as member,
  		        facs.name as facility,
  		        CASE 
  			        WHEN mems.memid = 0 THEN
  				    bks.slots*facs.guestcost
  		    	ELSE
  				    bks.slots*facs.membercost
  		        END AS cost
  		        FROM
  			    cd.members mems
  			    INNER JOIN cd.bookings bks
  				    ON mems.memid = bks.memid
  			    INNER JOIN cd.facilities facs
  				    ON facs.facid = bks.facid
  		        WHERE
  			    bks.starttime >= '2012-09-14' AND
			    bks.starttime < '2012-09-15'
                ) AS bookings
                WHERE COST > 30
                ORDER BY cost DESC
                """
                st.code(code, language='sql')
                df_m["member"] = df_m["firstname"] + " " + df_m["surname"]
                df_b["starttime"] = pd.to_datetime(df_b["starttime"])
                df_filtered = df_b[(df_b["starttime"] >= "2012-09-14") & (df_b["starttime"] < "2012-09-15")]
                df_merged = df_filtered.merge(df_m, on="memid", how="left").merge(df_f, on="facid", how="left")
                df_merged["cost"] = df_merged.apply(
                    lambda row: row["slots"] * row["guestcost"] if row["memid"] == 0 else row["slots"] * row["membercost"], axis=1
                    )
                df_result = df_merged[df_merged["cost"] > 30][["member", "name", "cost"]].rename(columns={"name": "facility"})
                df_result = df_result.sort_values(by="cost", ascending=False)
                df_result['cost'] = df_result['cost'].astype(int)
                st.table(df_result)

            elif selected == "Modifying Data":
                st.subheader("Modifying Data")
                # 1
                st.write("**1. The club is adding a new facility - a spa. You need to add it into the facilities table. Use the following values:**")
                st.markdown(""" 
                - **facid:** 9, **Name:** 'Spa', **membercost:** 20, **guestcost:** 30, **initialoutlay:** 100000, **monthlymaintenance:** 800""")
                code = """INSERT INTO cd.facilities (facid, name, membercost, 
                guestcost, initialoutlay, monthlymaintenance)
                VALUES ('9', 'Spa', '20', '30', '100000', '800')"""
                st.code(code, language='sql')
                new_facility = pd.DataFrame([{
                "facid": 9,
                "name": "Spa",
                "membercost": 20,
                "guestcost": 30,
                "initialoutlay": 100000,
                "monthlymaintenance": 800
                }])
                df_f = pd.concat([df_f, new_facility], ignore_index=True)
                st.table(df_f)
                # 2
                st.write("**2. Now add multiple facilities in one command. Use the following values:**")
                st.markdown(""" 
                - **facid:** 9, **Name:** 'Spa', **membercost:** 20, **guestcost:** 30, **initialoutlay:** 100000, **monthlymaintenance:** 800
                - **facid:** 10, **Name:** 'Squash Court 2', **membercost:** 3.5, **guestcost:** 17.5, **initialoutlay:** 5000, **monthlymaintenance:** 80""")
                code = """INSERT INTO cd.fascilities (facid, name, membercost, guestcost, initialoutlay, monthlymaintenance)
                VALUES (9, 'Spa', 20, 30, 100000, 800),
                   (10, 'Squash Court 2', 3.5, 17.5, 5000, 80)"""
                st.code(code, language='sql')
                new_facilities = pd.DataFrame([{
                "facid": 9,
                "name": "Spa",
                "membercost": 20,
                "guestcost": 30,
                "initialoutlay": 100000,
                "monthlymaintenance": 800
                },
                {"facid": 10,
                 "name": "Squash Court 2",
                 "membercost": 3.5,
                 "guestcost": 17.5,
                 "initialoutlay": 5000,
                 "monthlymaintenance": 80
                    }])
                df_f = pd.concat([df_f, new_facilities], ignore_index=True)
                st.table(df_f)
                # 3
                st.write("**3. Add the spa to the facilities table again. This time, though, we want to automatically generate the value for the next facid, "
                     "rather than specifying it as a constant. Use the following values for everything else:**")
                st.markdown("""
                - **Name:** 'Spa', **membercost:** 20, **guestcost:** 30, **initialoutlay:** 100000, **monthlymaintenance:** 800.""")
                code = """
                INSERT INTO cd.facilities (facid, name, membercost, 
                        guestcost, initialoutlay, monthlymaintenance)
                        SELECT (SELECT max(facid) FROM cd.facilities)+1, 'Spa', '20', '30', '100000', '800'
                """
                st.code(code, language='sql')
                next_facid = df_f["facid"].max() + 1
                new_facility = pd.DataFrame([{
                "facid": next_facid,
                "name": "Spa",
                "membercost": 20,
                "guestcost": 30,
                "initialoutlay": 100000,
                "monthlymaintenance": 800
                }])
                df_f = pd.concat([df_f, new_facility], ignore_index=True)
                st.table(df_f)
                # 4
                st.write("**4. We made a mistake when entering the data for the second tennis court. The initial outlay was 10000 rather than 8000: you need to alter the data to fix the error.**")
                code = """UPDATE cd.facilities
                      SET initialoutlay = 10000
                      WHERE facid = 1"""
                st.code(code, language='sql')
                df_f.loc[df_f["facid"] == 1, "initialoutlay"] = 10000
                st.table(df_f)
                # 5
                st.write("**5. We want to increase the price of the tennis courts for both members and guests. Update the costs to be 6 for members, and 30 for guests.**")
                code = """UPDATE cd.facilities 
                SET 
                membercost = 6,
                guestcost = 30 
                WHERE facid IN (0,1)"""
                st.code(code, language='sql')
                df_f.loc[df_f["facid"].isin([0, 1]), ["membercost", "guestcost"]] = [6, 30]
                st.table(df_f)
            
            elif selected == "Aggregation":
                st.subheader("Aggregation")
                # 1
                st.write("**1. We want to know how many facilities exist - simply produce a total count.**")
                code = '''SELECT count(*) FROM cd.facilities'''
                st.code(code, language='sql')
                count_value = 9
                df = pd.DataFrame({'count': [count_value]})
                st.dataframe(df)
                # 2
                st.write("**2. Produce a count of the number of facilities that have a cost to guests of 10 or more.**")
                code = """SELECT count(*) FROM cd.facilities
                WHERE guest_cost >= 10"""
                st.code(code, language='sql')
                filtered_count = 6
                df = pd.DataFrame({'count': [filtered_count]})
                st.dataframe(df)
                # 3
                st.write("**3. Produce a count of the number of recommendations each member has made. Order by member ID.**")
                code = """SELECT recommendedby, count(*) 
                FROM cd.members 
                WHERE recommendedby IS NOT NULL
                GROUP BY recommendedby
                ORDER BY recommendedby"""
                st.code(code, language='sql')
                filtered_df = df_m[df_m['recommendedby'].notna()]
                grouped_df = filtered_df.groupby('recommendedby').size().reset_index(name='count')
                grouped_df = grouped_df.sort_values(by='recommendedby')
                st.dataframe(grouped_df)
                # 4
                st.write("**4. Produce a list of the total number of slots booked per facility. For now, just produce an output table consisting of facility id and slots, sorted by facility id.**")
                code = """SELECT facid, sum(slots) as "Total Slots" 
                FROM cd.bookings 
                GROUP BY facid 
                ORDER by facid
                """
                st.code(code, language='sql')   
                grouped_df = df_b.groupby('facid')['slots'].sum().reset_index()
                grouped_df = grouped_df.rename(columns={'slots': 'Total Slots'})
                grouped_df = grouped_df.sort_values(by='facid')
                st.dataframe(grouped_df)
                # 5
                st.write("**5. Produce a list of the total number of slots booked per facility in the month of September 2012. Produce an output table consisting of facility id and slots, sorted by the number of slots.**")
                code = """SELECT facid, sum(slots) as "Total Slots"
                FROM cd.bookings
                WHERE starttime >= '2012-09-01' AND starttime < '2012-10-01'
                GROUP BY facid
                ORDER BY sum(slots)"""
                st.code(code, language='sql')
                df_b['starttime'] = pd.to_datetime(df_b['starttime'])
                start_date = pd.to_datetime('2012-09-01')
                end_date = pd.to_datetime('2012-10-01')
                filtered_df = df_b[(df_b['starttime'] >= start_date) & (df_b['starttime'] < end_date)]
                grouped_df = (
                    filtered_df.groupby('facid')['slots']
                    .sum()
                    .reset_index()
                    .rename(columns={'slots': 'Total Slots'})
                    .sort_values(by='Total Slots')
                )
                st.dataframe(grouped_df)
                # 6
                st.write("**6. Produce a list of the total number of slots booked per facility per month in the year of 2012. Produce an output table consisting of facility id and slots, sorted by the id and month.**")
                code = """SELECT facid, extract(month FROM starttime) as month, sum(slots) as "Total Slots"
                FROM cd.bookings
                WHERE extract(year FROM starttime) = 2012
                GROUP BY facid, month
                ORDER BY facid, month
                """
                st.code(code, language='sql')
                df_b['starttime'] = pd.to_datetime(df_b['starttime'])
                df_2012 = df_b[df_b['starttime'].dt.year == 2012].copy()
                df_2012['month'] = df_2012['starttime'].dt.month
                grouped_df = (
                    df_2012.groupby(['facid', 'month'])['slots']
                    .sum()
                    .reset_index()
                    .rename(columns={'slots': 'Total Slots'})
                    .sort_values(by=['facid', 'month'])
                )
                st.dataframe(grouped_df)
                # 7
                st.write("**7. Find the total number of members (including guests) who have made at least one booking.**")
                code = """SELECT count(DISTINCT memid) FROM cd.bookings"""
                st.code(code, language='sql')
                unique_members = df_b['memid'].nunique()
                df_result = pd.DataFrame({'Unique Members': [unique_members]})
                st.dataframe(df_result)
                # 8
                st.write("**8. Produce a list of facilities with more than 1000 slots booked. Produce an output table consisting of facility id and slots, sorted by facility id.**")
                code = """SELECT facid, sum(slots) as "Total Slots"
                FROM cd.bookings
                GROUP BY facid
                HAVING sum(slots) > 1000
                ORDER BY facid
                """
                st.code(code, language='sql')
                grouped_df = df_b.groupby('facid')['slots'].sum().reset_index()
                filtered_df = grouped_df[grouped_df['slots'] > 1000]
                filtered_df = filtered_df.rename(columns={'slots': 'Total Slots'})
                filtered_df = filtered_df.sort_values(by='facid')
                st.dataframe(filtered_df)
                # 9
                st.write("**9. Produce a list of facilities along with their total revenue. The output table should consist of facility name and revenue, sorted by revenue. Remember that there's a different cost for guests and members!**")
                code = """SELECT facs.name, sum(slots * case
			    when memid = 0 then facs.guestcost
			    else facs.membercost
		        end) as revenue
	            FROM cd.bookings bks
	            INNER JOIN cd.facilities facs
		        ON bks.facid = facs.facid
	            GROUP BY facs.name
                    ORDER BY revenue"""
                st.code(code, language='sql')
                merged_df = pd.merge(df_b, df_f, on='facid', how='inner')
                merged_df['cost_per_slot'] = merged_df.apply(
                    lambda row: row['guestcost'] if row['memid'] == 0 else row['membercost'], axis=1
                )
                merged_df['revenue'] = merged_df['slots'] * merged_df['cost_per_slot']
                result_df = merged_df.groupby('name')['revenue'].sum().reset_index()
                result_df = result_df.sort_values(by='revenue')
                st.dataframe(result_df)
                # 10
                st.write("**10. Produce a list of facilities with a total revenue less than 1000. Produce an output table consisting of facility name and revenue, sorted by revenue. Remember that there's a different cost for guests and members!**")
                code = """SELECT name, revenue FROM (
	                SELECT facs.name, sum(case 
				when memid = 0 then slots * facs.guestcost
				else slots * membercost
			        end) AS revenue
		        FROM cd.bookings bks
		        INNER JOIN cd.facilities facs
			    ON bks.facid = facs.facid
		        GROUP BY facs.name
	                ) AS agg WHERE revenue < 1000
                        ORDER BY revenue"""
                st.code(code, language='sql')
                merged_df = pd.merge(df_b, df_f, on='facid', how='inner')
                merged_df['cost_per_slot'] = merged_df.apply(
                    lambda row: row['guestcost'] if row['memid'] == 0 else row['membercost'],
                    axis=1
                )
                merged_df['revenue'] = merged_df['slots'] * merged_df['cost_per_slot']
                agg_df = merged_df.groupby('name')['revenue'].sum().reset_index()
                filtered_df = agg_df[agg_df['revenue'] < 1000]
                filtered_df = filtered_df.sort_values(by='revenue')
                st.dataframe(filtered_df)

            elif selected == "Working with Timestamps":
                st.subheader("Working with Timestamps")
                # 1
                st.write("**1. Produce a timestamp for 1 a.m. on the 31st of August 2012.**")
                code = """SELECT timestamp '2012-08-31 01:00:00"""
                st.code(code, language='sql')
                dt = pd.to_datetime('2012-08-31 01:00:00')
                st.write("Selected timestamp:", dt)
                # 2
                st.write("**2. Find the result of subtracting the timestamp '2012-07-30 01:00:00' from the timestamp '2012-08-31 01:00:00'**")
                code = '''SELECT timestamp '2012-08-31 01:00:00' - timestamp '2012-07-30 01:00:00' as interval'''
                st.code(code, language='sql')
                dt1 = pd.to_datetime('2012-08-31 01:00:00')
                dt2 = pd.to_datetime('2012-07-30 01:00:00')
                interval = dt1 - dt2
                st.write(f"Interval between dates: {interval}")
                # 3
                st.write("**3. Produce a list of all the dates in October 2012. They can be output as a timestamp (with time set to midnight) or a date.**")
                code = '''SELECT generate_series(timestamp '2012-10-01', timestamp '2012-10-31', interval '1 day') as ts'''
                st.code(code, language='sql')
                date_range = pd.date_range(start='2012-10-01', end='2012-10-31', freq='D')
                df_dates = pd.DataFrame({'ts': date_range})
                st.dataframe(df_dates)
                # 4
                st.write("**4. Get the day of the month from the timestamp '2012-08-31' as an integer.**")
                code = '''SELECT extract(day FROM timestamp '2012-08-31')'''
                st.code(code, language='sql')
                dt = pd.to_datetime('2012-08-31')
                day = dt.day
                st.write(f"Day of the month: {day}")
                # 5
                st.write("**5. Work out the number of seconds between the timestamps '2012-08-31 01:00:00' and '2012-09-02 00:00:00'**")
                code = '''SELECT extract(epoch FROM (timestamp '2012-09-02 00:00:00' - '2012-08-31 01:00:00'))'''
                st.code(code, language='sql')
                dt1 = pd.to_datetime('2012-09-02 00:00:00')
                dt2 = pd.to_datetime('2012-08-31 01:00:00')
                diff = dt1 - dt2
                seconds = diff.total_seconds()
                st.write(f"Difference in seconds: {seconds}")
                # 6
                st.write("**6. For each month of the year in 2012, output the number of days in that month. " \
                "Format the output as an integer column containing the month of the year, and a second column containing an interval data type.**")
                code = '''SELECT extract(month from cal.month) AS month,
                        (cal.month + interval '1 month') - cal.month AS length
                        FROM
                        (
                            SELECT generate_series(timestamp '2012-01-01', timestamp '2012-12-01', interval '1 month') AS month
                        ) cal
                        ORDER BY month;'''
                st.code(code, language='sql')
                months = pd.date_range(start='2012-01-01', end='2012-12-01', freq='MS')  
                df = pd.DataFrame({'month': months})
                df['month_number'] = df['month'].dt.month
                df['length'] = (df['month'] + pd.offsets.MonthBegin(1)) - df['month']
                df['length'] = df['length'].dt.days
                st.dataframe(df[['month_number', 'length']].sort_values(by='month_number'))
                # 7
                st.write("**7. For any given timestamp, work out the number of days remaining in the month. " \
                "The current day should count as a whole day, regardless of the time. Use '2012-02-11 01:00:00' " \
                "as an example timestamp for the purposes of making the answer. Format the output as a single interval value.**")
                code = '''SELECT (date_trunc('month',ts.testts) + interval '1 month') - date_trunc('day', ts.testts) AS remaining
	                        FROM (select timestamp '2012-02-11 01:00:00' AS testts) ts  '''
                st.code(code, language='sql')
                testts = pd.to_datetime('2012-02-11 01:00:00')
                month_start = testts.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                next_month_start = month_start + pd.offsets.MonthBegin(1)
                day_start = testts.replace(hour=0, minute=0, second=0, microsecond=0)
                remaining = next_month_start - day_start
                st.write(f"Time remaining until the end of the month from the beginning of the day {testts.date()}: {remaining}")
                # 8
                st.write("**8. Return a count of bookings for each month, sorted by month**")
                code = '''SELECT date_trunc('month', starttime) as month, COUNT(*)
                            FROM cd.bookings
                            GROUP BY month
                            ORDER by month '''
                st.code(code, language='sql')
                df_b['starttime'] = pd.to_datetime(df_b['starttime'], errors='coerce')
                df_b['month'] = df_b['starttime'].dt.to_period('M').dt.to_timestamp()
                monthly_counts = df_b.groupby('month').size().reset_index(name='booking_count')
                monthly_counts = monthly_counts.sort_values(by='month')
                st.dataframe(monthly_counts)

            elif selected == "String Operations":
                st.subheader("String Operations")
                # 1
                st.write("**1. Output the names of all members, formatted as 'Surname, Firstname'**")
                code = '''SELECT surname || ', ' || firstname AS name from cd.members'''
                st.code(code, language='sql')
                df_m['name'] = df_m['surname'] + ', ' + df_m['firstname']
                st.dataframe(df_m[['name']])
                # 2
                st.write("**2. Find all facilities whose name begins with 'Tennis'. Retrieve all columns.**")
                code = '''SELECT * FROM cd.facilities WHERE name LIKE 'Tennis%'  '''
                st.code(code, language='sql')
                tennis_facilities = df_f[df_f['name'].str.startswith('Tennis', na=False)]
                st.dataframe(tennis_facilities)
                # 3
                st.write("**3. Perform a case-insensitive search to find all facilities whose name begins with 'tennis'. Retrieve all columns.**")
                code = '''SELECT * FROM cd.facilities WHERE upper(name) LIKE 'TENNIS%'; '''
                st.code(code, language='sql')
                tennis_facilities = df_f[df_f['name'].str.upper().str.startswith('TENNIS', na=False)]
                st.dataframe(tennis_facilities)
                # 4
                st.write("**4. You've noticed that the club's member table has telephone numbers with very inconsistent formatting. " \
                "You'd like to find all the telephone numbers that contain parentheses, returning the member ID and telephone number sorted by member ID.**")
                code = """SELECT memid, telephone FROM cd.members WHERE telephone ~ '[()]'"""
                st.code(code, language='sql')
                pattern = r"[()]"
                df_filtered = df_m[df_m['telephone'].str.contains(pattern, na=False, regex=True)]
                st.dataframe(df_filtered[['memid', 'telephone']])
                # 5
                st.write("**5. The zip codes in our example dataset have had leading zeroes removed from them by virtue of being stored as a numeric type. " \
                "Retrieve all zip codes from the members table, padding any zip codes less than 5 characters long with leading zeroes. Order by the new zip code.**")
                code = '''SELECT lpad(cast(zipcode as char(5)),5,'0') zip FROM cd.members ORDER BY zip'''
                st.code(code, language='sql')
                df_m['zip'] = df_m['zipcode'].astype(str).str.zfill(5)
                df_sorted = df_m.sort_values(by='zip')
                st.dataframe(df_sorted[['zip']])
                # 6
                st.write("**6. You'd like to produce a count of how many members you have whose surname starts with each letter of the alphabet. " \
                "Sort by the letter, and don't worry about printing out a letter if the count is 0.**")
                code = '''SELECT substr (mems.surname,1,1) AS letter, COUNT(*) AS count 
                            FROM cd.members mems
                            GROUP BY letter
                            ORDER BY letter'''
                st.code(code, language='sql')
                df_m['letter'] = df_m['surname'].str[0]
                letter_counts = df_m.groupby('letter').size().reset_index(name='count')
                letter_counts = letter_counts.sort_values(by='letter')
                st.dataframe(letter_counts)
                # 7
                st.write("**7. The telephone numbers in the database are very inconsistently formatted. You'd like to print a list of member " \
                "ids and numbers that have had '-','(',')', and ' ' characters removed. Order by member id.**")
                code = '''SELECT memid, translate(telephone, '-() ', '') AS telephone
                            FROM cd.members
                            ORDER BY memid;  '''
                st.code(code, language='sql')
                df_m['telephone_clean'] = df_m['telephone'].str.replace(r'[-() ]', '', regex=True)
                df_sorted = df_m.sort_values(by='memid')
                st.dataframe(df_sorted[['memid', 'telephone_clean']])

    # elif selected == "BigQuery":
        # st.write("**SQL to MongoDB**")

    elif selected == "SQLite":
        st.subheader("SQLite")
        selected_options = ["#1. Movie Insights (basics)",
                            "#2. Movie Insights (joins)",
                            "#3. Pizza Sales Analysis"]
        selected = st.selectbox("Choose a project:", options = selected_options)
        if selected == "#1. Movie Insights (basics)":
            st.subheader("Movie Insights (basics)")
            df = pd.read_csv('datasets/Movies.csv')
            st.write(df)
            st.write("**Here is a description of each column in this dataset:**")
            st.write("üìå **Id** - Unique identifier for each movie.",
                 "<br>üìå **Title** - The name of the movie.",
                 "<br>üìå **Director** - The director of the movie",
                 "<br>üìå **Year** - Year this movie was released",
                 "<br>üìå **Length_minutes** - Length of the movie in minutes",
                 unsafe_allow_html=True)
            # 1
            st.write("**1. Find a list of the names of all movies.**")
            code = """SELECT Title FROM Movies"""
            st.code(code, language='sql')
            subset_df = df["Title"]  
            st.write(subset_df)
            # 2
            st.write("2. Find the director and year of each movie.")
            code = """SELECT Title, Director, Yar FROM Movies"""
            st.code(code, language='sql')
            subset_df = df[["Title", "Director", "Year"]]
            st.write(subset_df)
            # 3
            st.write("3. Find information about each movie.")
            code = """SELECT * FROM Movies"""
            st.code(code, language='sql')
            st.write(df)
            # 4
            st.write("4. Find movie which Id is 6")
            code = """SELECT * FROM Movies WHERE Id = 6"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Id'] == 6]
            st.write(subset_df)
            # 5
            st.write("5. Find movies released from 2000 to 2010")
            code = """SELECT * FROM Movies WHERE Year BETWEEN 2000 AND 2010"""
            st.code(code, language='sql')
            subset_df = df.loc[(df['Year'] >= 2000) & (df['Year'] <= 2010)]
            subset_df = subset_df.sort_values(by='Year')
            st.write(subset_df)
            # 6
            st.write("6. Find movies which were released in 2006, 2008, 2010.")
            code = """SELECT * FROM Movies WHERE Year IN (2006, 2008, 2010)"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Year'].isin([2006, 2008, 2010])]
            st.write(subset_df)
            # 7
            st.write("7. Find movies released before 2010 (using NOT).")
            code = """SELECT Title, Director FROM Movies WHERE NOT Year >= 2010"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Year']< 2010]
            #subset_df = df[["Title", "Director", "Year"]]
            st.write(subset_df)
            # 8
            st.write("8. Find all Toy Story movies.")
            code = """SELECT * FROM Movies WHERE Title LIKE '%Toy Story%'"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Title'].str.contains('Toy Story', case=False)]
            st.write(subset_df)
            # 9
            st.write("9. Find 2 Toy Story movies in a different way.")
            code = """SELECT * FROM Movies WHERE Title LIKE '%Toy Story%' LIMIT 2"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Title'].str.contains('Toy Story', case=False)]
            subset_df = subset_df.head(2)
            st.write(subset_df)
            # 10
            st.write("10. Find all movies directed by John Lasseter.")
            code = """SELECT * FROM Movies WHERE Director = 'John Lasseter'"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Director'] == 'John Lasseter']
            st.write(subset_df)
            # 11
            st.write("11. Find all movies NOT directed by John Lasseter.")
            code = """SELECT * FROM Movies WHERE Director != 'John Lasseter'"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Director'] != 'John Lasseter']
            st.write(subset_df)
            # 12
            st.write("12. Find all WALL-* movies.")
            code = """SELECT * FROM Movies WHERE Title Like 'WALL-_'"""
            st.code(code, language='sql')
            subset_df = df.loc[df['Title'].str.contains('WALL-', case=False)]
            st.write(subset_df)
            # 13
            st.write("13. List the directors (without dubbing) in alphabetical order.")
            code = """SELECT DISTINCT Director FROM Movies ORDER BY Director ASC"""
            st.code(code, language='sql')
            subset_df = df['Director'].unique()
            subset_df.sort()
            st.write(subset_df)
            # 14
            st.write("14. List the 4 most recent movies.")
            code = """SELECT * FROM Movies ORDER BY Year DESC LIMIT 4"""
            st.code(code, language='sql')
            subset_df = df.sort_values(by='Year', ascending=False).head(4)
            st.write(subset_df)
            # 15
            st.write("15. List the 5 movies sorted alphabetically.")
            code = """SELECT * FROM Movies ORDER BY Title ASC LIMIT 5"""
            st.code(code, language='sql')
            subset_df = df.sort_values(by='Title', ascending=True).head(5)
            st.write(subset_df)
            # 16
            st.write("16. List the next 5 movies sorted alphabetically.")
            code = """SELECT * FROM Movies ORDER BY Title ASC LIMIT 5 OFFSET 5"""
            st.code(code, language='sql')
            df_sorted = df.sort_values(by='Title', ascending=True)
            subset_df = df_sorted.iloc[5:10]
            st.write(subset_df)
            
        elif selected == "#2. Movie Insights (joins)":
            st.subheader("Movie Insights (joins)")
            df_m = pd.read_csv('datasets/Movies.csv')
            df_bf = pd.read_csv('datasets/Boxoffice.csv')
            df_e = pd.read_csv('datasets/Employees.csv')
            df_b = pd.read_csv('datasets/Buildings.csv')
            # –†–æ–∑–º—ñ—â–µ–Ω–Ω—è —É –¥–≤–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
            col1, col2 = st.columns(2)
            # === LEFT COLUMN ===
            with col1:
                st.subheader("Movies Table")
                st.dataframe(df_m)
                st.markdown("**Description of each column:**")
                st.markdown(
                    """
                    - **Id**: Unique identifier for each movie  
                    - **Title**: The name of the movie  
                    - **Director**: The director of the movie  
                    - **Year**: Year the movie was released  
                    - **Length_minutes**: Length of the movie in minutes
                    """
                )
                st.markdown("---")

                st.subheader("Employees Table")
                st.dataframe(df_e)
                st.markdown("**Description of each column:**")
                st.markdown(
                    """
                    - **Role**: Job title of the employee  
                    - **Name**: Full name of the employee  
                    - **Building**: Building where the employee works  
                    - **Years_employed**: Number of years the employee has worked
                    """
                )
            # === RIGHT COLUMN ===
            with col2:
                st.subheader("Boxoffice Table")
                st.dataframe(df_bf)
                st.markdown("**Description of each column:**")
                st.markdown(
                    """
                    <ul>
                        <li style="color: black;"><b>Movie_id</b>: Unique identifier for each movie</li>
                        <li style="color: black;"><b>Rating</b>: Viewer rating (e.g., PG, R)</li>
                        <li style="color: black;"><b>Domestic_sales</b>: Revenue from local markets</li>
                        <li style="color: black;"><b>International_sales</b>: Revenue from international markets</li>
                        <li style="color: white;"><b></b></li>
                    </ul>
                    """,
                    unsafe_allow_html=True
                )
                st.markdown("---")

                st.subheader("Buildings Table")
                st.dataframe(df_b)
                st.markdown("**Description of each column:**")
                st.markdown(
                    """
                    <ul>
                        <li style="color: black;"><b>Building_names</b>: Name of the building</li>
                        <li style="color: black;"><b>Capacity</b>: Maximum number of people it can accommodate</li>
                    </ul>
                    """,
                    unsafe_allow_html=True
                )
            st.markdown("---")
            # 1
            st.write("1. Find the domestic and international sales for each movie.")
            code = """SELECT Title, Domestic_sales, International_sales FROM Movies INNER JOIN Boxoffice ON Movies.Id = Boxoffice.Movie_id"""
            st.code(code, language='sql')
            sales_df = pd.merge(df_m, df_bf, left_on='Id', right_on='Movie_id')
            sales_df = sales_df[['Title', 'Domestic_sales', 'International_sales']]
            st.dataframe(sales_df)
            # 2
            st.write("2. Display all movies with their ratings in descending order.")
            code = """SELECT Title, Rating FROM Movies INNER JOIN Boxoffice ON Movies.Id = Boxoffice.Movie_id ORDER BY Rating DESC"""
            st.code(code, language='sql')
            df2 = pd.merge(df_m, df_bf, left_on='Id', right_on='Movie_id')
            df2 = df2[['Title', 'Rating']].sort_values(by='Rating', ascending=False)
            st.dataframe(df2)
            # 3
            st.write("3. Display a list of buildings and people who live in those buildings with their occupations.")
            code = """SELECT Role, Name, Building FROM Employees WHERE Building IS NOT ''"""
            st.code(code, language='sql')
            df3 = df_e[df_e['Building'].notna()]
            st.dataframe(df3[['Role', 'Name', 'Building']])
            # 4
            st.write("4. Display a list of people and their occupations that are not attached to buildings.")
            code = """SELECT Role, Name FROM Employees WHERE Building IS ''"""
            st.code(code, language='sql')
            df4 = df_e[df_e['Building'].isna()]
            st.dataframe(df4[['Role', 'Name']])
            # 5
            st.write("5. Display all movie titles and their total box office in dollars.")
            code = """SELECT Title, (Domestic_sales + International_sales) AS Total_sales FROM Movies INNER JOIN Boxoffice ON Movies.Id = Boxoffice.Movie_id"""
            st.code(code, language='sql')
            df5 = pd.merge(df_m, df_bf, left_on='Id', right_on='Movie_id')
            df5['Total_sales'] = df5['Domestic_sales'] + df5['International_sales']
            st.dataframe(df5[['Title', 'Total_sales']])

        elif selected == "#3. Pizza Sales Analysis":
            # page settings
            page = st.sidebar.selectbox('Choose a page:',
                                            ['Main Info',
                                            'Tables Closer',
                                            'SQL Code',
                                            'Findings'])
            if page == 'Main Info':
                st.subheader("Pizza Sales Analysis")
                st.write('**On the side bar choose the page on which you can focus)**')
                st.subheader("Dataset Overview")
                st.markdown("""
                        Dataset contains valuable information that could help restaurant optimize their operations, 
                        boost sales, and enhance customer satisfaction. Here's a quick rundown of what you can expect from the dataset:

                        - **Date and Time:** The author has meticulously recorded the date and time of each order, allowing to analyze 
                        customer behavior and identify potential peak hours.
                        - **Pizza Details:** Each entry includes comprehensive details about the pizzas ordered, including their types, 
                        sizes, quantities, prices, and the tantalizing ingredients that create an unforgettable culinary experience.
                        """)
                st.subheader("Main Points")
                st.markdown("""
                            1. **Customer Traffic Analysis:** Uncovering how many customers visits each day and exploring whether certain 
                            times of day experience higher footfall.
                            2. **Bestselling Pizzas:** Analyzing the data to find out which pizzas are the most popular among the customers. 
                            Identifying bestsellers can inform restaurant's marketing strategies and help focus on promoting crowd favorites.
                            3. **Revenue and Seasonality:** Calculating the total revenue generated over the year to get an overall picture 
                            of the financial performance. Additionally, investigating whether there are any seasonal patterns in sales to plan 
                            for peak and slow periods.
                            4. **Menu Optimization and Promotions:** Using the data to identify pizzas that are underperforming or receiving 
                            little attention.""")
                st.write("**We have 4 tables with the important information. Here we can see the explanation of each column in each table üëáüèª**")
                df = pd.read_csv('datasets/data_dictionary.csv')
                st.write(df)
            
            elif page == 'Tables Closer':
                st.subheader("Table Description")
                st.subheader("Pizzas")
                df_p = pd.read_csv('datasets/pizzas.csv')
                st.write(df_p)
                st.write("**Here is a description of each column in this dataset:**")
                st.write("üìå **pizza_id** - Unique identifier for each pizza variant (includes type and size).",
                    "<br>üìå **pizza_type_id** - Identifies the pizza type or flavor, without size.",
                    "<br>üìå **size** - Indicates pizza size.",
                    "<br>üìå **price** - Price of the pizza for the given size (in USD).",
                    unsafe_allow_html=True)
                
                st.subheader("Pizza Types")
                df_p_t = pd.read_csv('datasets/pizza_types.csv', encoding='latin1')
                st.write(df_p_t)
                st.write("**Here is a description of each column in this dataset:**")
                st.write("üìå **pizza_type_id** - A unique identifier for each type of pizza (used as a key to link with other datasets).",
                    "<br>üìå **name** - The full name of the pizza type.",
                    "<br>üìå **category** - The category of the pizza, usually based on the main ingredient or style.",
                    "<br>üìå **ingredients** - A comma-separated list of ingredients used in that pizza.",
                    unsafe_allow_html=True)
                
                st.subheader("Order Details")
                df_o_d = pd.read_csv('datasets/order_details.csv', encoding='latin1')
                st.write(df_p_t)
                st.write("**Here is a description of each column in this dataset:**")
                st.write("üìå **pizza_type_id** - A unique identifier for the pizza type (used for joining with other tables).",
                    "<br>üìå **name** - The full name of the pizza type.",
                    "<br>üìå **category** - Category of the pizza based on style or main ingredient.",
                    "<br>üìå **ingredients** - List of ingredients in the pizza, separated by commas.",
                    "<br>üìå **Length_minutes** - Length of the movie in minutes",
                    unsafe_allow_html=True)
                
                st.subheader("Orders")
                df_p_t = pd.read_csv('datasets/orders.csv', encoding='latin1')
                st.write(df_p_t)
                st.write("**Here is a description of each column in this dataset:**")
                st.write("üìå **order_id** - Unique identifier for each customer order.",
                    "<br>üìå **date** - The date when the order was placed, in YYYY-MM-DD format.",
                    "<br>üìå **time** - The time the order was placed, in HH : MM : SS format.", 
                    unsafe_allow_html=True)  

                st.subheader("Pizza Sales")
                df_p_s = pd.read_csv('datasets/pizza_sales.csv', encoding='latin1') 
                st.write(df_p_s)
                st.write("**Here is a description of each column in this dataset:**")
                st.write("üìå **pizza_id** - Unique identifier for a specific pizza item in the dataset (often links to pizza details).",
                    "<br>üìå **order_id** - Unique identifier for a customer order (groups multiple pizzas under one order).",
                    "<br>üìå **pizza_name_id** - Identifier for the pizza type, without size.", 
                    "<br>üìå **quantity** - Number of units of this pizza in the order.", 
                    "<br>üìå **order_date** - Date when the order was placed, typically in YYYY-MM-DD format.", 
                    "<br>üìå **order_time** - Time when the order was placed, typically in HH:MM:SS format.", 
                    "<br>üìå **unit_price** - Price of a single pizza unit (in USD).", 
                    "<br>üìå **total_price** - Total price for that line item (unit_price * quantity).", 
                    "<br>üìå **pizza_size** - Size of the pizza (S, M, L, XL, XXL).", 
                    "<br>üìå **pizza_category** - Category of the pizza (e.g., Chicken, Classic, Veggie).", 
                    "<br>üìå **pizza_ingredients** - List of ingredients used in the pizza.", 
                    "<br>üìå **pizza_name** - Full name of the pizza type.", 
                    unsafe_allow_html=True) 

            elif page == 'SQL Code':
                st.subheader("A. KPI's (Key Performance Indicators).")
                # 1
                df_p_s = pd.read_csv('datasets/pizza_sales.csv', encoding='latin1') 
                st.write("**1. Total Revenue:**")
                code = """
                SELECT 
                    SUM(total_price) AS total_revenue
                FROM 
                    pizza_sales;"""
                st.code(code, language='sql')
                total_revenue = df_p_s['total_price'].sum()
                st.metric(label="", value=f"${total_revenue:,.2f}")
                # 2
                st.write("**2. Average Order Value:**")
                code = """
                SELECT 
                    SUM(total_price) / COUNT (DISTINCT order_id) AS AVG_Order_Value
                FROM 
                    pizza_sales;"""
                st.code(code, language='sql')
                total_revenue = df_p_s['total_price'].sum()
                total_orders = df_p_s['order_id'].nunique() 
                avg_order_value = total_revenue / total_orders
                st.metric(label="", value=f"${avg_order_value:,.16f}")
                # 3
                st.write("**3. Total Pizza Sold:**")
                code = """
                SELECT 
                    SUM(quantity) AS Total_Pizza_Sold
                FROM 
                    pizza_sales;"""
                st.code(code, language='sql')
                total_pizza_sold = df_p_s['quantity'].sum()
                st.metric(label="", value=f"{total_pizza_sold:,}")
                # 4
                st.write("**4. Total Orders**")
                code = """
                SELECT
                    COUNT(DISTINCT order_id) AS Total_Orders
                FROM
                    pizza_sales;"""
                st.code(code, language='sql')
                total_orders = df_p_s['order_id'].nunique()
                st.metric(label="", value=f"{total_orders:,}")
                # 5
                st.write("**5. Average Pizzas Per Orders**")
                code = """
                SELECT
                    ROUND(
                        ROUND(SUM(quantity),2) / ROUND(COUNT(DISTINCT order_id),2),2) AS Average_pizza_per_order
                FROM
                    pizza_sales;"""
                st.code(code, language='sql')
                total_pizzas = df_p_s['quantity'].sum()
                total_orders = df_p_s['order_id'].nunique()
                avg_pizza_per_order = total_pizzas / total_orders
                st.metric(label="", value=f"{avg_pizza_per_order:.2f}")
                st.subheader("B. Daily Trend for Total Orders.")
                # 6
                code = """
                SELECT
                    TO_CHAR(order_date, 'Day') AS order_day,
                    COUNT(DISTINCT order_id) AS Total_Orders
                FROM
                    pizza_sales
                GROUP BY
                    order_day
                ORDER BY
                    Total_Orders DESC;"""
                st.code(code, language='sql')
                df_p_s['order_date'] = pd.to_datetime(df_p_s['order_date'], dayfirst=True)
                orders_by_day = (
                    df_p_s.groupby(df_p_s['order_date'].dt.day_name())['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_id': 'Total_Orders', 'order_date': 'order_day'})
                    .sort_values(by='Total_Orders', ascending=False)
                )
                st.dataframe(orders_by_day)
                # 7
                st.subheader("C. Monthly Trend for Total Orders.")
                code = """
                SELECT
                    TO_CHAR(order_date, 'Month') AS Month_Name,
                    COUNT(DISTINCT order_id) AS Total_Orders
                FROM
                    pizza_sales
                GROUP BY
                    Month_Name
                ORDER BY
                    Total_Orders DESC;"""
                st.code(code, language='sql')
                df_p_s['order_date'] = pd.to_datetime(df_p_s['order_date'], dayfirst=True)
                monthly_orders = (
                    df_p_s.groupby(df_p_s['order_date'].dt.month_name())['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_date': 'Month_Name', 'order_id': 'Total_Orders'})
                )
                monthly_orders = monthly_orders.sort_values(by='Total_Orders', ascending=False)
                st.dataframe(monthly_orders)
                # 8
                st.subheader("D. % of Sales by Pizza Category.")
                code = """
                SELECT
                    pizza_category,
                    ROUND(SUM(total_price) *100 / (SELECT SUM(total_price)
                        FROM pizza_sales),2) AS percent_of_Sales
                FROM
                    pizza_sales
                GROUP BY
                    pizza_category;"""
                st.code(code, language='sql')
                sales_by_category = (
                    df_p_s.groupby('pizza_category')['total_price']
                    .sum()
                    .reset_index()
                )
                total_sales = sales_by_category['total_price'].sum()
                sales_by_category['percent_of_Sales'] = round((sales_by_category['total_price'] / total_sales) * 100, 2)
                sales_by_category = sales_by_category[['pizza_category', 'percent_of_Sales']]
                sales_by_category = sales_by_category.sort_values(by='percent_of_Sales', ascending=False)
                st.dataframe(sales_by_category)
                # 9
                st.subheader("E. % of Sales by Pizza Size.")
                code = """
                SELECT
                    pizza_size,
                    ROUND(SUM(total_price)*100 / (SELECT SUM(total_price)
                        FROM pizza_sales),2) AS percent_of_sales
                FROM
                    pizza_sales
                GROUP BY
                    pizza_size
                ORDER BY
                    percent_of_sales DESC;"""
                st.code(code, language='sql')
                sales_by_size = (
                    df_p_s.groupby('pizza_size')['total_price']
                    .sum()
                    .reset_index()
                )
                total_sales = sales_by_size['total_price'].sum()
                sales_by_size['percent_of_sales'] = round((sales_by_size['total_price'] / total_sales) * 100, 2)
                sales_by_size = sales_by_size[['pizza_size', 'percent_of_sales']]
                sales_by_size = sales_by_size.sort_values(by='percent_of_sales', ascending=False)
                st.dataframe(sales_by_size)
                # 10
                st.subheader("F. Top 5 Best Sellers by Revenue, Total Quantity & Total Orders.")
                st.write("**1. Top 5 Best Sellers by Revenue**")
                code = """
                SELECT
                    pizza_name, SUM(total_price) AS Total_Revenue
                FROM
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Revenue DESC
                LIMIT 5;"""
                st.code(code, language='sql')
                top5_revenue = (
                    df_p_s.groupby('pizza_name')['total_price']
                    .sum()
                    .reset_index()
                    .rename(columns={'total_price': 'Total_Revenue'})
                    .sort_values(by='Total_Revenue', ascending=False)
                    .head(5)
                )
                st.dataframe(top5_revenue)
                # 11
                st.write("**2. Bottom 5 Sellers by Revenue**")
                code = """
                SELECT
                    pizza_name, SUM(total_price) AS Total_Revenue
                FROM
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Revenue 
                LIMIT 5;"""
                st.code(code, language='sql')
                bottom5_revenue = (
                    df_p_s.groupby('pizza_name')['total_price']
                    .sum()
                    .reset_index()
                    .rename(columns={'total_price': 'Total_Revenue'})
                    .sort_values(by='Total_Revenue', ascending=True)
                    .head(5)
                )
                st.dataframe(bottom5_revenue)
                # 12
                st.write("**3. Top 5 Best Sellers by Quantity**")
                code = """
                SELECT
                    pizza_name, SUM(quantity) AS Total_Quantity
                FROM 
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Quantity DESC
                LIMIT 5;"""
                st.code(code, language='sql')
                top5_quantity = (
                    df_p_s.groupby('pizza_name')['quantity']
                    .sum()
                    .reset_index()
                    .rename(columns={'quantity': 'Total_Quantity'})
                    .sort_values(by='Total_Quantity', ascending=False)
                    .head(5)
                )
                st.dataframe(top5_quantity)
                # 13
                st.write("**4. Bottom 5 Sellers by Revenue**")
                code = """
                SELECT
                    pizza_name, SUM(quantity) AS Total_Quantity
                FROM
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Quantity
                LIMIT 5;"""
                st.code(code, language='sql')
                bottom5_quantity = (
                    df_p_s.groupby('pizza_name')['quantity']
                    .sum()
                    .reset_index()
                    .rename(columns={'quantity': 'Total_Quantity'})
                    .sort_values(by='Total_Quantity', ascending=True)
                    .head(5)
                )
                st.dataframe(bottom5_quantity)
                # 14
                st.write("**5. Top 5 Best Sellers by Total Orders**")
                code = """
                SELECT
                    pizza_name, COUNT(DISTINCT order_id) AS Total_Orders
                FROM
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Orders DESC
                LIMIT 5;"""
                st.code(code, language='sql')
                top5_orders = (
                    df_p_s.groupby('pizza_name')['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_id': 'Total_Orders'})
                    .sort_values(by='Total_Orders', ascending=False)
                    .head(5)
                )
                st.dataframe(top5_orders)
                # 15
                st.write("**6. Bottom 5 Sellers by Total Orders**")
                code = """
                SELECT
                    pizza_name, COUNT(DISTINCT order_id) AS Total_Orders
                FROM
                    pizza_sales
                GROUP BY
                    pizza_name
                ORDER BY
                    Total_Orders
                LIMIT 5;"""
                st.code(code, language='sql')
                bottom5_orders = (
                    df_p_s.groupby('pizza_name')['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_id': 'Total_Orders'})
                    .sort_values(by='Total_Orders', ascending=True)
                    .head(5)
                )
                st.dataframe(bottom5_orders)
                # 16
                st.subheader("G. Number of Customers each day & Busiest hours.")
                code = """
                SELECT 
                    order_date,
                    COUNT(DISTINCT order_id) AS num_customers
                FROM
                    pizza_sales
                GROUP BY
                    order_date
                ORDER BY
                    order_date;"""
                st.code(code, language='sql')
                code = """
                SELECT
                    EXTRACT(HOUR FROM order_time) AS order_hour,
                    COUNT(DISTINCT order_id) AS num_orders
                FROM
                    pizza_sales
                GROUP BY
                    order_hour
                ORDER BY
                    num_orders DESC;"""
                st.code(code, language='sql')
                df_p_s['order_hour'] = pd.to_datetime(df_p_s['order_time'], format='%H:%M:%S').dt.hour
                orders_per_hour = (
                    df_p_s.groupby('order_hour')['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_id': 'num_orders'})
                    .sort_values(by='num_orders', ascending=False)
                )
                st.dataframe(orders_per_hour)
                # 17
                st.subheader("H. Seasonality Trends.")
                code = """
                SELECT
                    EXTRACT(MONTH FROM order_date) AS month,
                    COUNT(DISTINCT order_id) AS total_orders
                FROM
                    pizza_sales
                GROUP BY
                    month
                ORDER BY
                    month;"""
                st.code(code, language='sql')
                df_p_s['order_date'] = pd.to_datetime(df_p_s['order_date'], dayfirst=True)
                seasonality = (
                    df_p_s.groupby(df_p_s['order_date'].dt.month)['order_id']
                    .nunique()
                    .reset_index()
                    .rename(columns={'order_date': 'month', 'order_id': 'total_orders'})
                )
                month_map = {
                    1: "January", 2: "February", 3: "March", 4: "April",
                    5: "May", 6: "June", 7: "July", 8: "August",
                    9: "September", 10: "October", 11: "November", 12: "December"
                }
                seasonality['month'] = seasonality['month'].map(month_map)
                st.dataframe(seasonality)
                # 18
                st.subheader("I. Average Orders per Day.")
                code = """
                WITH daily_orders AS (
                    SELECT
                        order_date,
                        COUNT(DISTINCT order_id) AS daily_order_count
                    FROM
                        pizza_sales
                    GROUP BY
                        order_date)
                    SELECT
                        AVG(daily_order_count) AS avg_orders_per_day
                    FROM
                        daily_orders;"""
                st.code(code, language='sql')
                df_p_s['order_date'] = pd.to_datetime(df_p_s['order_date'], dayfirst=True)
                daily_orders = df_p_s.groupby('order_date')['order_id'].nunique()
                avg_orders_per_day = daily_orders.mean()
                st.metric(label="", value=f"{avg_orders_per_day:.16f}")
                # 19
                st.subheader("J. Average Pizza Per Day sold.")
                code = """
                WITH avg_pizza as(
                    SELECT order_date,
                        COUNT(quantity) as daily_pizza
                    FROM pizza_sales
                    GROUP BY order_date
                )
                SELECT
                    AVG(daily_pizza) AS AVG_PIZZA_PER_DAY
                FROM avg_pizza;"""
                st.code(code, language='sql')
                df_p_s['order_date'] = pd.to_datetime(df_p_s['order_date'], dayfirst=True)
                daily_pizza = df_p_s.groupby('order_date')['quantity'].sum()
                avg_pizza_per_day = daily_pizza.mean()
                st.metric(label="", value=f"{avg_pizza_per_day:.16f}")
            
            elif page == 'Findings':
                st.subheader("Summary of Findings")
                st.markdown("""
                            - **Most occupied Days & Month:**
                               
                                **Days**-Orders are highest on Friday & Saturday evenings
                                
                                **Month**-Orders are highest on January & July
                            
                            - **Sales Performance:**
                                
                                **Category**-Classical contributes maximum to Sales & Total Orders
                                
                                **Size**-Large pizza contributes maximum to Sales
                            
                            - **Best Sellers:**
                                
                                **Revenue**-Thai Chicken Pizza contribute maximum to Revenue
                                
                                **Quantity**-Classical Deluxe Pizza contributes maximum to Total Quantities
                                
                                **Total Orders**-Classic Deluxe Pizza contributes maximum to Total Orders
                            
                            - **Lowest Sellers:**
                                
                                **Revenue**-Brie Carre Pizza contribute minimum to Revenue
                                
                                **Quantity**-Brie Carre Pizza contribute minimum to Total Quantities
                                
                                **Total Orders**-Brie Carre Pizza contribute minimum to Total Orders
                            
                            - **Most occupied Time:**
                                
                                **Lunch**-12 P.M. - 1:30 P.M.
                                
                                **Dinner**-6 P.M. - 8 P.M.""")

# PYTHON
if selected == 'Python':
    st.markdown("<h1 style='text-align: center;'>Python</h1>", unsafe_allow_html=True)
    st.markdown("""
                Welcome to the **Python** section of my portfolio!

                This collection showcases the projects I developed during my university studies. 
                They range from beginner-level tasks to more complex applications ‚Äî 
                each one marking a step in my learning journey and personal growth as a developer.
                """)
    selected_options = ["Simple calculator", 
                        "Virtual Wallet",
                        "Fictional Person", 
                        "EDA TMDb Movie Dataset", 
                        "Regular Expressions"]
    selected = st.selectbox("Feel free to explore and click on any project to learn more and see the code üëá", options = selected_options)
    st.write("Current selection:", selected)
    if selected == "Simple calculator":
        st.subheader("Simple calculator :)")
        python_code = """import math 

def add(x, y):
    return (x + y)
        
def subtract(x, y):
    return (x - y)

def multiply(x, y):
    return (x * y)

def divide(x, y):
    return (x / y)

def degree(x):
    return (math.pow(x, 2))

def degree_1(x, y):
    return (math.pow(x, y))

def sin(x):
    return (math.sin(x))

def cos(x):
    return (math.cos(x))

print('Select operation: \n1.Add \n2.Substract \n3.Multiply \n4.Divide \n5.Square \n6.To the power of \n7.sin \n8.cos \n9.Exit
')

while True:
    choice = input('Choose operation(1/2/3/4/5/6/7/8/9)')
    if choice in ('1', '2', '3', '4', '5', '6', '7', '8'):
        try:
            num1 = int(input('Enter first number:'))
            num2 = int(input('Enter second number:'))
        except ValueError:
            print('Enter please a number!')

        if choice == '1':
           print(add(num1, num2))

        elif choice == '2':
            print(subtract(num1, num2))

        elif choice == '3':
            print(multiply(num1, num2))

        elif choice == '4':
            if num2 == 0:
                break
            print(divide(num1, num2))

        elif choice == '5':
            print(math.pow(num1, 2))

        elif choice == '6':
            print(math.pow(num1, num2))

        elif choice == '7':
            print(math.sin(num1))

        elif choice == '8':
            print(math.cos(num1))

    if choice in ('9'):
        try:
            print('Thank you :)')
            exit()
        except ValueError:
            print('Enter please a number!')"""
        st.code(python_code, language='python')
        import math 

        def add(x, y):
            return x + y

        def subtract(x, y):
            return x - y

        def multiply(x, y):
            return x * y

        def divide(x, y):
            if y == 0:
                return "‚ùå Division by zero!"
            return x / y

        def square(x):
            return math.pow(x, 2)

        def power(x, y):
            return math.pow(x, y)

        def sin(x):
            return math.sin(x)

        def cos(x):
            return math.cos(x)
        st.subheader("üßÆ That Simple Calculator")
        operation = st.selectbox("Choose operation:", [
            "Add", "Subtract", "Multiply", "Divide",
            "Square", "Power", "Sine", "Cosine"
            ])
        num1 = st.number_input("Enter first number:", format="%.0f")
        need_second = operation in ["Add", "Subtract", "Multiply", "Divide", "Power"]
        if need_second:
            num2 = st.number_input("Enter second number:", format="%.0f")
        if st.button("Calculate"):
            if operation == "Add":
                result = add(num1, num2)
            elif operation == "Subtract":
                result = subtract(num1, num2)
            elif operation == "Multiply":
                result = multiply(num1, num2)
            elif operation == "Divide":
                result = divide(num1, num2)
            elif operation == "Square":
                result = square(num1)
            elif operation == "Power":
                result = power(num1, num2)
            elif operation == "Sine":
                result = sin(num1)
            elif operation == "Cosine":
                result = cos(num1)
            else:
                result = "Unknown operation"
    
        if 'result' in locals() and result is not None:
            st.success(f"Result: {result}")

    elif selected == "Virtual Wallet":
        st.subheader("Virtual Wallet: A Person Class with Balance Control")
        python_code = """class Person():
        def __init__(self, name, gender, mood):
        self.name = name
        self.gender = gender
        self.mood = mood
        self.__cash = 0

    def get_balance(self):
        return self.__cash

    def insert(self, x):
        self.__cash += x

    def payment(self, x):
        self.__cash -= x

    def check(self):
        if self.gender == '—Ö–ª–æ–ø–µ—Ü—å':
            print('–ú–µ–Ω–µ –∑–≤–∞—Ç–∏ ' + self.name + '. ' + '–Ø - –ø—Ä–∏–∫–æ–ª—å–Ω–∏–π '
              + self.gender + '. ' + '–£ –º–µ–Ω–µ ' + self.mood + ' –Ω–∞—Å—Ç—Ä—ñ–π.')
        else:
            print('–ú–µ–Ω–µ –∑–≤–∞—Ç–∏ ' + self.name + '. ' + '–Ø - –ø—Ä–∏–∫–æ–ª—å–Ω–∞ '
                  + self.gender + '. ' + '–£ –º–µ–Ω–µ ' + self.mood + ' –Ω–∞—Å—Ç—Ä—ñ–π.')

    #def __del__(self):
     #   print('–í–∏–¥–∞–ª–µ–Ω–Ω—è ' + str(self))


Person1 = Person(input('–Ü–º—è: '), input('–°—Ç–∞—Ç—å: '), input('–ù–∞—Å—Ç—Ä—ñ–π: '))
Person1.check()

print('–û–±–µ—Ä—ñ—Ç—å –¥—ñ—é: \n1.–ü–æ–ø–æ–≤–Ω–µ–Ω–Ω—è \n2.–û–ø–ª–∞—Ç–∞ \n3.–ó–∞–ª–∏—à–æ–∫ –Ω–∞ —Ä–∞—Ö—É–Ω–∫—É')

while True:
    choice = input('–û–±–µ—Ä—ñ—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è(1/2/3) ')

    if choice == '1':
        x = int(input('–ü–æ–ø–æ–≤–Ω–∏—Ç–∏ –Ω–∞: '))
        Person1.insert(x)

    elif choice == '2':
        y = int(input('–û–ø–ª–∞—Ç–∏—Ç–∏: '))
        if y > Person1.get_balance():
            print('–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –≥—Ä–æ—à–µ–π!')
        else:
            Person1.payment(y)

    else:
        print('–í–∞—à –±–∞–ª–∞–Ω—Å:', Person1.get_balance())"""
        st.code(python_code, language='python')
        st.subheader("üßçüèª‚Äç‚ôÄÔ∏è Create Your Person")

        class Person:
            def __init__(self, name, gender, mood):
                self.name = name
                self.gender = gender
                self.mood = mood
                self.__cash = 0

            def get_balance(self):
                return self.__cash

            def insert(self, x):
                self.__cash += x

            def payment(self, x):
                self.__cash -= x

            def check(self):
                if self.gender.lower() == 'boy':
                    return f"My name is {self.name}. I am a cool {self.gender}, and right now I have {self.mood} mood."
                else:
                    return f"My name is {self.name}. I am a cool {self.gender}, and right now I have {self.mood} mood."

        if 'person' not in st.session_state:
            with st.form("user_form"):
                name = st.text_input("Name:")
                gender = st.selectbox("Gender:", ["boy", "girl"])
                mood = st.text_input("Mood:")
                submitted = st.form_submit_button("Create")

                if submitted:
                    st.session_state.person = Person(name, gender, mood)
                    st.success("The object has been created!")

        if 'person' in st.session_state:
            person = st.session_state.person
            st.info(person.check())

            action = st.radio("Choose the option:", ["Top-up", "Payment", "View balance"])

            if action == "Top-up":
                amount = st.number_input("Top-up amount:", min_value=0, step=1)
                if st.button("Continue"):
                    person.insert(amount)
                    st.success(f"Your balance has been successfully topped up with ${amount}")

            elif action == "Payment":
                amount = st.number_input("Payment amount:", min_value=0, step=1)
                if st.button("Continue"):
                    if amount > person.get_balance():
                        st.error("Not enough money!")
                    else:
                        person.payment(amount)
                        st.success(f"Payment of ${amount} was successful.")

            elif action == "View balance":
                st.write(f"Your currnet balance is ${person.get_balance()}.")

    elif selected == "Fictional Person":
        st.subheader("FictionalPersona: Interactive Life Simulator")
        python_code = """import random

import fictional
person = fictional.FictionalPerson

class MyDay():
    def __init__(self, person):
        self.person = person
        self.events = self.generate_events()

    def generate_events(self):
        events = []
        wakeup_time = random.randint(5, 7)
        events.append(f'{self.person.name} wakes up at {wakeup_time}:45 am')    
        breakfast_choices = ['cereal', 'toast', 'eggs', 'smoothie']
        breakfast = random.choice(breakfast_choices)
        events.append(f'{self.person.name} has {breakfast} for breakfast')  
        work_start_time = random.randint(9, 11)
        events.append(f'{self.person.name} starts work at {work_start_time}:30 am')   
        if work_start_time == 11:
            movie_choices = ['1+1', 'All Quiet on the Western Front', "Don't Look Up"]
            movie = random.choice(movie_choices)
            events.append(f'{self.person.name} watches {movie}') 
        else:
            tv_shows_choices = ['Breaking Bad', 'Supernatural', 'You']
            tv_show = random.choice(tv_shows_choices)
            events.append(f'{self.person.name} watches {tv_show}')  
        bedtime = random.randint(00, 2)
        events.append(f'{self.person.name} goes to bed at {bedtime}:00 pm')  
        return events

    def describe_day(self):
        print(f"{self.person.name}'s day:")
        for event in self.events:
            print('- ' + event)

person1 = fictional.FictionalPerson('Jane', 25, 'neutral', 5, 80, 5)
my_day = MyDay(person1)
my_day.describe_day()"""
        st.code(python_code, language='python')       
        # === –ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å ===
        class Person:
            def __init__(self, name, age):
                self.name = name
                self.age = age

        # === –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∫–ª–∞—Å FictionalPerson ===
        class FictionalPerson(Person):
            def __init__(self, name, age, mood, reaction, heart_beats, energy):
                super().__init__(name, age)
                self.mood = mood
                self.reaction = reaction
                self.heart_beats = heart_beats
                self.energy = energy

            def coffee_cup(self):
                self.mood = 'happier than before'
                self.reaction += 5
                self.heart_beats += 10
                return "After coffee, mood: {}, reaction: {}, heart beats: {}".format(
                    self.mood, self.reaction, self.heart_beats
                )

            def music_listening(self):
                self.mood = 'happier than ever'
                self.reaction -= 2
                self.energy += 5
                return f"After music, mood: {self.mood}, reaction: {self.reaction}, energy: {self.energy}"

            def have_a_date(self):
                self.mood = 'excited'
                self.heart_beats += 30
                return f"On a date! Mood: {self.mood}, heart beats: {self.heart_beats}"

            def shower(self):
                self.energy -= 2
                return f"Took a shower. Energy: {self.energy}. Feeling fresh!"

            def celebrate_birthday(self):
                self.age += 1
                return f"{self.name} is now {self.age} years old! üéâ"

        # === –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ–¥—ñ–π –¥–Ω—è ===
        class MyDay:
            def __init__(self, person):
                self.person = person
                self.events = self.generate_events()

            def generate_events(self):
                events = []
                wakeup_time = random.randint(5, 7)
                events.append(f'{self.person.name} wakes up at {wakeup_time}:45 am')

                breakfast = random.choice(['cereal', 'toast', 'eggs', 'smoothie'])
                events.append(f'{self.person.name} has {breakfast} for breakfast')

                work_start = random.randint(9, 11)
                events.append(f'{self.person.name} starts work at {work_start}:30 am')

                if work_start == 11:
                    movie = random.choice(['1+1', 'All Quiet on the Western Front', "Don't Look Up"])
                    events.append(f'{self.person.name} watches {movie}')
                else:
                    tv_show = random.choice(['Breaking Bad', 'Supernatural', 'You'])
                    events.append(f'{self.person.name} watches {tv_show}')

                bedtime = random.randint(0, 2)
                events.append(f'{self.person.name} goes to bed at {bedtime}:00 pm')
                return events

            def describe_day(self):
                return self.events

        # === STREAMLIT APP ===
        st.markdown("Meet Jane, a fictional person whose day you get to explore. " \
        "From waking up and choosing breakfast to working and relaxing with shows or movies, " \
        "every moment shapes her mood, energy, and reactions. This project brings Jane‚Äôs life to life " \
        "through code ‚Äî showing how little events like drinking coffee or taking a shower change her " \
        "feelings and energy throughout the day.")

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ —É session_state
        if "person" not in st.session_state:
            st.session_state.person = FictionalPerson('Jane', 25, 'neutral', 5, 80, 5)

        person = st.session_state.person

        # –°–∏–º—É–ª—è—Ü—ñ—è –¥–Ω—è
        st.subheader("Create Jane's Day")
        if st.button("generate"):
            my_day = MyDay(person)
            for event in my_day.describe_day():
                st.write("- " + event)

        # –ö–Ω–æ–ø–∫–∏ –¥—ñ–π
        st.subheader("Daily Activities (choose 'celebrate birthday')")
        if st.button("drink coffee"):
            st.success(person.coffee_cup())

        if st.button("listen to music"):
            st.success(person.music_listening())

        if st.button("go on a date"):
            st.success(person.have_a_date())

        if st.button("take a shower"):
            st.success(person.shower())

        if st.button("celebrate birthday"):
            st.balloons()
            st.success(person.celebrate_birthday())

        # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω
        st.subheader("üß† Current Status")
        st.write(f"Name: {person.name}")
        st.write(f"Age: {person.age}")
        st.write(f"Mood: {person.mood}")
        st.write(f"Reaction: {person.reaction}")
        st.write(f"Heart Beats: {person.heart_beats}")
        st.write(f"Energy: {person.energy}")

    elif selected == "EDA TMDb Movie Dataset":
        st.markdown("""
                    **A bucket of popcorn and a cozy blanket, that‚Äôs my perfect movie night...** üé¨üçø
                
                    But choosing which movie to watch is always a tricky decision. For this, I often 
                    turn to TMDb ‚Äì one of the world‚Äôs most popular and community-driven movie databases. 
                    From ratings to reviews, it provides a vast collection of insights into the film industry.  
                    
                    Being a movie enthusiast, I decided to dive deeper into this dataset. This project focuses 
                    on performing Exploratory Data Analysis (EDA) on the TMDb Movie Dataset 
                    to uncover trends, patterns, and interesting facts about movies and the global cinema world.
                    """)
        selected_options = [
            "1. Project overview",
            "2. Data Wrangling",
            "3. Exploratory Data Analysis",
            "4. Conclusions"
            ]
        selected = st.selectbox("Let's begin. To make it easier to understand, I divided this project into several sections üëáüèª", options = selected_options)
        st.write("Current selection:", selected)
        if selected == "1. Project overview":
            st.title("1. Project Overview")
            st.markdown("""The main objective of this project is to analyze a dataset containing information about 10,000 movies 
                        from 1960 to 2015, collected from The Movie Database (TMDb) and including all films details such as the 
                        production cost, the revenue generated, rating information, actors and directors, etc. You can ckeck the whole dataset and I will 
                        try to answer to the questions below...""")
            df = pd.read_csv('datasets/DATA.csv')
            st.write(df)
            st.subheader("*Research questions*")
            st.markdown("""
                        1. **Movie Properties and Success Factors**
                            - What kinds of properties are associated with the most and least successful movies?
                            - Which movie had the highest or lowest profit?
                            - In which year did the movie industry make the highest profit?
                            - In which month did the movie industry make the highest profit?
                            - Do popular movies generate higher profit?
                            - What were the most and least expensive movies?
                            - What is the statistical relationship between budget and profit?
                            - Do movies with the highest budget receive the highest ratings?
                            - Which movie had the highest or lowest revenue?
                            - Is there any statistical relationship between revenue and profit / revenue and budget?
                            - What movie length is most liked by the audience?
                            - Which movies were rated the highest or lowest?
                            - Do highly rated movies generate higher profit?
                        2. **Top 10 Movies According to Different Features**
                            - Profit
                            - Budget
                            - Revenue
                            - Popularity
                        3. **Genres: Popularity and Profitability**
                            - Which genres are the most popular and profitable overall?
                            - Which genres are the most profitable year by year?
                            - Which genres are the most popular overall?
                            - How has genre popularity evolved over time?
                        4. **Top 10 Cast, Directors, and Production Companies**
                            - Who are the top 10 actors/actresses (cast)?
                            - Who are the top 10 directors?
                            - What are the top 10 production companies?""")
        
        elif selected == "2. Data Wrangling":
            st.title("2. Data Wrangling")
            with open("html/data_wrangling.html", "r", encoding="utf-8") as f:
                notebook_html = f.read()
            st.components.v1.html(notebook_html, height=800, scrolling=True)

        elif selected == '3. Exploratory Data Analysis':
            st.title("3. Exploratory Data Analysis")
            with open("html/exploratory_data_analysis.html", "r", encoding="utf-8") as f:
                notebook_html = f.read()
            st.components.v1.html(notebook_html, height=800, scrolling=True)
        
        elif selected == '4. Conclusions':
            st.title("4. Conclusions")
            st.markdown("""
            Throughout this data analysis, I found that:

            - **"Star Wars," "Avatar," and "Titanic"** are the most profitable movies, while the top movies by revenue are "Avatar," "Star Wars," and "Titanic." Despite their financial success, these films are not among the top 5 in popularity.
            - **June and December** are the months when movies generate the highest profits, highlighting the impact of holidays and festivities on box office performance.
            - There is a **strong positive correlation between profit and revenue**, indicating that higher revenue generally leads to higher profit. Revenue and budget are also positively correlated, suggesting that higher production costs often result in greater earnings. However, profit shows only a weak relationship with movie ratings, and there is no significant relationship between budget and runtime.
            - The **top 5 dominant genres (1960‚Äì2015)** are Drama, Comedy, Thriller, Action, and Adventure. This does not imply they are the most profitable (Animation, Adventure, Family, Fantasy, and Science Fiction) or the most popular (Adventure, Science Fiction, Fantasy, Animation, and Action). Profitability and popularity vary over the years, with a noticeable upward trend in recent periods.
            - The **most frequent actors** are Robert De Niro, Bruce Willis, and Samuel L. Jackson; the **top directors** are Steven Spielberg, Clint Eastwood, and Ridley Scott; and the **leading production companies** are Universal Pictures, Warner Bros, and Paramount Pictures.
            """)

            st.markdown("### *Limitations...*")
            st.write("""
            This is only a preliminary analysis of a small sample of movies. 
            The film industry includes thousands more films, but due to the large amount of missing data, it was not possible to perform this analysis on a larger sample. 
            Therefore, these findings may differ if a more comprehensive dataset were used.
            """)

    elif selected == "Regular Expressions":
        st.subheader("Regular Expressions")
        st.write("""
                 What are Regular Expressions?
                    
                 Regular expressions (or regex) are like special search patterns that help us **find, match, or change text** in a smart way.  
                 For example:
                    - Find all email addresses in a text.
                    - Replace all spaces with underscores.
                    - Check if a password is strong.

                    They are useful when you work with **text data** and need to **search or clean it quickly**.
                    """)
        selected_options = [
            "1. Word Counter",
            "2. Email Extractor",
            "3. Extract Phone Numbers",
            "4. Car Number Validator",
            "5. Validate Password Strength",
            "6. Replace Multiple Spaces with One"]
        selected = st.selectbox("Now, choose a project to explore here üëáüèª", options = selected_options)
        st.write("Current selection:", selected)

        if selected == "1. Word Counter":
            def count_words(text):
                # Regex: \b means word boundary, [a-zA-Z–ê-–Ø–∞-—è–á—ó–Ü—ñ–Ñ—î“ê“ë]+ means letters only
                pattern = r'\b[a-zA-Z–ê-–Ø–∞-—è–á—ó–Ü—ñ–Ñ—î“ê“ë]{2,}\b'
                words = re.findall(pattern, text)
                return len(words)

            def word_count_project():
                st.subheader("Word Count App (Regex)")
                st.write("""
                **Description:**  
                This tool counts words in a text using **regular expressions**, considering English and Cyrillic letters.
                
                **Regex pattern:**  
                `\\b[a-zA-Z–ê-–Ø–∞-—è–á—ó–Ü—ñ–Ñ—î“ê“ë]{2,}\\b`
                
                - It ignores punctuation  
                - Works for multiple languages  
                - Counts only words with 2+ letters
                """)

                text = st.text_area("Enter text here:", "")
                if st.button("Count Words"):
                    result = count_words(text)
                    st.success(f"Total words: {result}")

                    st.write("**Matched words:**")
                    st.write(re.findall(r'\b[a-zA-Z–ê-–Ø–∞-—è–á—ó–Ü—ñ–Ñ—î“ê“ë]{2,}\b', text))

            if __name__ == "__main__":
                word_count_project()
        
        elif selected == "2. Email Extractor":
            def email_extractor_project():
                st.subheader("Email Extractor")
                st.write("""
                **Description:**  
                This tool extracts **valid email addresses** from your text using a regular expression.  

                **Regex pattern:**  
                `\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b`
                
                - Handles multiple emails  
                - Works with different domain formats  
                - Ignores invalid entries
                """)

                example_text = """Here are some emails as examples:
                - disposable.style.email.with+symbol@example.com
                - other.email-with-hyphen@example.net
                - A@b@c@example.com
                - fully-qualifies-domain@example.com
                - user.name@example.com
                - x@example.com
                - 5786@example.ua
                - ‚Äò‚Äôjohn..doe‚Äô‚Äô@example.org
                - mailhost!username@example.org
                - this is"notallowed@example.com 
                - i_like_underscore@but_its_not_allow_in _this_part.example.com 
                """

                text = st.text_area("Enter text here:", example_text, height=150)
                
                if st.button("Extract Emails"):
                    pattern = r'\b[\w\.-]+@[\w\.-]+\.\w+\b'
                    matches = re.findall(pattern, text)

                    if matches:
                        st.success(f"Found {len(matches)} email(s):")
                        for email in matches:
                            st.markdown(f"- **{email}**")
                    else:
                        st.warning("No valid email addresses found.")

            if __name__ == "__main__":
                email_extractor_project()

        elif selected == "3. Extract Phone Numbers":
            def phone_number_extractor():
                st.subheader("Phone Number Extractor")
                st.write("""
                **Description:**  
                This tool extracts phone numbers from text.  
                Typical valid formats include:
                - +380 XX XXX XXXX  
                - 0XX XXX XX XX  
                - +380XXXXXXXXX

                **Regex pattern:**  
                `(?:\+380|0)\s?\(?\d{2}\)?[\s\-]?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}` 
                """)

                example_text = """Try these phone numbers as examples:
Alex - +380 67 123 45 67 
Tom - 067 123 45 67
Linda - +380(44)123-45-67
Nick - 0441234567
John - +380 123 456 7890
                """

                text = st.text_area("Enter text here:", example_text, height=150)

                if st.button("Extract Phone Numbers"):
                    pattern = r'(?:\+380|0)\s?\(?\d{2}\)?[\s\-]?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}'
                    matches = re.findall(pattern, text)

                    if matches:
                        st.success(f"Found {len(matches)} phone number(s):")
                        for num in matches:
                            st.markdown(f"- **{num}**")
                    else:
                        st.warning("No valid phone numbers found.")

            if __name__ == "__main__":
                phone_number_extractor()

        elif selected == "4. Car Number Validator":
            def car_number_validator_project():
                st.subheader("Car Number Validator")
                st.write("""
                **Description:**  
                This tool validates Ukrainian car number plates matching the format:  
                Two uppercase letters (Cyrillic or Latin) + 4 digits + two uppercase letters.
                
                It also identifies the region based on the first two letters.
                
                **Regex pattern:**  
                `[–ê-–Ø–Ü–á–Ñ“êA-Z]{2}\\d{4}[–ê-–Ø–Ü–á–Ñ“êA-Z]{2}`
                         
                (For clarity, this section has been provided in Ukrainian.)
                """)

                regions = {
                    '–ê–í': '–í—ñ–Ω–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–í': '–í—ñ–Ω–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–°': '–í–æ–ª–∏–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–°': '–í–æ–ª–∏–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–ï': '–î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–ï': '–î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–ù': '–î–æ–Ω–µ—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–ù': '–î–æ–Ω–µ—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–ú': '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–ú': '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–û': '–ó–∞–∫–∞—Ä–ø–∞—Ç—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–û': '–ó–∞–∫–∞—Ä–ø–∞—Ç—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–†': '–ó–∞–ø–æ—Ä—ñ–∑—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–†': '–ó–∞–ø–æ—Ä—ñ–∑—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–¢': '–Ü–≤–∞–Ω–æ-–§—Ä–∞–Ω–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–¢': '–Ü–≤–∞–Ω–æ-–§—Ä–∞–Ω–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    "–ê–ê": "–ö–∏—ó–≤",
                    "–ê–Ü": "–ö–∏—ó–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å",
                    '–í–ê': '–ö—ñ—Ä–æ–≤–æ–≥—Ä–∞–¥—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–ù': '–ö—ñ—Ä–æ–≤–æ–≥—Ä–∞–¥—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–í': '–õ—É–≥–∞–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–í': '–õ—É–≥–∞–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–°': '–õ—å–≤—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–°': '–õ—å–≤—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–ï': '–ú–∏–∫–æ–ª–∞—ó–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–ï': '–ú–∏–∫–æ–ª–∞—ó–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–ù': '–û–¥–µ—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–ù': '–û–¥–µ—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–Ü': '–ü–æ–ª—Ç–∞–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–Ü': '–ü–æ–ª—Ç–∞–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–ö': '–†—ñ–≤–Ω–µ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–ö': '–†—ñ–≤–Ω–µ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–ú': '–°—É–º—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–ú': '–°—É–º—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–û': '–¢–µ—Ä–Ω–æ–ø—ñ–ª—å—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–û': '–¢–µ—Ä–Ω–æ–ø—ñ–ª—å—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ê–•': '–•–∞—Ä–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ö–•': '–•–∞—Ä–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–¢': '–•–µ—Ä—Å–æ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–¢': '–•–µ—Ä—Å–æ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–í–•': '–•–º–µ–ª—å–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–ù–•': '–•–º–µ–ª—å–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–°–ê': '–ß–µ—Ä–∫–∞—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–Ü–ê': '–ß–µ—Ä–∫–∞—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–°–ê': '–ß–µ—Ä–Ω—ñ–≥—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å',
                    '–Ü–ê': '–ß–µ—Ä–Ω—ñ–≥—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å'
                }

                car_number = st.text_input("–í–≤–µ–¥—ñ—Ç—å –Ω–æ–º–µ—Ä –≤–∞—à–æ–≥–æ –∞–≤—Ç–æ (—Ñ–æ—Ä–º–∞—Ç: XX1234XX)", "")

                if st.button("–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–æ–º–µ—Ä"):
                    pattern = r'[–ê-–Ø–Ü–á–Ñ“êA-Z]{2}\d{4}[–ê-–Ø–Ü–á–Ñ“êA-Z]{2}'

                    if re.fullmatch(pattern, car_number):
                        region_code = car_number[0:2]
                        region = regions.get(region_code)
                        if region:
                            st.success(f"–ù–æ–º–µ—Ä {car_number} –¥—ñ–π—Å–Ω–∏–π. –ê–≤—Ç–æ –∑–∞—Ä–µ—î—Å—Ç—Ä–æ–≤–∞–Ω–µ –≤ —Ä–µ–≥—ñ–æ–Ω—ñ: {region}")
                        else:
                            st.info(f"–ù–æ–º–µ—Ä {car_number} –¥—ñ–π—Å–Ω–∏–π. –†–µ–≥—ñ–æ–Ω —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ.")
                    else:
                        st.error(f"–ù–æ–º–µ—Ä {car_number} –Ω–µ–¥—ñ–π—Å–Ω–∏–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –Ω–æ–º–µ—Ä –∞–≤—Ç–æ.")

            if __name__ == "__main__":
                car_number_validator_project()

        elif selected == "5. Validate Password Strength":
            def password_strength_checker():
                st.subheader("Password Strength Validator")
                st.write("""
                **Description:**  
                Checks if a password meets the following criteria:  
                - At least 8 characters  
                - Contains at least one uppercase letter  
                - Contains at least one lowercase letter  
                - Contains at least one digit  
                - Contains at least one special character (`!@#$%^&*()_+-=[]{}|;:'",.<>?/`)  
                - No spaces allowed

                **Regex pattern:**  
                `^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[!@#$%^&*()_+\\-=\[\]{}|;:'",.<>?/])[A-Za-z\\d!@#$%^&*()_+\\-=\[\]{}|;:'",.<>?/]{8,}$`
                """)

                password = st.text_input("Enter password:", type="password")

                if st.button("Check Strength"):
                    pattern = r'^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[!@#$%^&*()_+\-=\[\]{}|;:\'",.<>?/])[A-Za-z\d!@#$%^&*()_+\-=\[\]{}|;:\'",.<>?/]{8,}$'
                    if re.match(pattern, password):
                        st.success("Strong password!")
                    else:
                        st.error("Weak password. Make sure it meets all the criteria.")

            if __name__ == "__main__":
                password_strength_checker()

        elif selected == "6. Replace Multiple Spaces with One":
            def clean_text_project():
                st.subheader("üßπ Clean Text: Replace Multiple Spaces & Remove Duplicate Words")
                st.write("""
                **Description:**  
                This tool cleans up text by:  
                - Replacing multiple consecutive spaces with a single space  
                - Removing consecutive duplicate words (case-insensitive)
                
                **Regex patterns used:**  
                - Multiple spaces: `\\s{2,}`
                - Duplicate words: `\\b(\\w+)\\s+\\1\\b` 
                """)

                example_text = """This is is    an example   text with  multiple   spaces and and duplicate duplicate words."""

                text = st.text_area("Enter text here:", example_text, height=150)

                if st.button("Clean Text"):
                    # Replace multiple spaces with one
                    text_single_space = re.sub(r'\s{2,}', ' ', text)

                    # Remove consecutive duplicate words (case-insensitive)
                    cleaned_text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text_single_space, flags=re.IGNORECASE)

                    st.success("Text cleaned!")
                    st.write(cleaned_text)

            if __name__ == "__main__":
                clean_text_project()

# VISUALIZATION
if selected == 'Visualization':
    st.markdown("<h1 style='text-align: center;'>Visualization</h1>", unsafe_allow_html=True)
    st.markdown("""
                Welcome to the part of my portfolio where data comes to life!

                In this section, you‚Äôll find some of my favorite projects where I‚Äôve turned numbers into stories using different tools:

                - üêç Python ‚Äì because sometimes code is art.
                - üìä Tableau ‚Äì for when I want to impress you with drag-and-drop magic.
                - üßÆ R ‚Äì the tool I use when I feel fancy and statistical.
                
                Whether you're into clean dashboards, interactive charts, or just enjoy watching data glow up ‚Äî you're in the right place.
                
                Grab a cup of coffee and scroll through. I promise no pie charts were harmed in the making of these visuals. üç∞
                """)
    selected_options = ["Python", 
                        "Tableau",
                        "R"]
    selected = st.selectbox("Feel free to explore and click here üëá", options = selected_options)
    st.write("Current selection:", selected)
    if selected == "Python":
        selected_optipns = ["Shopology. Analysis of Clothes Shopping",
                            "Visualization of Mortality",
                            "Letterbox Movie Classification"]
        selected = st.selectbox("Select a project", options = selected_optipns)
        st.write("Current selection:", selected)
        if selected == "Shopology. Analysis of Clothes Shopping":
            import pandas as pd
            import streamlit as st
            import pandas as pd
            import streamlit as st
            import plotly.express as px
            import folium
            def main():
            # page settings
                page = st.sidebar.selectbox('Choose a page:',
                                            ['DataSet',
                                            'Items Info',
                                            'Location',
                                            'Payment Methods'])
                if page == 'DataSet':

                    df = pd.read_csv('datasets/Shopping_Trends.csv')

                    st.title(':necktie: Shopology: Decoding Customer Shopping Trends')

                    st.write("**The Shopping Trends Dataset** offers valuable insights into consumer behavior "
                            "and purchasing patterns. Understanding customer preferences and trends is critical "
                            "for businesses to tailor their products, marketing strategies, and overall customer experience. ")
                    st.write(
                        "\nThis dataset captures a **wide range of customer attributes** including age, gender, purchase history, "
                        "preferred payment methods, frequency of purchases, and more. ")
                    st.write("\nAnalyzing this data can help businesses make informed decisions, optimize product offerings, "
                            "and enhance customer satisfaction. The dataset stands as a valuable resource for businesses "
                            "aiming to align their strategies with customer needs and preferences. ")
                    st.write('\nHere it is üôÉ')

                    st.write(df)

                    st.markdown("## Details")
                    st.write("**Title**: Shopping Trends ",
                            "<br>**Rows**: 3900",
                            "<br>**Columns**: 22",
                            unsafe_allow_html=True)

                    st.markdown('## Data Dictionary')
                    st.write("üìå **Customer ID** - Unique identifier for each customer.",
                            "<br>üìå **Age** - Age of the customer.",
                            "<br>üìå **Gender** - Gender of the customer (Male/Female)",
                            "<br>üìå **Item Purchased** - The item purchased by the customer",
                            "<br>üìå **Category** - Category of the item purchased",
                            "<br>üìå **Purchase Amount (USD)** - The amount of the purchase in USD",
                            "<br>üìå **Location** - Location where the purchase was made",
                            "<br>üìå **Size** - Size of the purchased item",
                            "<br>üìå **Color** - Color of the purchased item",
                            "<br>üìå **Season** - Season during which the purchase was made",
                            "<br>üìå **Review Rating** - Rating given by the customer for the purchased item",
                            "<br>üìå **Payment Method** - Customer's most preferred payment method",
                            "<br>üìå **Shipping Type** - Type of shipping chosen by the customer",
                            "<br>üìå **Previous Purchases** - Number of previous purchases made by the customer",
                            "<br>üìå **Preferred Payment Method** - Method which was used to pay for the purchase",
                            "<br>üìå **Frequency of Purchases** - Frequency at which the customer makes purchases",
                            "<br>üìå **Customer Segmentation** - The official label of a customer",
                            "<br>üìå **Seasonal Trends** - The purpose of purchase",
                            "<br>üìå **Delivery Time** - Duration it takes for a products to be delivered",
                            "<br>üìå **Number of Items Purchased** - The amount of items purchased by a customer",
                            unsafe_allow_html=True)

                elif page == 'Items Info':
                
                    df = pd.read_csv('datasets/Shopping_Trends.csv')

                    st.title(':bar_chart: Sales Dashboard')
                    st.markdown('##')

                    st.write("Here, you can easily discover a wealth of relevant information tailored to your needs. "
                            "Navigate effortlessly through the data by employing our user-friendly filtering options. "
                            "Explore details related to Categories, delve into specifics about individual Items, "
                            "and gain insights into Size and Color attributes. ")
                    st.write('Your journey to comprehensive and insightful data exploration begins here...')

                    # sidebar
                    st.sidebar.header('Choose Filter Here: ')
                    gender = st.sidebar.multiselect(
                        "Select the Gender: ",
                        options=df['Gender'].unique(),
                        default=df['Gender'].unique()
                    )

                    season = st.sidebar.multiselect(
                        'Select the Season: ',
                        options=df['Season'].unique(),
                        default=df['Season'].unique()
                    )

                    segmentation = st.sidebar.multiselect(
                        'Select the Customer Segmentation: ',
                        options=df["Customer Segmentation"].unique(),
                        default=df['Customer Segmentation'].unique()
                    )

                    df_selection = df.query(
                        "Gender == @gender & Season == @season & `Customer Segmentation` == @segmentation"
                    )

                    # main page
                    total_sales = int(df_selection['Purchase Amount (USD)'].sum())
                    average_rating = round(df_selection['Review Rating'].mean(), 1)
                    star_rating = ':star:' * int(round(average_rating, 0))
                    average_amount_of_orders = round(df_selection['Number of Items Purchased'].mean())

                    left_column, middle_column, right_column = st.columns(3)
                    with left_column:
                        st.subheader('Total Sales:')
                        st.write(f'**USD $ {total_sales}**')
                    with middle_column:
                        st.subheader('Average Rating:')
                        st.write(f'**{average_rating} {star_rating}**')
                    with right_column:
                        st.subheader('Average Items Purchased:')
                        st.write(f'**{average_amount_of_orders} items**')

                    st.markdown('---')

                    # Data Set
                    st.dataframe(df_selection)

                    # first left graph
                    items_line = df_selection.groupby(by=['Item Purchased']).sum()[['Purchase Amount (USD)']]. \
                        sort_values(by='Purchase Amount (USD)')

                    fig_items = px.histogram(
                        items_line,
                        x=items_line.index,
                        y='Purchase Amount (USD)',
                        title='<b>Purchase Amount for each item (USD)</b>',
                        color_discrete_sequence=['#9B67E2'] * len(items_line),
                        template='plotly_white'
                    )

                    # st.plotly_chart(fig_items)

                    # first right graph
                    category_line = df_selection.groupby(by=['Category']).sum()[['Purchase Amount (USD)']]
                    fig_category = px.bar(
                        category_line,
                        x='Purchase Amount (USD)',
                        y=category_line.index,
                        title='<b>Purchase Amount for each category (USD)</b>',
                        color_discrete_sequence=['#FBE426'] * len(category_line),
                        template='plotly_white'
                    )

                    # right place
                    left_column, right_column = st.columns(2)
                    left_column.plotly_chart(fig_items, use_container_width=True)
                    right_column.plotly_chart(fig_category, use_container_width=True)

                    # second left graph
                    colors = df_selection['Color'].unique()
                    selected_color = st.multiselect('Choose color:', colors)
                    filtered_df = df_selection[df_selection['Color'].isin(selected_color)]

                    color_graph = px.histogram(
                        filtered_df,
                        x='Purchase Amount (USD)',
                        y='Color',
                        title=f'<b>Purchase amount for {", ".join(selected_color)} items<b>',
                        color_discrete_sequence=['#FBE426'] * len(category_line),
                        template='plotly_white'
                    )

                    # second right graph
                    size_graph = px.pie(filtered_df,
                                        title='<b>Purchase Amount for each Size<b>',
                                        values='Purchase Amount (USD)',
                                        names='Size',
                                        color='Size',
                                        color_discrete_map={'M': '#9B67E2',
                                                            'L': '#BEADFA',
                                                            'S': '#DFCCFB',
                                                            'XL': '#FFF8C9'},
                                        hole=0.4
                                        )

                    # right place
                    left_column, right_column = st.columns(2)
                    left_column.plotly_chart(color_graph, use_container_width=True)
                    right_column.plotly_chart(size_graph, use_container_width=True)

                elif page == 'Location':

                    df = pd.read_csv('datasets/Shopping_Trends.csv')

                    st.title(':bar_chart: Sales Dashboard')
                    st.markdown('##')

                    st.write(
                        "Step into the next level of precision by filtering information based on Location and Shipping Type columns."
                        " Seamlessly refine your search to pinpoint data relevant to specific locations and shipping preferences."
                        " Whether you're seeking details about a particular region or looking to understand the nuances of different "
                        "shipping methods, our platform empowers you to tailor your exploration. "
                        "Navigate with ease as you unlock insights tied to location-based dynamics and shipping intricacies. "
                        "Your quest for targeted and refined data analysis continues here.")

                    # filter the data
                    st.sidebar.header('Choose Filter Here: ')
                    # create for location
                    location = st.sidebar.multiselect('Pick your State', df['Location'].unique())
                    if not location:
                        df2 = df.copy()
                    else:
                        df2 = df[df['Location'].isin(location)]

                    # create for shipping type
                    shipping = st.sidebar.multiselect('Pick your Shipping Type', df2['Shipping Type'].unique())
                    if not shipping:
                        df3 = df2.copy()
                    else:
                        df3 = df2[df2['Shipping Type'].isin(shipping)]

                    # filter the data
                    if not location and not shipping:
                        filtered_df = df
                    elif location and shipping:
                        filtered_df = df3[df['Location'].isin(location) & df3['Shipping Type'].isin(shipping)]
                    elif location:
                        filtered_df = df3[df['Location'].isin(location)]
                    else:
                        filtered_df = df3[df['Shipping Type'].isin(shipping)]

                    # Data Set
                    st.dataframe(filtered_df)

                    # left graph
                    st.markdown('### Percentage by Gender/Season/Customer Segmentation:')
                    fig = px.sunburst(
                        data_frame=filtered_df,
                        path=['Gender', 'Season', 'Customer Segmentation'],
                        color='Season',
                        color_discrete_sequence=px.colors.qualitative.Pastel,
                        maxdepth=-1
                    )

                    fig.update_traces(textinfo='label+percent entry')
                    fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))

                    st.markdown("---")  # Add horizontal space

                    # —Åenter the chart 
                    col1, col2, col3 = st.columns([1, 2, 1])  # Middle column is wider
                    with col2:
                        st.plotly_chart(fig, use_container_width=True)
           
                    st.markdown("---")  # Add more horizontal space

                    # map
                    st.title(':world_map: Sales Map')
                    st.markdown('##')

                    st.write("Here you can explore and uncover insights into the spending habits of different states across the "
                            "United States. Dive into the data to find 8 states that lead the way in terms of purchase expenditures. ")

                    df_map = pd.read_csv('datasets/map_file.csv')

                    st.dataframe(df_map)

                    fig = px.scatter_geo(
                        df_map,
                        locations='Code',
                        locationmode='USA-states',
                        color='Code',
                        scope='usa',
                        color_discrete_map={'MT': '#636EFA', 'ME': '#00CC96', 'FL': '#AB63FA', 'DE': '#FFA15A', 'VA': '#B6E880',
                                            'KY': '#B6E880', 'IN': '#FBE426', 'AR': '#DEA0FD'}
                    )

                    # —Åenter the chart 
                    col1, col2, col3 = st.columns([1, 2, 1])  # Middle column is wider
                    with col2:
                        st.plotly_chart(fig, use_container_width=True)

                elif page == 'Payment Methods':
                    
                    df = pd.read_csv('datasets/Shopping_Trends.csv')

                    st.title(':bar_chart: Sales Dashboard')
                    st.markdown('##')

                    st.write("Comprehensive payment methods information page. "
                            "Here, you'll effortlessly discover everything you need to know about various payment methods. "
                            "Our platform provides a user-friendly interface that allows you to navigate seamlessly and explore "
                            "details related to payment options, ensuring a smooth and informed experience. "
                            "Whether you're interested in understanding available payment methods, comparing their features, "
                            "or seeking insights into payment trends, this page is your go-to resource. "
                            "Simplify your quest for payment information and make informed decisions with ease."
                            "Take your exploration to the next level by customizing your payment methods information based on age "
                            "and customer segmentation. ")

                    # selection
                    customer = df['Customer Segmentation'].unique().tolist()
                    ages = df['Age'].unique().tolist()

                    age_selection = st.sidebar.slider('**Age:**',
                                                    min_value=min(ages),
                                                    max_value=max(ages),
                                                    value=(min(ages), max(ages)))

                    customer_selection = st.sidebar.multiselect('**Customer Segmentation:**',
                                                                customer,
                                                                default=customer)

                    # filter dataframe
                    mask = (df['Age'].between(*age_selection)) & (df['Customer Segmentation'].isin(customer_selection))
                    number_of_results = df[mask].shape[0]
                    st.markdown(f'*Available Results: {number_of_results}*')

                    # group dataframe
                    df_grouped = df[mask].groupby(by=['Payment Method']).count()[['Age']]
                    df_grouped = df_grouped.rename(columns={'Age': 'Number of Customers'})
                    df_grouped = df_grouped.reset_index()

                    # first left graph
                    bar_chart = px.bar(df_grouped,
                                    title='<b>Number of Customers for every Payment Method<b>',
                                    x='Payment Method',
                                    y='Number of Customers',
                                    text='Number of Customers',
                                    color_discrete_sequence=['#9B67E2'] * len(df_grouped),
                                    template='plotly_white')

                    bar_chart.update_traces(texttemplate='%{text:.2s}', textposition='outside')
                    bar_chart.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')

                    # first right graph
                    pie_chart = px.pie(df_grouped,
                                    title='<b>Percent of Customers for every Payment Method<b>',
                                    values='Number of Customers',
                                    names='Payment Method',
                                    color='Payment Method',
                                    color_discrete_map={'Credit Card': '#FF9B9B',
                                                        'Venmo': '#FFD28F',
                                                        'Cash': '#D0BFFF',
                                                        'Paypal': '#FFFAD7',
                                                        'Debit Card': '#FFFEC4',
                                                        'Bank Transfer': '#CBFFA9'},
                                    hole=0.4
                                    )

                    left_column, right_column = st.columns(2)
                    left_column.plotly_chart(bar_chart, use_container_width=True)
                    right_column.plotly_chart(pie_chart, use_container_width=True)

            main()
        
        elif selected == "Visualization of Mortality":
            st.title('üöë How Do People Die in the USA? A Visualization of Mortality')
            st.markdown("""**Death** is a difficult topic, but it is crucial for government, healthcare, economy, and medicine. 
                        Understanding how people die can lead to changes in research funding or strengthening preventive measures against certain contemporary diseases. """)
            st.markdown("""In the USA, **Centers for Disease Control and Prevention (CDC)** collected [mortality data](https://wonder.cdc.gov/ucd-icd10.html) from 1999 to 2015. 
                        The data is rich in demographic information, including age at death, the disease causing it, gender, race, and geographical location (city/state).""")
            st.markdown("""
                This data will help us answer many **questions about death**:

                - What are the leading causes of death in the USA?
                - Are men or women more likely to die? Does it depend on the cause of death? Or the age?
                - Which causes of death are becoming more or less common over time?
                """)
            st.markdown("For this project the dataset was used:")
            df = pd.read_csv('datasets/deaths.csv')
            st.write(df)
            col1, col2 = st.columns(2)
            # --- –ö–æ–ª–æ–Ω–∫–∞ 1: –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–º–µ—Ä—Ç–µ–π –ø–æ —Ä–æ–∫–∞—Ö ---
            with col1:
                col1.markdown("### 1. Total Deaths by Year")
                by_year = df.groupby("Year")["Deaths"].sum()
                
                fig1, ax1 = plt.subplots(figsize=(5, 4))
                by_year.plot(kind="bar", color=['blue', 'blue', 'red'], ax=ax1)
                ax1.set_title("Total Deaths by Year")
                ax1.set_xlabel("Year")
                ax1.set_ylabel("Deaths")
                ax1.tick_params(axis='x', rotation=45)
                st.pyplot(fig1)
            # --- –ö–æ–ª–æ–Ω–∫–∞ 2: –°–º–µ—Ä—Ç—ñ –∑–∞ —Å—Ç–∞—Ç—Ç—é —É 2015 ---
            with col2:
                col2.markdown("### 2. Deaths by Gender in 2015")
                df2015 = df[df["Year"] == 2015]
                by_gender_2015 = df2015.groupby("Gender")["Deaths"].sum()
                
                fig2, ax2 = plt.subplots(figsize=(5, 4))
                by_gender_2015.plot(kind="bar", color=["red", "blue"], ax=ax2)
                ax2.set_title("Deaths by Gender (2015)")
                ax2.set_xlabel("Gender")
                ax2.set_ylabel("Deaths")
                st.pyplot(fig2)
            st.markdown("In the **first graph** I demonstrated the total deaths by years and highlighted " \
            "the year with the greatest amount. In the **second graph** I created a simple bar chart to " \
            "compare the total number of deaths by gender in this year.")
            df2015 = df[df["Year"] == 2015]
            causes = df2015.groupby('Cause')['Deaths'].sum().reset_index()
            causes_sorted = causes.sort_values(by='Deaths', ascending=False)
            # –¢–æ–ø-5 –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö —ñ –Ω–∞–π–º–µ–Ω—à –ø–æ–ø—É–ª—è—Ä–Ω–∏—Ö
            top_5 = causes_sorted.head(5)
            bottom_5 = causes_sorted.tail(5)
            # –î–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            col1, col2 = st.columns(2)
            # --- –ö–æ–ª–æ–Ω–∫–∞ 1: –¢–æ–ø-5 ---
            with col1:
                col1.markdown("### 3. Top 5 Most Common Causes")
                fig_top, ax_top = plt.subplots(figsize=(6, 4))
                top_5.set_index('Cause').plot(kind='barh', color='darkgreen', ax=ax_top)
                ax_top.set_xlabel("Number of Deaths")
                ax_top.set_ylabel("Cause")
                ax_top.invert_yaxis()
                st.pyplot(fig_top)
            # --- –ö–æ–ª–æ–Ω–∫–∞ 2: –ù–∞–π–º–µ–Ω—à 5 ---
            with col2:
                col2.markdown("### 4. Bottom 5 Least Common Causes")
                fig_bottom, ax_bottom = plt.subplots(figsize=(6, 4))
                bottom_5.set_index('Cause').plot(kind='barh', color='orange', ax=ax_bottom)
                ax_bottom.set_xlabel("Number of Deaths")
                ax_bottom.set_ylabel("Cause")
                ax_bottom.invert_yaxis()
                st.pyplot(fig_bottom)
            st.markdown("""
                        Here I broke down one big graph into two small ones for clear understanding. 
                        
                        **Conclusions:**
                        
                        - The most prevalent diseases - heart disease, malignant neoplasms.
                        - The least prevalent - acute poliomyelitis; arthropod-borne viral encephalitis; measles; scarlet fever and diphtheria.""")

            st.subheader("5. Deaths by Age and Gender in 2015")
        
            df2015 = df[df["Year"] == 2015]
            age_gender_deaths = df2015.groupby(["Age", "Gender"])["Deaths"].sum()
            pivot_table = age_gender_deaths.unstack(1)
            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(17, 10), sharex=True)
            colors = ["blue", "red"]  # –î–ª—è —Å—Ç–∞—Ç–µ–π
            pivot_table.plot(
                kind="bar",
                ax=axes,
                subplots=True,
                color=colors,
                legend=False,
                width=0.8
            )
            axes[0].set_title("Deaths by Age - Male")
            axes[1].set_title("Deaths by Age - Female")
            axes[1].set_xlabel("Age")
            axes[0].set_ylabel("Deaths")
            axes[1].set_ylabel("Deaths")

            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("""
                **Conclusions:**
                - First months of life: males (higher mortality).
                - Up to 13 years old: approximately the same.
                - Up to 84 years old: males (higher mortality).
                - Up to 100 years old: females (higher mortality).""")
            
            st.subheader("6. Stacked Bar Chart of Deaths by Age and Gender (2015)")
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–∞–Ω—ñ –∑–∞ 2015 —Ä—ñ–∫
            df2015 = df[df["Year"] == 2015]

            # –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è: Age + Gender ‚Üí Deaths
            age_gender_deaths = df2015.groupby(["Age", "Gender"])["Deaths"].sum()

            # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ —Ç–∞–±–ª–∏—Ü—é –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—ñ–∫–∞
            pivot_table = age_gender_deaths.unstack(1)

            # –ü–æ–±—É–¥–æ–≤–∞ stacked bar chart
            fig, ax = plt.subplots(figsize=(22, 12))  # –∑–º—ñ–Ω–µ–Ω–æ –∑ 18x10 –Ω–∞ 12x6 –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏–≥–ª—è–¥—É –≤ –±—Ä–∞—É–∑–µ—Ä—ñ

            pivot_table.plot(
                kind="bar",
                stacked=True,
                color=["red", "blue"],
                ax=ax
            )

            ax.set_title("Deaths by Age and Gender (2015)", fontsize=14)
            ax.set_xlabel("Age")
            ax.set_ylabel("Number of Deaths")
            ax.legend(title="Gender", loc="upper right")
            ax.tick_params(axis='x', rotation=45)

            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("""
                        I would combine the two graphs... However, it's not very informative because 
                        it's difficult to compare male and female indicators in a single bar chart.
                        
                        **That would be suitable for:**
                        
                        - Comparison of one characteristic across different types.
                        - Sales of different types of products across different points.""")
            st.subheader("7. Line Chart of Deaths by Age and Gender (2015)")
            df2015 = df[df["Year"] == 2015]
            age_gender_deaths = df2015.groupby(["Age", "Gender"])["Deaths"].sum()
            pivot_table = age_gender_deaths.unstack(1)
            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 6), sharex=True)
            colors = ["blue", "red"]
            pivot_table.plot(
                kind="line",
                subplots=True,
                ax=axes,
                color=colors,
                legend=False
            )

            axes[0].set_title("Deaths by Age - Male")
            axes[1].set_title("Deaths by Age - Female")
            axes[1].set_xlabel("Age")
            axes[0].set_ylabel("Deaths")
            axes[1].set_ylabel("Deaths")

            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("Since **Age**] is a continuous variable, it would be appropriate to use a line plot for comparison.")
            st.subheader("8. Line Chart of Deaths by Age and Gender (2015) ‚Äì Combined View")
            # –î–∞–Ω—ñ –∑–∞ 2015 —Ä—ñ–∫
            df2015 = df[df["Year"] == 2015]

            # –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è: Age + Gender ‚Üí Deaths
            pivot_table = df2015.groupby(["Age", "Gender"])["Deaths"].sum().unstack(1)

            # –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞
            fig, ax = plt.subplots(figsize=(12, 5))  # –∑–º—ñ–Ω–µ–Ω–æ –∑ 18x6 –Ω–∞ –∑—Ä—É—á–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
            pivot_table.plot(kind="line", color=["red", "blue"], ax=ax, title="Deaths in 2015 by Age and Gender")

            ax.set_ylabel("Deaths")
            ax.set_xlabel("Age")
            ax.legend(title="Gender")
            ax.grid(False)

            st.pyplot(fig)

            st.markdown("Now, using only lines, it's easy to compare the difference between genders by age on **one line graph.**")
            
            st.subheader("9. Causes of Death in 2015")
            deaths_by_cause = (
                df[df["Year"] == 2015]
                .groupby("Cause")
                .agg({"Deaths": "sum"})
                .sort_values("Deaths", ascending=True)
            )
            fig, ax = plt.subplots(figsize=(9, 12))
            deaths_by_cause.plot(
                kind="barh",
                legend=False,
                color="black",
                ax=ax
            )

            ax.set_xlabel("Number of Deaths")
            ax.set_ylabel("Cause")
            ax.set_title("Causes of Death in 2015")
            plt.tight_layout()

            st.pyplot(fig)
            st.markdown("Excess information in visualizations hinders data comprehension, " \
            "creates visual noise, and distracts from the main insights.")
            st.subheader("10. 10 Most Common Causes of Death in 2015")
            least_common_causes = (
                df[df["Year"] == 2015]
                .groupby("Cause")
                .agg({"Deaths": "sum"})
                .sort_values("Deaths", ascending=True)
                .tail(10) 
            )
            fig, ax = plt.subplots(figsize=(9, 6))
            least_common_causes.plot(
                kind="barh",
                legend=False,
                color="black",
                ax=ax
            )
            ax.set_xlabel("Number of Deaths")
            ax.set_ylabel("Cause")
            ax.set_title("10 Most Common Causes of Death in 2015")
            plt.tight_layout()

            st.pyplot(fig)
            st.markdown("""
                        This graph is easier to understand. It is clear that:
                        
                        - Diseases of heart
                        - Malignant neoplasms
                        - Chronic lower respiratory diseases
                        
                        are the most prevalent causes of mortality.""")
            st.subheader("11. Top causes of death by age ")
            cause_year_deaths = (
                df.groupby(["Cause", "Year"])
                .agg({'Deaths': 'sum'})
                .sort_values('Deaths', ascending=False)
                .unstack(1)  # –†–æ–∫–∏ ‚Äî –æ–∫—Ä–µ–º—ñ –∫–æ–ª–æ–Ω–∫–∏
            )
            # –ü–æ–±—É–¥–æ–≤–∞ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ—ñ–∫–∞
            fig, ax = plt.subplots(figsize=(20, 15))  # —à–∏—Ä—à–∏–π, –±–æ –±–∞–≥–∞—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            cause_year_deaths.plot(kind="bar", legend=True, ax=ax)

            ax.set_xlabel("Cause")
            ax.set_ylabel("Number of Deaths")
            ax.set_title("Deaths by Cause and Year")
            ax.tick_params(axis='x', rotation=90)

            plt.tight_layout()
            st.pyplot(fig)

            st.markdown("""The visualization above contains a lot of information (perhaps too much). 
                        However, it's easy to notice that **mortality due to HIV infection has been decreasing every 5 years, 
                        starting from 2005!**""")
            
            st.subheader("12. Deaths by Cause and Gender")
            cause_gender_deaths = (
                df.groupby(['Cause', 'Gender'])
                .agg({'Deaths': 'sum'})
                .sort_values('Deaths', ascending=True)
                .unstack(1)  # Gender ‚Üí –æ–∫—Ä–µ–º—ñ –∫–æ–ª–æ–Ω–∫–∏
            )

            # –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –∑ –∫–æ–ª—å–æ—Ä–∞–º–∏
            fig, ax = plt.subplots(figsize=(20, 15))
            cause_gender_deaths.plot(
                kind='bar',
                legend=True,
                ax=ax,
                color=['blue', 'red']  # –ß–æ–ª–æ–≤—ñ–∫–∏ ‚Äî blue, –ñ—ñ–Ω–∫–∏ ‚Äî red
            )

            ax.set_xlabel("Cause")
            ax.set_ylabel("Number of Deaths")
            ax.set_title("Deaths by Cause and Gender")
            plt.tight_layout()

            st.pyplot(fig)
            st.subheader("13. Causes of death by age")
            st.markdown ("""
                         Since the dataset contains a large number of causes of death, I've selected only a few for visualization:

                        - "Alzheimer's disease"
                        - "Diseases of heart"
                        - "Malignant neoplasms"
                        - "Accidents (unintentional injuries)""")

            clist = [
                "Alzheimer's disease",
                "Diseases of heart",
                "Malignant neoplasms",
                "Accidents (unintentional injuries)"
            ]
            df2015_clist = df[df["Year"] == 2015]
            df2015_clist = df2015_clist[df2015_clist["Cause"].isin(clist)]

            age_cause_deaths = (
                df2015_clist
                .groupby(["Age", "Cause"])
                .agg({'Deaths': 'sum'})
                .unstack(1)  # Cause ‚Üí –æ–∫—Ä–µ–º—ñ –ª—ñ–Ω—ñ—ó
            )

            fig, ax = plt.subplots(figsize=(10, 6))
            age_cause_deaths.plot(kind="line", ax=ax)

            ax.set_title("Deaths by Age for Selected Causes in 2015")
            ax.set_xlabel("Age")
            ax.set_ylabel("Number of Deaths")
            ax.legend(title="Cause", loc="upper left")
            ax.grid(True)

            plt.tight_layout()
            st.pyplot(fig)

            st.markdown("""
                        **Conclusions:**
                        
                        - Mortality due to unintentional injuries is almost constant.
                        - Mortality due to Alzheimer's disease remains constant until 70 years old, then increases until 90 years old, and decreases afterward.
                        - Mortality due to heart diseases increases from 19 years old to 90. It then sharply decreases after 90.
                        - Mortality due to malignant neoplasms increases from 20 years old to 70. It then sharply decreases after 70.""")

            st.subheader("14. Deaths by Age and Gender for 3 Selected Causes (2015)")
            clist = df["Cause"].unique()[:3]

            for cause in clist:
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ –ø–æ—Ç–æ—á–Ω—ñ–π –ø—Ä–∏—á–∏–Ω—ñ
                df2015_clist = df[df["Year"] == 2015]
                df2015_clist = df2015_clist[df2015_clist["Cause"] == cause]
                
                # –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞
                grouped = (
                    df2015_clist
                    .groupby(["Age", "Gender"])
                    .agg({'Deaths': 'sum'})
                    .unstack("Gender")  # –°—Ç–∞—î –¥–≤—ñ –ª—ñ–Ω—ñ—ó: Male, Female
                )
                
                fig, ax = plt.subplots(figsize=(10, 6))
                grouped.plot(kind="line", ax=ax, color=['red', 'blue'], title=cause)
                
                ax.set_xlabel("Age")
                ax.set_ylabel("Number of Deaths")
                ax.legend(title="Gender")
                ax.grid(True)
                plt.tight_layout()
                
                st.pyplot(fig)

            st.markdown("""
                            **Overall conclusions:**

                            - The most common causes of death across all years are heart diseases and malignant neoplasms.
                            - The least common are acute poliomyelitis; arthropod-borne viral encephalitis; measles; scarlet fever; and diphtheria.
                            - The number of deaths is generally higher among men than women.
                            - Mortality due to unintentional injuries remains almost constant throughout life, depending on Alzheimer's disease, heart diseases, and neoplasms.
                            - Higher mortality is observed among both men and women from 75 to 100 years old.""")

        elif selected == "Letterbox Movie Classification":
            page = st.sidebar.selectbox('Choose a page:',
                                ['DataSet',
                                 'About language',
                                 'Genres',
                                 'Studios Behind the Success',
                                 'Rating-Based Film Rankings'])
            if page == 'DataSet':
                df = pd.read_csv('datasets/Letterbox Movie Classification Dataset.csv')

                st.title('üé¨ Letterbox Movie Analysis')

                st.markdown("""
                            The Letterbox Movie Classification Dataset is a rich and detailed 
                            collection of metadata for 10,002 movies, sourced from a popular 
                            movie rating and review platform (inspired by Letterboxd-like data). 
                            This dataset provides a comprehensive snapshot of movie attributes, 
                            including titles, directors, genres, ratings, runtime, language, studios, 
                            and user engagement metrics such as watches, likes, and list appearances. 
                            """)
                st.write('\nHere it is üôÉ')

                st.write(df)
                st.markdown('---')
                st.markdown("## Details")
                st.write("**Title**: Letterbox Movie Classification Dataset ",
                        "<br>**Rows**: 10,002",
                        "<br>**Columns**: 15",
                        unsafe_allow_html=True)
                
                st.markdown('## Data Dictionary')
                st.write("üìå **Film_title** - The title of the movie.",
                    "<br>üìå **Director** - The primary director(s) of the movie. Multiple directors are listed together.",
                    "<br>üìå **Average_rating** - The average user rating for the movie (on a scale of 1 to 5).",
                    "<br>üìå **Genres** - A list of genres associated with the movie (e.g., ['Horror', 'Drama']).",
                    "<br>üìå **Runtime** - The runtime of the movie in minutes.",
                    "<br>üìå **Original_language** - The runtime of the movie in minutes.",
                    "<br>üìå **Description** - A brief synopsis or description of the movie‚Äôs plot or theme.",
                    "<br>üìå **Studios** - A list of production studios associated with the movie.",
                    "<br>üìå **Watches** - The total number of times the movie has been watched by users.",
                    "<br>üìå **List_appearances** - The number of times the movie appears in user-curated lists.",
                    "<br>üìå **Likes** - The total number of likes the movie has received from users.",
                    "<br>üìå **Fans** - The number of users who have marked themselves as fans of the movie.",
                    "<br>üìå **Lowest ‚òÖ** - The number of 1-star ratings the movie has received.",
                    "<br>üìå **Medium ‚òÖ‚òÖ‚òÖ** - The number of 3-star ratings the movie has received.",
                    "<br>üìå **Highest ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ** - The number of 5-star ratings the movie has received.",
                    "<br>üìå **Total_ratings** - The total number of ratings (across all star levels) for the movie.",
                        unsafe_allow_html=True)
            
            elif page == 'About language':
                st.title("Language Distribution & Cultural Reach")
                st.markdown("""
                            Let‚Äôs take a trip around the world ‚Äî no passport needed (sounds like a dream for men in Ukraine now). 
                            This section explores how different languages shape the movie landscape. 
                            From runtime quirks to average ratings and which languages dominate the screen, each chart gives 
                            a peek into how culture and cinema go hand in hand. And yes, English might be the loudest in the room... 
                            but it's definitely not the only one talking.""")

                df = pd.read_csv('datasets/Letterbox Movie Classification Dataset.csv')

                st.markdown(
                    """
                    <style>
                    .highlight-block {
                        background-color: #f2f6fc;
                        border-left: 6px solid #1f77b4;
                        padding: 1rem;
                        margin-bottom: 2rem;
                        border-radius: 8px;
                    }
                    .highlight-block h3 {
                        margin-top: 0;
                        color: #1f4e79;
                    }
                    .highlight-block ul {
                        padding-left: 1.2rem;
                    }
                    </style>
                    """,
                    unsafe_allow_html=True
                )

                # Compute values
                num_movies = df["Film_title"].nunique()
                top_language = df["Original_language"].mode().iloc[0]
                avg_runtime = round(df["Runtime"].mean(), 1)
                top_rated = df.loc[df["Average_rating"].idxmax()]
                top_movie_title = top_rated["Film_title"]
                top_movie_rating = top_rated["Average_rating"]

                # Creative container block
                st.markdown(
                    f"""
                    <div class="highlight-block">
                        <h3>üé¨ Dataset Overview: Key Insights</h3>
                        <ul>
                            <li><strong>Total movies:</strong> {num_movies}</li>
                            <li><strong>Most common language:</strong> {top_language}</li>
                            <li><strong>Average runtime:</strong> {avg_runtime} minutes</li>
                            <li><strong>Top-rated film:</strong> <em>{top_movie_title}</em> ‚≠ê {top_movie_rating}</li>
                        </ul>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                st.markdown('---')
                st.write(df)

                # ---- Top Languages by Movie Count (Histogram) ----
                top_lang_counts = (
                    df["Original_language"]
                    .value_counts()
                    .head(10)
                    .reset_index()
                )
                top_lang_counts.columns = ["Language", "Movie Count"]

                fig_hist = px.bar(
                    top_lang_counts,
                    x="Language",
                    y="Movie Count",
                    color="Language",
                    title="Top 10 Most Common Languages",
                )
                st.plotly_chart(fig_hist, use_container_width=True)
                st.markdown('---')

                # Pie Chart for Percentage Distribution
                top_lang_count = df["Original_language"].value_counts().head(10).reset_index()
                top_lang_count.columns = ["Language", "Number of Movies"]

                fig_pie = px.pie(
                    top_lang_count,
                    names="Language",
                    values="Number of Movies",
                    title="Language Share of Top 10 Movie Languages",
                    hole=0.3  
                )
                fig_pie.update_traces(textposition='inside', textinfo='percent+label')
                st.plotly_chart(fig_pie, use_container_width=True)
                st.markdown('---')

                st.subheader("Now you can interact with grpahs. Choose the options here üëáüèª")

                # --- Dropdowns on main screen (not sidebar) ---
                all_languages = df["Original_language"].dropna().unique()

                selected_languages = st.multiselect(
                    "Select Languages:",
                    sorted(all_languages),
                    default=["English", "Japanese", "Ukrainian", "Italian", "Swedish", "Finnish"]
                )

                metric = st.selectbox(
                    "Choose a Metric:",
                    ["Number of Movies", "Average Rating", "Average Runtime"]
                )

                # --- Filter Data ---
                filtered_df = df[df["Original_language"].isin(selected_languages)]

                # --- Plot based on selected metric ---
                if metric == "Number of Movies":
                    data = filtered_df["Original_language"].value_counts().reset_index()
                    data.columns = ["Language", "Count"]
                    fig = px.bar(
                        data,
                        x="Language",
                        y="Count",
                        title="Number of Movies by Language",
                        color="Language"
                    )

                elif metric == "Average Rating":
                    data = filtered_df.groupby("Original_language")["Average_rating"].mean().reset_index()
                    data.columns = ["Language", "Average Rating"]
                    fig = px.bar(
                        data,
                        x="Language",
                        y="Average Rating",
                        title="Average Rating by Language",
                        color="Language"
                    )

                elif metric == "Average Runtime":
                    data = filtered_df.groupby("Original_language")["Runtime"].mean().reset_index()
                    data.columns = ["Language", "Average Runtime"]
                    fig = px.bar(
                        data,
                        x="Language",
                        y="Average Runtime",
                        title="Average Runtime by Language",
                        color="Language"
                    )

                # --- Display the chart ---
                st.plotly_chart(fig, use_container_width=True)

            elif page == 'Genres':
                df = pd.read_csv('datasets/Letterbox Movie Classification Dataset.csv')

                st.title("Genres and Popularity")
                st.markdown("""
                            In the genres section, I wanted to see what kind of movies dominate the platform‚Äîspoiler: 
                            drama never dies. I broke down the data to find which genres are the most common, 
                            which ones get the highest ratings, and which attract the most likes. 
                            It turns out that action and science fiction are like that one friend who always shows up 
                            (and gets all the attention). Meanwhile, poor experimental films are probably crying 
                            in a corner of the dataset.""")
                
                st.write(df)
                st.markdown("---")

                # --- Preprocess genres ---
                df["Genres"] = df["Genres"].str.strip("[]").str.replace("'", "")
                df_genres = df.assign(Genres=df["Genres"].str.split(", ")).explode("Genres")

                # --- Filter UI ---
                st.subheader("Filter Options")
                col1, col2 = st.columns(2)

                with col1:
                    genre_limit = st.slider("Number of genres to display", min_value=5, max_value=30, value=15)

                with col2:
                    selected_directors = st.multiselect(
                        "Select Director(s):",
                        sorted(df["Director"].dropna().unique()),
                        default=[]
                    )

                # --- Apply director filter if selected ---
                if selected_directors:
                    df_genres_filtered = df_genres[df_genres["Director"].isin(selected_directors)]
                else:
                    df_genres_filtered = df_genres.copy()

                # --- Chart 1: Genre frequency ---
                genre_counts = df_genres_filtered["Genres"].value_counts().head(genre_limit).reset_index()
                genre_counts.columns = ["Genre", "Count"]

                fig1 = px.bar(
                    genre_counts,
                    x="Genre",
                    y="Count",
                    title="Most Common Genres",
                    labels={"Count": "Number of Movies"},
                    color="Genre"
                )
                st.plotly_chart(fig1, use_container_width=True)

                st.markdown("---")

                # --- Chart 2: Average rating by genre ---
                avg_rating = (
                    df_genres_filtered.groupby("Genres")["Average_rating"]
                    .mean()
                    .sort_values(ascending=False)
                    .head(genre_limit)
                    .reset_index()
                )

                fig2 = px.bar(
                    avg_rating,
                    x="Genres",
                    y="Average_rating",
                    title="Average Rating by Genre",
                    labels={"Genres": "Genre", "Average_rating": "Average Rating"},
                    color="Genres"
                )
                st.plotly_chart(fig2, use_container_width=True)

                st.markdown("---")

                st.subheader("For the next graph I offer you to deep your interaction and choose the main topic :)")

                # --- Chart 3: Selected popularity metric by genre ---
                popularity_metric = st.selectbox(
                    "Select metric for popularity analysis:",
                    ["Likes", "Fans", "Watches", "Runtime"]
                )
                popularity_data = (
                    df_genres_filtered.groupby("Genres")[popularity_metric]
                    .mean() if popularity_metric == "Runtime" else df_genres_filtered.groupby("Genres")[popularity_metric].sum()
                ).sort_values(ascending=False).head(genre_limit).reset_index()

                fig3 = px.bar(
                    popularity_data,
                    x="Genres",
                    y=popularity_metric,
                    title=f"{popularity_metric} by Genre",
                    labels={"Genres": "Genre", popularity_metric: popularity_metric},
                    color="Genres"
                )
                st.plotly_chart(fig3, use_container_width=True)

                st.markdown("---")

                st.subheader("And, finally, here is the last graph in this section...")

                # --- Chart 3: Top Fan-Favourite Films with Genres ---
                genre_fans = df.groupby(["Film_title", "Genres"])["Fans"].sum().nlargest(genre_limit).reset_index()

                fig3 = px.bar(
                    genre_fans,
                    x="Fans",
                    y="Film_title",
                    color="Genres",
                    title=f"Top {genre_limit} Fan-Favourite Films and Their Genres",
                    labels={"Fans": "Fan Count", "Film_title": "Film Title"}
                )
                fig3.update_layout(yaxis=dict(autorange="reversed"))
                st.plotly_chart(fig3, use_container_width=True)

                st.markdown("---")

            elif page == 'Studios Behind the Success':
                df = pd.read_csv('datasets/Letterbox Movie Classification Dataset.csv')

                st.title("Studios Behind the Success")
                st.markdown("""
                            Ever wondered who‚Äôs pulling the strings behind your favorite films? 
                            In this section, I explore which studios are truly making movie magic ‚Äî and which ones are just... 
                            showing up with snacks. From average ratings to total likes and fan love, 
                            let‚Äôs see which studios are the real MVPs (Movie-Valuing Producers üòÑ).""")
                
                st.write(df)
                st.markdown("---")

                # --- Sidebar Filter ---
                st.sidebar.header("Chart Settings")
                top_n = st.sidebar.slider("Select number of top films to show", min_value=5, max_value=50, value=25)

                # --- Chart 1: Top Films by Ratings and Studios ---
                ratings_studio = df.groupby(["Film_title", "Studios"])["Total_ratings"]\
                    .sum().nlargest(top_n).reset_index()
                ratings_studio["Label"] = ratings_studio["Film_title"] + " (" + ratings_studio["Studios"] + ")"

                fig1 = px.bar(
                    ratings_studio,
                    x="Total_ratings",
                    y="Label",
                    orientation="h",
                    title=f"Top {top_n} Highest Rated Films and Their Studios",
                    labels={"Total_ratings": "Total Ratings", "Label": "Film (Studio)"},
                    color="Total_ratings",
                    color_continuous_scale="purp"
                )
                fig1.update_layout(yaxis=dict(autorange="reversed"))
                st.plotly_chart(fig1, use_container_width=True)

                st.markdown("---")

                # --- Chart 2: Top Films by Ratings and Directors ---
                ratings_director = df.groupby(["Film_title", "Director"])["Total_ratings"]\
                    .sum().nlargest(top_n).reset_index()
                ratings_director["Label"] = ratings_director["Film_title"] + " (" + ratings_director["Director"] + ")"

                fig2 = px.bar(
                    ratings_director,
                    x="Total_ratings",
                    y="Label",
                    orientation="h",
                    title=f"Top {top_n} Highest Rated Films and Their Directors",
                    labels={"Total_ratings": "Total Ratings", "Label": "Film (Director)"},
                    color="Total_ratings",
                    color_continuous_scale="purp"
                )
                fig2.update_layout(yaxis=dict(autorange="reversed"))
                st.plotly_chart(fig2, use_container_width=True)

                st.markdown("---")

                # --- Chart 4: Most Watched Movies ---
                most_watched = df.groupby("Film_title")["Watches"].sum().nlargest(top_n).reset_index()

                fig4 = px.bar(
                    most_watched,
                    x="Film_title",
                    y="Watches",
                    title=f"Top {top_n} Most Watched Movies",
                    color="Watches",
                    color_continuous_scale="purp"
                )
                fig4.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig4, use_container_width=True)

                st.markdown("---")

                # --- Chart 5: Movies with Most Fans ---
                most_fans = df.groupby(["Film_title", "Genres"])["Fans"].sum().nlargest(top_n).reset_index()

                fig5 = px.bar(
                    most_fans,
                    x="Film_title",
                    y="Fans",
                    title=f"Top {top_n} Most Followed Movies (Fan Count)",
                    color="Fans",
                    color_continuous_scale="purp",
                    labels={"Fans": "Fan Count", "Film_title": "Film Title"},
                    hover_data=["Genres"]  # Show genres only on hover
                )
                fig5.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig5, use_container_width=True)

                st.markdown("---")

                # --- Chart 6: Most Liked Movies ---
                most_liked = df.groupby("Film_title")["Likes"].sum().nlargest(top_n).reset_index()

                fig6 = px.bar(
                    most_liked,
                    x="Film_title",
                    y="Likes",
                    title=f"Top {top_n} Most Liked Movies",
                    color="Likes",
                    color_continuous_scale="purp"
                )
                fig6.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig6, use_container_width=True)

                st.markdown("---")

            elif page == 'Rating-Based Film Rankings':
                
                df = pd.read_csv('datasets/Letterbox Movie Classification Dataset.csv')

                st.title("Rating-Based Film Rankings")
                st.markdown("""
                            Not all stars shine equally in the film world! In this section, I explore how movies 
                            stack up based on the number of 1-star, 3-star, and glorious 5-star ratings they‚Äôve received ‚Äî plus 
                            their total rating count. Whether it's universally adored or a beautiful disaster, each chart helps 
                            uncover how the audience really felt. Spoiler alert: not every "classic" is beloved by the crowd üòÖ.""")
                
                st.write(df)
                st.markdown("---")

                st.subheader("Chart Settings")

                # --- Film count slider ---
                top_n_rating = st.slider("Select number of top films to display", min_value=5, max_value=50, value=25)

                # --- 1 Star Rated ---
                lowest_rated = (
                    df.groupby("Film_title")["Lowest‚òÖ"]
                    .sum()
                    .sort_values(ascending=False)
                    .head(top_n_rating)
                    .reset_index()
                )

                fig1 = px.bar(
                    lowest_rated,
                    x="Film_title",
                    y="Lowest‚òÖ",
                    title=f"Top {top_n_rating} Lowest 1‚òÖ Rated Films",
                    labels={"Lowest‚òÖ": "1 Star Ratings", "Film_title": "Film Title"},
                    color="Lowest‚òÖ",
                    color_continuous_scale="purp"
                )
                fig1.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig1, use_container_width=True)

                st.markdown("---")

                # --- 3 Star Rated ---
                medium_rated = (
                    df.groupby("Film_title")["Medium‚òÖ‚òÖ‚òÖ"]
                    .sum()
                    .sort_values(ascending=False)
                    .head(top_n_rating)
                    .reset_index()
                )

                fig2 = px.bar(
                    medium_rated,
                    x="Film_title",
                    y="Medium‚òÖ‚òÖ‚òÖ",
                    title=f"Top {top_n_rating} Medium 3‚òÖ Rated Films",
                    labels={"Medium‚òÖ‚òÖ‚òÖ": "3 Star Ratings", "Film_title": "Film Title"},
                    color="Medium‚òÖ‚òÖ‚òÖ",
                    color_continuous_scale="purp"
                )
                fig2.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig2, use_container_width=True)

                st.markdown("---")

                # --- 5 Star Rated ---
                highest_rated = (
                    df.groupby("Film_title")["Highest‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ"]
                    .sum()
                    .sort_values(ascending=False)
                    .head(top_n_rating)
                    .reset_index()
                )

                fig3 = px.bar(
                    highest_rated,
                    x="Film_title",
                    y="Highest‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ",
                    title=f"Top {top_n_rating} Highest 5‚òÖ Rated Films",
                    labels={"Highest‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ": "5 Star Ratings", "Film_title": "Film Title"},
                    color="Highest‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ",
                    color_continuous_scale="purp"
                )
                fig3.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig3, use_container_width=True)

                st.markdown("---")

                # --- Total Ratings ---
                total_rated = (
                    df.groupby("Film_title")["Total_ratings"]
                    .sum()
                    .sort_values(ascending=False)
                    .head(top_n_rating)
                    .reset_index()
                )

                fig4 = px.bar(
                    total_rated,
                    x="Film_title",
                    y="Total_ratings",
                    title=f"Top {top_n_rating} Films by Total Ratings",
                    labels={"Total_ratings": "Total Ratings", "Film_title": "Film Title"},
                    color="Total_ratings",
                    color_continuous_scale="purp"
                )
                fig4.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig4, use_container_width=True)

       # elif selected == "Student Habits vs Academic Performance":
            page = st.sidebar.selectbox('Choose a page:',
                                            ['DataSet',
                                            'Basic Analysis',
                                            'Location',
                                            'Payment Methods'])
            if page == 'DataSet':
                df = pd.read_csv("datasets/student_habits_performance.csv")
                st.title("üë©üèª‚Äçüéì Student Habits vs Academic Performance")
                st.markdown("""
                            Ever wondered how much Netflix, sleep, or TikTok scrolling affects your grades? üëÄ
                            
                            This dataset shows how students‚Äô daily habits ‚Äî like how much they study, how often 
                            they attend classes, and whether they‚Äôre early birds or night owls ‚Äî affect their 
                            academic performance.

                            It includes things like:

                            - Study hours (a.k.a. how long they stared at the book before scrolling TikTok again),
                            - Attendance (because showing up is half the battle, right?),
                            - Gender, sleep habits, and of course,
                            - Their Performance score (the final boss of all this effort).

                            I used this dataset to explore if habits actually matter ‚Äî or if some students are 
                            just naturally lucky. Spoiler: attending classes might still be useful üòÖ""")
                st.write(df)
                st.markdown("---")
                st.markdown("## Details")
                st.write("**Title**: Student Habits vs Academic Performance ",
                        "<br>**Rows**: 1000",
                        "<br>**Columns**: 16",
                        unsafe_allow_html=True)
                st.markdown('## Data Dictionary')
                st.markdown("""
                            üìå **student_id** ‚Äì Unique identifier for each student.<br>
                            üìå **age** ‚Äì Student‚Äôs age.<br>
                            üìå **gender** ‚Äì Gender of the student (e.g., Male, Female, Other).<br>
                            üìå **study_hours_per_day** ‚Äì Average number of hours spent studying per day.<br>
                            üìå **social_media_hours** ‚Äì Time spent scrolling and liking ‚Äì includes TikTok, Instagram, etc.<br>
                            üìå **netflix_hours** ‚Äì How long the student spends binge-watching Netflix per day.<br>
                            üìå **part_time_job** ‚Äì Whether the student has a part-time job (Yes/No).<br>
                            üìå **attendance_percentage** ‚Äì Class attendance percentage. Skipping class?<br>
                            üìå **sleep_hours** ‚Äì Average sleep per night.<br>
                            üìå **diet_quality** ‚Äì Rating of their diet (e.g., Poor, Average, Excellent). Pizza doesn‚Äôt count as a veggie!<br>
                            üìå **exercise_frequency** ‚Äì How often the student exercises (times per week).<br>
                            üìå **parental_education_level** ‚Äì Highest education level of the student‚Äôs parents.<br>
                            üìå **internet_quality** ‚Äì Self-rated quality of their internet connection (Slow, Medium, Fast).<br>
                            üìå **mental_health_rating** ‚Äì How the student rates their mental health on a scale.<br>
                            üìå **extracurricular_participation** ‚Äì Whether the student takes part in activities beyond class (clubs, sports, etc.).<br>
                            üìå **exam_score** ‚Äì Final exam score.
                            """, unsafe_allow_html=True)
                st.markdown("---")

            # elif page == 'Basic Analysis':
                
    elif selected == "Tableau":
        selected_optipns = ["1. Sales Dashboard: Food & Beverages",
                            "2. Sales & Customer Analysis",
                            "3. Tube Map Kyiv",
                            "4. Car Sales Analysis"]
        selected = st.selectbox("Select a project", options = selected_optipns)
        st.write("Current selection:", selected)
        if selected == "1. Sales Dashboard: Food & Beverages":
            st.title('Sales Dashboard: Food & Beverages')
            st.markdown("## **[Project - 1](https://public.tableau.com/app/profile/julie.shcherbyna/viz/SalesAnalysis_17551785285480/Dashboard1)**")
            st.write("In this project I used the dataset presented below:")
            df = pd.read_csv('datasets/sales_analysis.csv')
            st.write(df)
            st.markdown("""
                        This dashboard provides a comprehensive overview of **sales performance** across categories, products, regions, and customers.  

                        #### *Key Features*:
                        - **Profit by Categories** ‚Äì bar chart showing which product categories generate the highest profit.  
                        - **Top 5 Products** ‚Äì ranking of the most profitable products.  
                        - **Profit by Regions** ‚Äì comparison of regional sales performance.  
                        - **Regional Share** ‚Äì pie chart visualizing each region‚Äôs contribution to total profit.  
                        - **Quarterly Profit** ‚Äì trend analysis of profit dynamics by quarters across different years.  
                        - **Customer Profitability** ‚Äì profit distribution by customers with percentage contribution.  
                        - **City Information (Map)** ‚Äì geographic visualization displaying profits by major European cities.  

                        #### *Interactivity*:
                        The dashboard includes filters for **city, region, customer, category, and year** to allow users to dynamically explore the dataset and customize the view.  

                        #### *Value*:
                        This dashboard helps to:  
                        - identify the most profitable categories and regions,  
                        - monitor performance trends over time,  
                        - and better understand customer contributions.  
                        """)
            st.image(Image.open("images/t1.png"))

        elif selected == "2. Sales & Customer Analysis":
            st.title('Sales & Customer Analysis')
            st.markdown("## **[Project - 2](https://public.tableau.com/app/profile/julie.shcherbyna/viz/SalesCustomerAnalysis_17552719229610/SalesDashboard)**")
            st.write("In this project I used 4 datasets (Costomers, Location, Orders, Products).")
            st.markdown("The first dashboard was created to depict **Sales Analysis**.")
            st.image(Image.open("images/t2.png"))
            st.markdown("""
                        #### *Created Calculated Fields*:
                        - **Current/Previous Year Metrics** ‚Äì to compare key performance indicators across different years.
                        - **Percentage Difference** ‚Äì to measure growth or decline between periods.
                        - **Min/Max Values** ‚Äì to highlight the highest and lowest sales, profit, and quantity values.
                        
                        #### *Visualizations*:
                        - **Total Sales, Total Profit, Total Quantity** ‚Äì provides an overall view of business performance.
                        - **Sales & Profit by Subcategory** ‚Äì helps identify which product categories contribute most to revenue and profit.
                        - **Sales & Profit Trends over Time** ‚Äì shows performance trends, enabling analysis of seasonality and growth patterns.  
                        """)
            st.markdown("The second dashboard was created to depict **Customer Analysis**.")
            st.image(Image.open("images/t3.png"))
            st.markdown("""
                        #### *Created Calculated Fields*:
                        - **Current/Previous Year Metrics** ‚Äì to compare key performance indicators across different years.
                        - **Percentage Difference** ‚Äì to measure growth or decline between periods.
                        - **Min/Max Values** ‚Äì to highlight the highest and lowest sales, profit, and quantity values.
                        
                        #### *Visualizations*:
                        - **Total Customers, Total Sales per Customer, Total Orders** ‚Äì provides an overview of customer activity and revenue contribution.
                        - **Customer Distribution by Nr. of Orders** ‚Äì shows how frequently customers make purchases, helping identify loyal vs. occasional buyers.
                        - **Top 10 Customers by Profit** ‚Äì highlights the most profitable customers, useful for targeted marketing and relationship management. 
                        """)
            st.markdown("And You can also open **Filter Section**.")
            st.image(Image.open("images/t4.png"))
            st.markdown("""
                        #### *Interactivity*:
                        The dashboard includes filters for **category, sub-category, year, region, state and city** to allow users to dynamically explore the dataset and customize the view.""")

        elif selected == "3. Tube Map Kyiv":
            st.title('Tube Map Kyiv')
            st.markdown("## **[Project - 3](https://public.tableau.com/app/profile/julie.shcherbyna/viz/TubeMapKyiv/sheet2)**")
            st.write("In this project I used the dataset presented below with the name of the station:")
            df = pd.read_csv('datasets/tube.csv')
            st.dataframe(df.drop(columns=["Unnamed: 6", "Unnamed: 7", "Unnamed: 8", "Unnamed: 9"])) 
            st.markdown("""
                        #### *Created Calculated Fields*:
                        - **Geocoded Station Names** ‚Äì to map each Kyiv station to its geographic location.

                        #### *Visualizations*:
                        - **Interactive Map of Kyiv Stations** ‚Äì displays all stations on a city map, enabling spatial analysis and exploration.
                        """)
            st.image(Image.open("images/t5.png"))

        elif selected == "4. Car Sales Analysis":
            st.title('Car Sales Analysis')
            st.markdown("## **[Project - 4](https://public.tableau.com/app/profile/julie.shcherbyna/viz/Book1_17555904414740/Dashboard1)**")
            st.write("In this project I used the dataset presented below:")
            df = pd.read_csv('datasets/Car-Sales-Data.csv')
            st.write(df)
            st.markdown("""
                        #### *Created Calculated Fields*:
                        - **YTD/PYTD Sales, Cars Sold, Average Price** ‚Äì to track year-to-date performance compared to the previous year.
                        - **YoY Sales, Cars Sold, Average Price** ‚Äì to measure year-over-year growth and performance trends.

                        #### *Visualizations*:
                        - **YTD Sales Monthly Trend** ‚Äì shows how sales evolve month by month within the current year.
                        - **Sales Distribution by Region** ‚Äì provides insights into regional performance differences.
                        - **Total Sales by Car Body Type** ‚Äì highlights which car body types contribute the most to sales volume.
                        - **Annual Revenue Distribution by Company** ‚Äì compares revenue across companies on a yearly basis.
                        """)
            st.image(Image.open("images/t6.png"))

    elif selected == "R":
        selected_optipns = ["Data Input and Output in R",
                            "Data Loading and its Description in R",
                            "Descriptive Analytics",
                            "Probability Distributions of Random Variables",
                            "Statistical Hypothesis Testing",
                            "Exploratory Analytics. Using Basic Graphics Package",
                            "Advanced Visual Analytics Tools"]
        selected = st.selectbox("Select a project", options = selected_optipns)
        st.write("Current selection:", selected)
        if selected == "Data Input and Output in R":
            st.title("Data Input and Output in R")
            # 1
            st.write("**1. A code that asks the user for their name and displays a message like 'Hello, name!**")
            code = """
            name <- readline(prompt = "Enter your name: ")
            cat("Hello, ", name, "!", sep = "")"""
            st.code(code, language='r')
            # 2.1
            st.write("**2.1 A code that asks the user to input two floating-point numbers (one number ‚Äì one input) "
            "and displays their sum on the screen. The user enters floating-point numbers using a dot as the decimal separator.**")
            code = """
            num1 <- as.numeric(readline(prompt = "–í–≤–µ–¥—ñ—Ç—å –ø–µ—Ä—à–µ –¥—Ä–æ–±–æ–≤–µ —á–∏—Å–ª–æ (—á–µ—Ä–µ–∑ –∫—Ä–∞–ø–∫—É): ")) 
num2 <- as.numeric(readline(prompt = "–í–≤–µ–¥—ñ—Ç—å –¥—Ä—É–≥–µ –¥—Ä–æ–±–æ–≤–µ —á–∏—Å–ª–æ (—á–µ—Ä–µ–∑ –∫—Ä–∞–ø–∫—É): "))
            
sum_result <- num1 + num2
cat("–°—É–º–∞ —á–∏—Å–µ–ª", num1, "—Ç–∞", num2, "–¥–æ—Ä—ñ–≤–Ω—é—î", sum_result)"""
            st.code(code, language='r')
            # 2.2
            st.write("**2.2 A code that asks the user to input two floating-point numbers (one number ‚Äì one input) "
            "and displays their sum on the screen. The user enters floating-point numbers using a comma as the decimal separator.**")
            code = """
            num1 <- readline(prompt = "–í–≤–µ–¥—ñ—Ç—å –ø–µ—Ä—à–µ –¥—Ä–æ–±–æ–≤–µ —á–∏—Å–ª–æ (—á–µ—Ä–µ–∑ –∫–æ–º—É): ")
num2 <- readline(prompt = "–í–≤–µ–¥—ñ—Ç—å –¥—Ä—É–≥–µ –¥—Ä–æ–±–æ–≤–µ —á–∏—Å–ª–æ (—á–µ—Ä–µ–∑ –∫–æ–º—É): ")

num1 <- as.numeric(gsub(",", ".", num1))
num2 <- as.numeric(gsub(",", ".", num2))

sum_result <- num1 + num2
cat("–°—É–º–∞ —á–∏—Å–µ–ª", num1, "—Ç–∞", num2, "–¥–æ—Ä—ñ–≤–Ω—é—î", sum_result)"""
            st.code(code, language='r')
            # 3
            st.write("**3. A code that asks the user to enter a speed value in kilometers per hour and displays the speed in meters per second.**")
            code = """
            kmph <- as.numeric(readline(prompt = "–í–≤–µ–¥—ñ—Ç—å —à–≤–∏–¥–∫—ñ—Å—Ç—å –≤ –∫—ñ–ª–æ–º–µ—Ç—Ä–∞—Ö –Ω–∞ –≥–æ–¥–∏–Ω—É (–∫–º/–≥–æ–¥): "))

mps <- kmph * (1000 / 3600)

cat("–®–≤–∏–¥–∫—ñ—Å—Ç—å –≤ –º–µ—Ç—Ä–∞—Ö –Ω–∞ —Å–µ–∫—É–Ω–¥—É (–º/c) –¥–æ—Ä—ñ–≤–Ω—é—î ", mps)"""
            st.code(code, language='r')
            # 4
            st.write("**4. Create a table with meaningful data in Excel that contains 5 rows and 3 columns with both numeric and character data, " \
            "as well as column names. Read this file in R from a CSV format.**")
            code = """
            data <- read.csv("/Users/julie/Desktop/lab2/olympics.csv", header = TRUE)
            data"""
            st.code(code, language='r')
            df_o = pd.read_csv('datasets/olympics.csv', sep=';')
            st.write(df_o)
            # 5.1
            st.write("**5.1 Create a data frame in R with meaningful data, containing 5 rows and 3 columns with both numeric and character data, " \
            "as well as column headers. Enter the table values by initializing three separate vectors.**")
            code = """
            ISBN <- c("978-617-679-832-3", "978-617-8203-81-8", "978-617-548-147-9", "978-617-8023-80-5", "978-617-7853-82-3")
            Title <- c("–Ø –±–∞—á—É –≤–∞—Å —Ü—ñ–∫–∞–≤–∏—Ç—å –ø—ñ—Ç—å–º–∞", "–ó–∞ –ø–µ—Ä–µ–∫–æ–ø–æ–º —î –∑–µ–º–ª—è", "–î—ñ–º —É –≤–æ–ª–æ—à–∫–æ–≤–æ–º—É –º–æ—Ä—ñ", "–í–∞–≤–∏–ª–æ–Ω", "–ú–∞–∫–æ–≤–∞ –≤—ñ–π–Ω–∞")
            Author <- c("–Ü–ª–∞—Ä—ñ–æ–Ω –ü–∞–≤–ª—é–∫", "–ê–Ω–∞—Å—Ç–∞—Å—ñ—è –õ–µ–≤–∫–æ–≤–∞", "–¢—ñ –î–∂–µ–π –ö–ª—É–Ω", "–†–µ–±–µ–∫–∫–∞ –ö–≤–∞–Ω", "–†–µ–±–µ–∫–∫–∞ –ö–≤–∞–Ω")

            books <- data.frame(ISBN, Title, Author, stringsAsFactors = FALSE)
            print(books)"""
            st.code(code, language='r')
            # 5.2
            st.write("**5.2 Enter the table values using a text editor and the edit() function.**")
            code = """
            new_books <- edit(books)
            print(new_books)"""
            st.code(code, language='r')
            st.image(Image.open("images/r1.png"))
            # 5.3
            st.write("**5.3 At first, export the result to a text file using the write.table() function. Then export the result to an " \
            "MS Excel file using the functions from the xlsReadWrite package.**")
            code = """
            write.table(new_books, file = "/Users/julie/Desktop/lab2/books.txt", sep = ",")
            
            write_xlsx(new_books, "/Users/julie/Desktop/lab2/books.xlsx")"""
            st.code(code, language='r')
            st.image(Image.open("images/r2.png"))
        
        elif selected == "Data Loading and its Description in R":
            st.title("Data Loading and its Description in R")
            # 1
            st.write("**1. Load data from the file firtree.csv, which contains the results of a fictional survey of visitors " \
            "to a Christmas tree market, and store it in the variable tree.**")
            code = """
            tree <- read.csv("firtree.csv", encoding = "UTF-8", header = TRUE, sep = ","  )
            head(tree)"""
            st.code(code, language='r')
            df_f = pd.read_csv('datasets/firtree.csv')
            st.write(df_f)
            # 2
            st.write("**2. Display the first rows of the dataset using the head() function.**")
            code = """
            head(tree)"""
            st.code(code, language='r')
            st.write(df_f.head())
            # 3
            st.write("**3. Display the first 8 rows of the tree dataset.**")
            code = """
            head(tree, 8)"""
            st.code(code, language='r')
            st.write(df_f.head(8))
            # 4
            st.write("**4. Display the last rows of the dataset using the tail() function.**")
            code = """
            tail(tree)"""
            st.code(code, language='r')
            st.write(df_f.tail())
            # 5
            st.write("**5. Display the last 10 rows in a separate window using the View() function.**")
            code = """
            last_10_rows <- tail(tree, 10)
            View(last_10_rows)"""
            st.code(code, language='r')
            st.write(df_f.tail(10))
            # 6
            st.write("**6. Determine the dimensions of the dataset (number of rows and columns) using the dim() function.**")
            code = """
            dim(tree)"""
            st.code(code, language='r')
            rows, columns = df_f.shape
            st.write(f"The dataset has {rows} rows and {columns} columns.")
            # 7
            st.write("**7. Determine the number of rows and columns separately using the nrow() and ncol() functions.**")
            code = """
            nrow(tree)
            ncol(tree)"""
            st.code(code, language='r')
            st.write(f"The dataset has {df_f.shape[0]} rows and {df_f.shape[1]} columns")
            # 8
            st.write("**8. Get the full technical structure of the dataset using the str() function. What type of data is stored in each field of the dataset?**")
            code = """
            str(tree)"""
            st.code(code, language='r')
            st.write(df_f.dtypes)
            st.markdown(""" 
            - **integer** ‚Äì whole number
            - **character** ‚Äì string (text) type""")
            # 9
            st.write("**9. Check if the dataset contains any missing values.**")
            code = """
            any(is.na(tree))
            sum(is.na(tree))"""
            st.code(code, language='r')
            has_na = df_f.isnull().values.any()
            st.write(f"–ß–∏ —î –ø—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —É –¥–∞—Ç–∞—Å–µ—Ç—ñ? **{has_na}**")

            total_na = df_f.isnull().sum().sum()
            st.write(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å: **{total_na}**")
            # 10
            st.write("**10. Knowing the columns and row indices (IDs) that contain missing values, print the specific 'empty' elements to confirm that they are truly missing.**")
            code = """
            wish_47 <- tree$wish[47]
wish_20 <- tree$wish[20]

print(paste("–ó–Ω–∞—á–µ–Ω–Ω—è —É —Ä—è–¥–∫—É 47 —Å—Ç–æ–≤–ø—Ü—è 'wish':", wish_47))
print(paste("–ó–Ω–∞—á–µ–Ω–Ω—è —É —Ä—è–¥–∫—É 20 —Å—Ç–æ–≤–ø—Ü—è 'wish':", wish_20))"""
            st.code(code, language='r')
            # 11
            st.write("**11. Replace missing values with NA during data import so that R treats them as valid missing data.**")
            code = """
            tree <- read.csv("firtree.csv", encoding = "UTF-8", header = TRUE, sep = ",", na.strings = "") 
            View(tree)"""
            st.code(code, language='r')
            # 12
            st.write("**12. Count the number of rows in the dataset that contain missing values..**")
            code = """
            empty <- complete.cases(tree)
            num_rows_with_na <- sum(!empty)
            print(paste("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤, —â–æ –º—ñ—Å—Ç—è—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:", num_rows_with_na))"""
            st.code(code, language='r')
            # 13
            st.write("**13. Since R treats TRUE as 1 and FALSE as 0, use sum(complete.cases()) to count the complete rows.**")
            code = """
            num_complete_rows <- sum(complete.cases(tree))
print(paste("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –±–µ–∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å:", num_complete_rows))

num_rows_with_na <- sum(!complete.cases(tree))
print(paste("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏:", num_rows_with_na))"""
            st.code(code, language='r')
            # 14
            st.write("**14. Use another way to identify missing values ‚Äî the is.na() function.**")
            code = """
            missing_values <- is.na(tree)
head(missing_values)

total_na <- sum(missing_values)
print(paste("–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å:", total_na))"""
            st.code(code, language='r')
            # 15
            st.write("**15. Load the installed libraries (VIM and mice) and display plots that show which variables have the most " \
            "missing values and what the pattern of missing values in the dataset looks like.**")
            code = """
            library(VIM)
            aggr(tree, numbers = TRUE, prop = TRUE, cex.axis = 0.7)"""
            st.code(code, language='r')
            st.image(Image.open("images/r3.png"))
            st.markdown("""
                        - The plot on the left shows how frequently missing values occur in each variable. 
                        
                        - The plot on the right shows in which combinations these missing values appear.""")
            # 16 
            st.write("**16. Use the matrixplot function from the mice package.**")
            code = """
            library(mice)
            matrixplot(tree)"""
            st.code(code, language='r')
            st.image(Image.open("images/r4.png"))
            st.markdown("""
                        The resulting plot visualizes the completeness of observations (missing values are marked in red, while the rest are 
                        observed values; the darker the color, the higher the value). 
                        The vertical axis represents the row number in the data frame, i.e., the observation ID.""")
            # 17
            st.write("**17. Use the matrixplot() function to visualize missing values in the dataset test.**")
            code = """
            library(mice)
test <- cbind.data.frame(a = c(NA, 2, 3), b = c(NA, NA, 1))
sum(!complete.cases(test))
sum(is.na(test))
matrixplot(test)"""
            st.code(code, language='r')
            st.image(Image.open("images/r5.png"))

        elif selected == "Descriptive Analytics":
            st.title("Descriptive Analytics")
            # 1
            st.write("**1. Create a variable lvl that contains the following numeric elements. Now perform the sum, mean, median, length and sd.**")
            code = """
            lvl <- c(8, 10, 10, 1, 10, 10, 8, 12, 1, 12)

            cat("–°—É–º–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", sum(lvl))
            cat("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", mean(lvl))
            cat("–ú–µ–¥—ñ–∞–Ω–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", median(lvl))
            cat("–î–æ–≤–∂–∏–Ω–∞ –∑–º—ñ–Ω–Ω–æ—ó:", length(lvl))
            cat("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:", sd(lvl))
            cat("–û–∫—Ä—É–≥–ª–µ–Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:", round(sd(lvl), 2))"""
            st.code(code, language='r')
            lvl = [8, 10, 10, 1, 10, 10, 8, 12, 1, 12]
            st.write("–°—É–º–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", np.sum(lvl))
            st.write("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", np.mean(lvl))
            st.write("–ú–µ–¥—ñ–∞–Ω–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:", np.median(lvl))
            st.write("–î–æ–≤–∂–∏–Ω–∞ –∑–º—ñ–Ω–Ω–æ—ó:", len(lvl))
            st.write("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:", np.std(lvl, ddof=1))
            st.write("–û–∫—Ä—É–≥–ª–µ–Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:", round(np.std(lvl, ddof=1), 2))
            # 2
            st.markdown("""**2. Imagine you are tracking the mileage of a car after each refueling. 
                        After the last eight refuelings, the mileage was recorded. Enter these numbers into R. What do you get as a result? We see the number of miles between refuelings. 
                        Use the max(), mean(), and min() functions to calculate the maximum, 
                        average, and minimum mileage between refuelings.**""")
            code = """
            mileage <- c(65311, 65624, 65908, 66219, 66499, 66821, 67145, 67447)

            cat("–ü—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", diff(mileage))

            cat("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", max(diff(mileage)))
            cat("–°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", mean(diff(mileage)))
            cat("–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", min(diff(mileage)))"""
            st.code(code, language='r')
            mileage = np.array([65311, 65624, 65908, 66219, 66499, 66821, 67145, 67447])
            mileage_diff = np.diff(mileage)
            st.write("–ü—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", ", ".join(str(x) for x in mileage_diff))
            st.write()
            st.write("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", mileage_diff.max())
            st.write("–°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", mileage_diff.mean())
            st.write("–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–±—ñ–≥ –º—ñ–∂ –∑–∞–ø—Ä–∞–≤–∫–∞–º–∏:", mileage_diff.min())
            # 3
            st.markdown("""**3. Suppose you need to buy equipment for your factory and you have 2 options to choose from. 
                        Before making the final decision, you collected 10 performance measurements for each option. 
                        If your final decision was based on these measurements, which equipment would you choose?**""")
            code = """
            equip1 <- c(151.2, 150.5, 149.2, 147.5, 152.9, 152.0, 151.3, 149.7, 149.4, 150.7)
            equip2 <- c(151.9, 151.4, 150.3, 151.2, 151.0, 150.2, 151.2, 151.4, 150.4, 151.7)

            cat("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 1:", mean(equip1))
            cat("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 2:", mean(equip2))

            if (mean(equip1) > mean(equip2)) {
                cat("–£—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 1 —î –∫—Ä–∞—â–∏–º –≤–∏–±–æ—Ä–æ–º.")
            } else if (mean(equip1) < mean(equip2)) {
                cat("–£—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 2 —î –∫—Ä–∞—â–∏–º –≤–∏–±–æ—Ä–æ–º.")
            } else {
                cat("–û–±–∏–¥–≤–∞ —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–µ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è.")
            }"""
            st.code(code, language='r')
            equip1 = np.array([151.2, 150.5, 149.2, 147.5, 152.9, 152.0, 151.3, 149.7, 149.4, 150.7])
            equip2 = np.array([151.9, 151.4, 150.3, 151.2, 151.0, 150.2, 151.2, 151.4, 150.4, 151.7])
            mean1 = np.mean(equip1)
            mean2 = np.mean(equip2)
            st.write("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 1:", round(mean1, 2))
            st.write("–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 2:", round(mean2, 2))
            if mean1 > mean2:
                st.success("–£—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 1 —î –∫—Ä–∞—â–∏–º –≤–∏–±–æ—Ä–æ–º.")
            elif mean1 < mean2:
                st.success("–£—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è 2 —î –∫—Ä–∞—â–∏–º –≤–∏–±–æ—Ä–æ–º.")
            else:
                st.info("–û–±–∏–¥–≤–∞ —É—Å—Ç–∞—Ç–∫—É–≤–∞–Ω–Ω—è –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–µ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è.")
            # 4
            st.markdown("""**4. Create a vector of the heights of twenty students in a class. Which student is in the middle 
                        of this list (by height)? Which height is the most frequent? Calculate these values using descriptive analytics tools in R.**""")
            code = """
            heights <- c(100, 106, 121, 111, 109, 111, 103, 117, 114, 108, 111, 105, 120, 116, 104, 103, 108, 111, 101, 120)
            # –Ø–∫–∏–π —É—á–µ–Ω—å –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –ø–æ—Å–µ—Ä–µ–¥–∏–Ω—ñ —Ü—å–æ–≥–æ —Å–ø–∏—Å–∫—É (–∑–∞ –∑—Ä–æ—Å—Ç–æ–º)?
            median_height <- median(heights)
            cat("–ó—Ä—ñ—Å—Ç —É—á–Ω—è –ø–æ—Å–µ—Ä–µ–¥–∏–Ω—ñ:", median_height)

            # –Ø–∫–∏–π –∑—Ä—ñ—Å—Ç —î –Ω–∞–π–±—ñ–ª—å—à –ø–æ–ø—É–ª—è—Ä–Ω–∏–º?
            mode_height <- as.numeric(names(sort(table(heights), decreasing = TRUE)[1]))
            cat("–ù–∞–π–±—ñ–ª—å—à –ø–æ–ø—É–ª—è—Ä–Ω–∏–π –∑—Ä—ñ—Å—Ç:", mode_height)"""
            st.code(code, language='r')
            heights = [100, 106, 121, 111, 109, 111, 103, 117, 114, 108, 111, 105, 120, 116, 104, 103, 108, 111, 101, 120]
            sorted_heights = sorted(heights)
            n = len(sorted_heights)
            if n % 2 == 1:
                median_height = sorted_heights[n // 2]
            else:
                median_height = (sorted_heights[n // 2 - 1] + sorted_heights[n // 2]) / 2
            counts = {}
            for h in heights:
                counts[h] = counts.get(h, 0) + 1
            mode_height = max(counts, key=counts.get)
            st.write("–ó—Ä—ñ—Å—Ç —É—á–Ω—è –ø–æ—Å–µ—Ä–µ–¥–∏–Ω—ñ (–º–µ–¥—ñ–∞–Ω–∞):", median_height)
            st.write("–ù–∞–π–±—ñ–ª—å—à –ø–æ–ø—É–ª—è—Ä–Ω–∏–π –∑—Ä—ñ—Å—Ç (–º–æ–¥–∞):", mode_height)
            # 5
            st.markdown("""**5. Calculate quantiles, quartiles, the interquartile range (IQR), and the standard error based on the data 
                        from the previous task.**""")
            code = """
            cat("–ö–≤–∞–Ω—Ç–∏–ª—ñ:", quantile(heights))

            quartiles <- quantile(heights, probs = c(0.25, 0.75))
            cat("–ü–µ—Ä—à–∏–π –∫–≤–∞—Ä—Ç–∏–ª—å (Q1):", quartiles[1])
            cat("–ü–µ—Ä—à–∏–π –∫–≤–∞—Ä—Ç–∏–ª—å (Q2):", median(heights))
            cat("–¢—Ä–µ—Ç—ñ–π –∫–≤–∞—Ä—Ç–∏–ª—å (Q3):", quartiles[2])

            cat("–ú—ñ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–∏–π —Ä–æ–∑–º–∞—Ö (IQR):", IQR(heights))

            cat("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –ø–æ—Ö–∏–±–∫–∞:", sd(heights))"""
            st.code(code, language='r')
            heights = [100, 106, 121, 111, 109, 111, 103, 117, 114, 108,
                    111, 105, 120, 116, 104, 103, 108, 111, 101, 120]
            quantiles = np.quantile(heights, [0, 0.25, 0.5, 0.75, 1])
            Q1 = quantiles[1]
            Q2 = quantiles[2]
            Q3 = quantiles[3]
            IQR = Q3 - Q1
            std_dev = np.std(heights, ddof=1)
            se = std_dev / np.sqrt(len(heights))
            df_quartiles = pd.DataFrame({
                'Percentile': ['0%', '25%', '50%', '75%', '100%'],
                'Height': quantiles
            })
            st.write("–ö–≤–∞—Ä—Ç–∏–ª—ñ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –≤—ñ–¥—Å–æ—Ç–∫–∏")
            st.table(df_quartiles)
            st.write("–ü–µ—Ä—à–∏–π –∫–≤–∞—Ä—Ç–∏–ª—å (Q1):", Q1)
            st.write("–ú–µ–¥—ñ–∞–Ω–∞ (Q2):", Q2)
            st.write("–¢—Ä–µ—Ç—ñ–π –∫–≤–∞—Ä—Ç–∏–ª—å (Q3):", Q3)
            st.write("–ú—ñ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–∏–π —Ä–æ–∑–º–∞—Ö (IQR):", IQR)
            st.write("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:", std_dev)
            st.write("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –ø–æ—Ö–∏–±–∫–∞:", se)
            # 6 
            st.markdown("""**6. Calculate the standard deviation for the data from task 4 using the formula, 
                        and compare the result with the sd() function. Are the results the same?**""")
            code = """
            cat("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é sd():", sd(heights))

            variance <- sum((heights - mean(heights))^2) / length(heights)  
            std_dev <- sqrt(variance)  

            cat("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é:", std_dev)

            if (identical(round(sd(heights), 4), round(std_dev, 4))) {
            cat("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ.")
            } else {
            cat("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä—ñ–∑–Ω—è—Ç—å—Å—è.")"""
            st.code(code, language='r')
            heights = [100, 106, 121, 111, 109, 111, 103, 117, 114, 108,
                    111, 105, 120, 116, 104, 103, 108, 111, 101, 120]
            std_numpy = np.std(heights, ddof=1)
            mean_height = np.mean(heights)
            variance_manual = sum((x - mean_height) ** 2 for x in heights) / len(heights)
            std_manual = np.sqrt(variance_manual)
            st.write("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é np.std:", std_numpy)
            st.write("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é:", std_manual)
            if round(std_numpy, 4) == round(std_manual, 4):
                st.success("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ.")
            else:
                st.warning("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä—ñ–∑–Ω—è—Ç—å—Å—è.")
            # 7
            st.markdown("""**7. Load data from two columns of a table into two vectors. Determine the skewness of the data 
                        visually using exploratory analytics tools, and using descriptive statistics metrics. Interpret the results.**""")
            code = """
            column1 <- c(212, 869, 220, 654, 511, 624, 420, 121, 428, 865, 
                        799, 405, 230, 670, 870, 366, 99, 55, 489, 312, 
                        493, 163, 221, 84, 144, 48, 375, 86, 168, 100)

            column2 <- c(586, 760, 495, 678, 559, 415, 370, 659, 119, 288, 
                        241, 787, 522, 207, 160, 526, 656, 848, 720, 676, 
                        581, 929, 653, 661, 770, 800, 529, 975, 995, 947)

            par(mfrow = c(1, 2))
            hist(column1, main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ –∫–æ–ª–æ–Ω–∫–∏ 1", xlab = "–ó–Ω–∞—á–µ–Ω–Ω—è", col = "lightblue", border = "black")
            hist(column2, main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ –∫–æ–ª–æ–Ω–∫–∏ 2", xlab = "–ó–Ω–∞—á–µ–Ω–Ω—è", col = "lightgreen", border = "black")

            install.packages("moments")
            library(moments)

            cat("–°–∫–æ—à–µ–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 1:", skewness(column1))
            cat("–°–∫–æ—à–µ–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 2:", skewness(column2))

            cat("C–∫–æ—à–µ–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 1 –¥–æ—Ä—ñ–≤–Ω—é—î 1.5, –∞ –∫–æ–ª–æ–Ω–∫–∏ 2 –¥–æ—Ä—ñ–≤–Ω—é—î -0.5, 
                —Ü–µ –≤–∫–∞–∑—É—î –Ω–∞ —Ç–µ, —â–æ –ø–µ—Ä—à–∞ –∫–æ–ª–æ–Ω–∫–∞ –º–∞—î –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é —Å–∫–æ—à–µ–Ω—ñ—Å—Ç—å, 
                –∞ –¥—Ä—É–≥–∞ - –ª—ñ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é —Å–∫–æ—à–µ–Ω—ñ—Å—Ç—å.")"""
            st.code(code, language='r')
            column1 = [212, 869, 220, 654, 511, 624, 420, 121, 428, 865, 
                    799, 405, 230, 670, 870, 366, 99, 55, 489, 312, 
                    493, 163, 221, 84, 144, 48, 375, 86, 168, 100]

            column2 = [586, 760, 495, 678, 559, 415, 370, 659, 119, 288, 
                    241, 787, 522, 207, 160, 526, 656, 848, 720, 676, 
                    581, 929, 653, 661, 770, 800, 529, 975, 995, 947]
            skew1 = skew(column1)
            skew2 = skew(column2)
            st.write("–°–∫–æ—à–µ–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 1:", skew1)
            st.write("–°–∫–æ—à–µ–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 2:", skew2)
            st.markdown("""
                        –°–∫–æ—à–µ–Ω—ñ—Å—Ç—å **–∫–æ–ª–æ–Ω–∫–∏ 1** –¥–æ—Ä—ñ–≤–Ω—é—î –ø—Ä–∏–±–ª–∏–∑–Ω–æ **1.5**, –∞ **–∫–æ–ª–æ–Ω–∫–∏ 2** ‚Äî –ø—Ä–∏–±–ª–∏–∑–Ω–æ **-0.5**, 
                        —â–æ –≤–∫–∞–∑—É—î –Ω–∞ —Ç–µ, —â–æ –ø–µ—Ä—à–∞ –∫–æ–ª–æ–Ω–∫–∞ –º–∞—î **–ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é —Å–∫–æ—à–µ–Ω—ñ—Å—Ç—å**, 
                        –∞ –¥—Ä—É–≥–∞ ‚Äî **–ª—ñ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é —Å–∫–æ—à–µ–Ω—ñ—Å—Ç—å**.""")
            st.image(Image.open("images/r6.png"))

        elif selected == "Probability Distributions of Random Variables":
            st.title("Probability Distributions of Random Variables")
            # 1
            st.write("""**1. An insurance company has 500 life insurance contracts with 500 clients. 
                     The probability of an insured event occurring in this group of clients is the same and equals 0.002. 
                     The payout amount to each client is the same and equals 40,000 UAH. What is the probability distribution 
                     law of the random variable representing the number of insured events? Plot the distribution graph of the total 
                     payout amount by the insurance company. Calculate the probability that the insurance company will pay its clients 
                     no more than 100,000 UAH.**""")
            code = """
            n <- 500 
            p <- 0.002
            per_person <- 40000 
            max <- 100000 

            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç—Ä–∞—Ö–æ–≤–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∞ –∑–∞ –∑–∞–∫–æ–Ω–æ–º –±—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ–º—É —Ä–æ–∑–ø–æ–¥—ñ–ª—É
            probs <- dbinom(0:n, size = n, prob = p)
            payout <- 0:n * per_person

            plot(payout, probs, type = "l", 
                main = "–†–æ–∑–ø–æ–¥—ñ–ª —Å—É–º–∞—Ä–Ω–∏—Ö –≤–∏–ø–ª–∞—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ—ó –∫–æ–º–ø–∞–Ω—ñ—ó",
                xlab = "–°—É–º–∞ –≤–∏–ø–ª–∞—Ç (–≥—Ä–Ω)", ylab = "–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å")

            prob_payout <- pbinom(max/per_person, size = n, prob = p)
            cat("–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –≤–∏–ø–ª–∞—Ç–∏ –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∞—Ç—å 100 —Ç–∏—Å. –≥—Ä–Ω:", prob_payout)"""
            st.code(code, language='r')
            n = 500
            p = 0.002
            per_person = 40000
            max_payout = 100000
            x = np.arange(n + 1)
            probs = binom.pmf(x, n, p)
            payout = x * per_person
            max_cases = max_payout // per_person
            prob_payout = binom.cdf(max_cases, n, p)
            st.write("–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –≤–∏–ø–ª–∞—Ç–∏ –Ω–µ –ø–µ—Ä–µ–≤–∏—â–∞—Ç—å 100 —Ç–∏—Å. –≥—Ä–Ω:", prob_payout)
            st.image(Image.open("images/r7.png"))
            # 2
            st.write("**2. Plot the binomial distribution with parameters n = 1000 and p = 0.25.**")
            code = """
            n <- 1000  
            p <- 0.25  
            size <- 10000

            rbinom(size, size = n, prob = p)
            hist(rbinom(size, size = n, prob = p), col = "lightblue",
                main = "–ë—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏–π –∑–∞–∫–æ–Ω —Ä–æ–∑–ø–æ–¥—ñ–ª—É")"""
            st.code(code, language='r')
            st.image(Image.open("images/r8.png"))
            # 3
            st.write("**3. Generate 100 random numbers and graphically display the Poisson distribution with parameter Œª = 10.**")
            code = """
            rpois(n=100, lambda=10)

            plot(rpois(n=100, lambda=10), type='s', col = "lightgreen",
                ylab = "–†–æ–∑–ø–æ–¥—ñ–ª",
                main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –ü—É–∞—Å—Å–æ–Ω–∞")"""
            st.code(code, language='r')
            st.image(Image.open("images/r9.png"))
            # 4
            st.write("""**4. Calculate the probability that a random variable distributed according to the Poisson distribution 
                     with parameter Œª = 7 takes values in the interval [5; 10].**""")
            code = """
            cat("–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–µ–ª–∏—á–∏–Ω–∞ –±—É–¥–µ –≤ –º–µ–∂–∞—Ö [5; 10]:"
                , ppois(10, lambda = 7) - ppois(5, lambda = 7))"""
            st.code(code, language='r')
            lambda_ = 7
            prob = poisson.cdf(10, lambda_) - poisson.cdf(4, lambda_)
            st.write("–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–µ–ª–∏—á–∏–Ω–∞ –±—É–¥–µ –≤ –º–µ–∂–∞—Ö [5; 10]:", prob)
            # 5
            st.write("""**5. Plot the cumulative distribution function and the probability density function 
                     of a uniformly distributed random variable on the interval [15; 25].**""")
            code = """
            ?dunif
            ru = runif(n = 100, max = 100)
            hist(ru, main = '–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞', col = 'darkolivegreen1')

            plot(dunif(ru, min = 15, max = 25), type = 'h', main = '–©—ñ–ª—å–Ω—ñ—Å—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª—É')
            plot(punif(ru, min = 15, max = 25), type = 'h', main = '–§—É–Ω–∫—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É')"""
            st.code(code, language='r')
            st.image(Image.open("images/r10.png"))
            st.image(Image.open("images/r11.png"))
            st.image(Image.open("images/r12.png"))
            # 6
            st.write("""**6. Generate 500 uniformly distributed random variables in the range from 50 to 100. 
                     Graphically display the resulting distribution.**""")
            code = """
            u <- runif(n= 0:500, min = 50, max = 100)
            names(u) = c(0:500)
            u

            hist(u, col = "darkslategray", border = "black",
                main = "–†–æ–∑–ø–æ–¥—ñ–ª –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –≤–µ–ª–∏—á–∏–Ω",
                xlab = "–ó–Ω–∞—á–µ–Ω–Ω—è", ylab = "–ß–∞—Å—Ç–æ—Ç–∞")"""
            st.code(code, language='r')
            st.image(Image.open("images/r13.png"))
            # 7
            st.write("""**7. Generate a sample of 50 values of a random variable distributed according to the exponential 
                     distribution law with parameter Œª = 105. Graphically display the exponential distribution with this parameter.**""")
            code = """
            exp_data <- dexp(x=0:500, rate = 105)

            plot(exp_data, type = 'o', col = "palegoldenrod",
                main = "–ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–∏–π –∑–∞–∫–æ–Ω —Ä–æ–∑–ø–æ–¥—ñ–ª—É")
            warnings()"""
            st.code(code, language='r')
            st.image(Image.open("images/r14.png"))
            # 8
            st.write("""**8. Calculate the probability that a random variable distributed according to the exponential distribution 
                     law with parameter Œª = 10 does not exceed the value 7.**""")
            code = """
            ?pexp
            pexp(7, rate = 0.1)
            plot(pexp(7, rate = 0.1), type = 'h', lwd = 3, col = 'plum2')
            """
            st.code(code, language='r')
            st.image(Image.open("images/r15.png"))
            # 9
            st.write("**9. Output a set of 200 random numbers distributed according to the standard normal distribution.**")
            code = """
            normal_data <- rnorm(200, mean = 0, sd = 1)
            normal_data"""
            st.code(code, language='r')
            # 10
            st.write("""**10. Calculate the probability that a random variable distributed according to the normal distribution with parameters Œ± = 20 and œÉ = 87 takes values in the interval [14; 70]. Plot the cumulative distribution function and the probability density function of such distribution.**""")
            code = """
            prob <- pnorm(70, mean = 20, sd = 87) - pnorm(14, mean = 20, sd = 87)
            cat("–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–µ–ª–∏—á–∏–Ω–∞ –Ω–∞–±—É–≤–∞—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω—å –∑ –ø—Ä–æ–º—ñ–∂–∫—É [14; 70]:", prob, "\n")

            a = seq(14, 70, by = 1)
            plot(a, dnorm(a, 20, 87), type = 's', col = 'peru',
                main = '–ù–æ—Ä–º–∞–ª—å–Ω–∏–π –∑–∞–∫–æ–Ω —Ä–æ–∑–ø–æ–¥—ñ–ª—É')
            plot(a, dnorm(a, 20, 87),
                main = '–ù–æ—Ä–º–∞–ª—å–Ω–∏–π –∑–∞–∫–æ–Ω —Ä–æ–∑–ø–æ–¥—ñ–ª—É')"""
            st.code(code, language='r')
            mu = 20
            sigma = 87
            a, b = 14, 70
            prob = norm.cdf(b, loc=mu, scale=sigma) - norm.cdf(a, loc=mu, scale=sigma)
            st.write(f"–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–µ–ª–∏—á–∏–Ω–∞ –Ω–∞–±—É–≤–∞—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω—å –∑ –ø—Ä–æ–º—ñ–∂–∫—É [{a}; {b}]:", prob)
            st.image(Image.open("images/r16.png"))
            st.image(Image.open("images/r17.png"))

        elif selected == "Statistical Hypothesis Testing":
            st.title("Statistical Hypothesis Testing")
            st.subheader("Part 1")
            # 1
            st.write("**1. Plot the density graph of the random variable 'weight of the box' and visually assess its similarity to the normal distribution.**")
            code = """
            library(ggplot2)

            hist(butter, col='cornflowerblue', main='–ì—Ä–∞—Ñ—ñ–∫ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –≤–∞–≥–∏',
                xlab='–í–∞–≥–∞ –∫–æ—Ä–æ–±–∫–∏', ylab='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–æ–±–æ–∫')"""
            st.code(code, language='r')
            st.image(Image.open("images/r18.png"))
            st.markdown("""
                        Based on the resulting density plot, it can be concluded that the distribution of box weights is close to normal. 
                        The graph visually resembles a normal distribution, although there are some deviations near the edges (especially on the right side, 
                        where there is some asymmetry).""")
            # 2
            st.write("**2. Test the hypothesis of normality of the box weight distribution using the Pearson's chi-squared test.**")
            code = """
            ?chisq.test
            chisq.test(butter, y = NULL, correct = TRUE,
                    p = rep(1/length(butter), length(butter)), rescale.p = FALSE,
                    simulate.p.value = FALSE)"""
            st.code(code, language='r')
            st.markdown("""
                        Normal distribution of box weight:
                        - X-squared = 16.646
                        - Degrees of freedom (df) = 179
                        - p-value = 1
                        
                        The obtained **p-value = 1** is significantly greater than the standard significance level (0.05). 
                        The test results indicate that **the distribution of box weights does not differ from the normal distribution**. 
                        Thus, the hypothesis of normality of the box weight distribution is supported by the Pearson's test.""")
            # 3
            st.write("**3. Test the hypothesis of normality using the Shapiro-Wilk test.**")
            code = """
            shapiro.test(butter)"""
            st.code(code, language='r')
            st.markdown("""
                        Results of the Shapiro-Wilk test for normality of the box weight:
                        - W = 0.9907
                        - p-value = 0.2943
                        
                        The obtained **p-value = 0.2943** is greater than the standard significance level (0.05). 
                        This means there is no reason to reject the null hypothesis that the data come from a normal distribution. 
                        Thus, according to the Shapiro-Wilk test, it can be stated that the **weight of the boxes with butter is normally distributed**.""")
            # 4 
            st.write("""**4. The weight of the box with butter should be 10 kg. Can it be considered that the deviation from the norm of 10 kg 
                     is insignificant? Use Student‚Äôs t-test for the equality of the mean of a normally distributed random variable to the specified value.**""")
            code = """
            t.test(butter, mu=10)"""
            st.code(code, language='r')
            st.markdown("""
                        Results of the test for equality of the mean box weight to 10 kg:
                        - t = 0.61577
                        - degrees of freedom (df) = 179
                        - p-value = 0.5388
                        - confidence interval: from 9.9022 to 10.1865
                        - mean value: 10.0444 kg
                        
                        The obtained **p-value = 0.5388** is greater than the standard significance level (0.05). 
                        This means there is **no reason to reject the null hypothesis**. Thus, according to the t-test results, the **deviation from the 
                        10 kg norm is insignificant**. The average box weight is approximately 10.0444 kg, and this deviation is not statistically significant.""")
            st.subheader("Part 2")
            # 1
            st.write("**1. Plot on the same coordinate system the daily revenues of both firms as scatter plots. Each firm's points should have its own color.**")
            code = """
            plot(revenue1, col='darkorchid3', pch=16, xlab='–î–Ω—ñ',
                ylab='–í–∏—Ä—É—á–∫–∞ –≤ –¥–µ–Ω—å (—Ç–∏—Å. –≥—Ä–Ω)', main='–î–µ–Ω–Ω–∞ –≤–∏—Ä—É—á–∫–∞')
            points(revenue2, pch=16, col='goldenrod2')
            grid()"""
            st.code(code, language='r')
            st.image(Image.open("images/r19.png"))
            # 2
            st.write("**2. Can the volumes of daily revenues of both firms be considered comparable? Provide an answer based on the results of testing the hypothesis about the equality of the means of revenue volumes of the two firms using Student‚Äôs t-test.**")
            code = """
            t.test(revenue1, revenue2)"""
            st.code(code, language='r')
            st.markdown("""
                        Results of Welch‚Äôs t-test for testing the hypothesis about equality of the means of daily revenues of two firms:
                        - t value = 0.96119
                        - Degrees of freedom (df) = 197.51
                        - p-value = 0.3376
                        - Confidence interval: from -5.9171 to 17.1697
                        - Estimated mean revenues:
                            - Firm 1: 579.6824
                            - Firm 2: 574.0561
                        
                        Since the obtained p-value = 0.3376 is much greater than the standard significance level (0.05), there is **no reason to reject the null hypothesis** 
                        stating that the mean revenues of both firms are equal. Thus, based on the test results, it can be concluded that the **volumes of daily revenues of both 
                        firms can be considered comparable**. No statistically significant difference was found between the mean revenue volumes of the firms.""")
            # 3
            st.write("**3. Test the hypothesis about equality of variances of sales volumes of the two firms using Fisher's F-test.**")
            code = """
            var.test(revenue1, revenue2)"""
            st.code(code, language='r')
            st.markdown("""
                        Results of the F-test for testing the hypothesis about equality of variances of sales volumes of two firms:
                        - F value = 1.1044
                        - Degrees of freedom numerator: 99
                        - Degrees of freedom denominator: 99
                        - p-value = 0.6222
                        - Confidence interval: from 0.7431 to 1.6414
                        - Estimated ratio of variances: 1.1044
                        
                        Since the obtained **p-value = 0.6222** is much greater than the standard significance level (0.05), there is **no reason to reject the null hypothesis** stating 
                        that the variances of sales volumes of the two firms are equal. Thus, it can be concluded that the **variances of sales volumes of the two firms can be considered equal**, 
                        as no statistically significant difference between them was detected.""")
            # 4.1
            st.write("**4.1 Conduct a correlation analysis of sales volumes of the two firms. A scatter plot of sales volumes of firm 1 versus firm 2.**")
            code = """
            par(mfrow=c(1,1))
            plot(revenue1, revenue2, pch=16, col='darkseagreen3',
                main='–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –¥–æ—Ö–æ–¥—É —Ñ—ñ—Ä–º–∏ 1 –≤—ñ–¥ —Ñ—ñ—Ä–º–∏ 2',
                xlab='–§—ñ—Ä–º–∞ 1', ylab='–§—ñ—Ä–º–∞ 2')"""
            st.code(code, language='r')
            st.image(Image.open("images/r20.png"))
            # 4.2
            st.write("**4.2 Conduct a correlation analysis of sales volumes of the two firms. A scatter plot of sales volumes of firm 2 versus firm 1.**")
            code = """
            plot(revenue2, revenue1, pch=16, col='deeppink4',
                main='–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –¥–æ—Ö–æ–¥—É —Ñ—ñ—Ä–º–∏ 2 –≤—ñ–¥ —Ñ—ñ—Ä–º–∏ 1',
                xlab='–§—ñ—Ä–º–∞ 2', ylab='–§—ñ—Ä–º–∞ 1')"""
            st.code(code, language='r')
            st.image(Image.open("images/r21.png"))
            st.markdown("""If a strong correlation existed, we would expect to see some structure or trend (a line) in the distribution of points. 
                        In this case, the points are scattered throughout the plots without a clear trend, indicating the absence of a significant relationship between sales volumes.""")
            # 5
            st.write("**5. Use Pearson, Spearman, and Kendall criteria to test for the presence of correlation between sales volumes of the two firms.**")
            st.markdown("Results:")
            code = """
            cor(revenue1, revenue2, method='pearson')"""
            st.code(code, language='r')
            st.markdown("Pearson correlation coefficient: -0.0991")
            code = """
            cor(revenue1, revenue2, method='spearman')"""
            st.code(code, language='r')
            st.markdown("Spearman correlation coefficient: -0.0903")
            code = """
            cor(revenue1, revenue2, method='kendall')"""
            st.code(code, language='r')
            st.markdown("Kendall correlation coefficient: -0.0683")
            st.markdown("""
                        **Presence of correlation:**
                        
                        - All three correlation coefficients (Pearson, Spearman, and Kendall) are negative and close to zero. This indicates that there is no significant correlation between the sales volumes of the two firms.
                        - Values close to zero suggest that changes in the sales volume of one firm do not noticeably affect the sales volume of the other firm.
                        
                        **Type of correlation:**
                        
                        Pearson coefficient measures linear relationship, while Spearman and Kendall evaluate monotonic relationship. All three methods give similar results confirming the lack of correlation.
                        
                        **Significance of results:**
                        
                        Since all correlation coefficients are near zero, it can be stated that there is no statistically significant correlation between the sales volumes of the two firms.
                        
                        Thus, the results indicate that the sales volumes of the firms are not related, and changes in one firm‚Äôs sales do not necessarily cause changes in the other‚Äôs.""")

        elif selected == "Exploratory Analytics. Using Basic Graphics Package":
            st.title("Exploratory Analytics. Using Basic Graphics Package")
            # 1
            st.write("**1. Create a scatter plot in R using the built-in women dataset to visualize the relationship between height and weight of American women.**")
            code = """
            women
            plot(women$height, women$weight, 
                xlab = "Height", ylab = "Weight", 
                main = "American Women-weight vs Height", 
                col = "blue", 
                pch = 19)"""
            st.code(code, language='r')
            st.image(Image.open("images/r22.png"))
            # 2
            st.write("**2. Using R‚Äôs built-in dataset Nile, plot the annual river flow over the years, calculate the mean annual flow and add a horizontal line representing this average.**")
            code = """
            Nile
            plot(Nile, 
                main = "Nile River Annual Flow",  
                xlab = "Year",                    
                ylab = "Flow",                    
                col = "red",                     
                type = "l")                       

            abline(h=mean(Nile)) 

            text(1940, 1300, 
                labels = paste("Average Flow:", round(mean(Nile), 2)))"""
            st.code(code, language='r')
            st.image(Image.open("images/r23.png"))
            # 3
            st.write("**3. The dataset cars contains speed and braking distance. Plot the data on a scatter plot and analyze the results.**")
            code = """
            cars
            plot(cars$speed, cars$dist, 
                xlab = "–®–≤–∏–¥–∫—ñ—Å—Ç—å",          
                ylab = "–ì–∞–ª—å–º—ñ–≤–Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å",
                main = "–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –≥–∞–ª—å–º—ñ–≤–Ω–æ—ó –≤—ñ–¥—Å—Ç–∞–Ω—ñ –≤—ñ–¥ —à–≤–∏–¥–∫–æ—Å—Ç—ñ",  
                pch = 19,                 
                col = "lightseagreen", 
                col.main = "darkred",  
                col.lab = "blue")   
            grid(col = 'black', lty = 1)"""
            st.code(code, language='r')
            st.image(Image.open("images/r24.png"))
            st.markdown("""
                        **Relationship between speed and braking distance:** The points on the graph are arranged approximately 
                        along a line that goes from the lower left to the upper right, indicating a positive correlation between 
                        the car‚Äôs speed and braking distance. This means that as the car‚Äôs speed increases, the braking distance 
                        also increases.
                        
                        **Linearity of the relationship:** Although the points do not form a perfectly straight line, the trend 
                        toward linearity is quite clear. At higher speeds, the car needs more time and distance to come to a full stop. 
                        Therefore, an increase in speed significantly affects braking distance.
                        """)
            # 4
            st.write("""**4. Use the rivers dataset. Create a graph showing the lengths of rivers against their index in the dataset. 
                     Add a label to the Y-axis: "Length in miles". Add a red-colored title in two lines: "Length of Major N. American Rivers". Display the points in green.**""")
            code = """
            rivers
            plot(rivers,
                ylab = "Length in miles",          
                col = "lightgreen",                  
                pch = 19)                             
            title(main = "Length of Major N. American Rivers", 
                col.main = "red", font.main = 2)"""
            st.code(code, language='r')
            st.image(Image.open("images/r25.png"))
            # 5
            st.write("""**5. Create a vector with the following numbers: 60 85 72 59 37 75 93 7 98 63 41 90 5 17 97. 
                     Build a histogram and a bar chart for this vector, placing them in one row. What is the difference?**""")
            code = """
            numbers <- c(60, 85, 72, 59, 37, 75, 93, 7, 98, 63, 41, 90, 5, 17, 97)

            par(mfrow = c(1, 2))

            hist(numbers, 
                col = rainbow(length(numbers)), 
                main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞",
                col.main = "lightgreen",
                font.main = 3)

            barplot(numbers, 
                    col = rainbow(length(numbers)),  
                    main = "–°—Ç–æ–≤–ø—á–∏–∫–æ–≤–∞ –¥—ñ–∞–≥—Ä–∞–º–∞",
                    col.main = 'blue', 
                    font.main = 3)"""
            st.code(code, language='r')
            st.image(Image.open("images/r26.png"))
            st.markdown("""
                        - **Histogram:**
                            - Used to display the distribution of numerical data.
                            - Divides the data into intervals (bins), showing the count of values that fall into each bin.
                            - Displays frequency as adjacent bars.
                        
                        - **Bar chart:**
                            - Used for categorical data.
                            - Each bar represents a separate category (in this case, the individual values of the vector).
                            - The height of each bar shows the frequency or magnitude of the corresponding category.
                        """)
            # 6
            st.write("""**6. Using the command rnorm(100), generate 100 random numbers from a normal distribution. 
                     Create two histograms with two different sets of 100 numbers generated from a normal distribution using 
                     the same command. Will the histograms be identical?**""")
            code = """
            par(mfrow = c(1, 2))
            set1 <- rnorm(100, mean = 10, sd = 5)
            hist(set1, breaks = 20, col = "lightgreen", main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ 1")

            set2 <- rnorm(100, mean = 10, sd = 5)
            hist(set2, breaks = 20, col = "lightblue", main = "–ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ 2")"""
            st.code(code, language='r')
            st.image(Image.open("images/r27.png"))
            st.markdown("""
                        The histograms will not be exactly the same, as each set of random numbers is generated independently 
                        and will differ due to randomness. However, they may look similar because both sets are generated from 
                        the same normal distribution.
                        """)
            # 7
            st.write("**7. Load the data from the file firtree.csv. Build a bar chart showing the distribution of the number of coniferous trees in both numeric and percentage formats, using different shades of green.**")
            code = """
            data <- read.csv("firtree.csv")
            str(data)
            data <- na.omit(data)
            par(mfrow = c(1, 2))

            barplot(table(data$ftype), main = "–•–≤–æ–π–Ω—ñ –¥–µ—Ä–µ–≤–∞", 
                        col = c('darkolivegreen', 'darkolivegreen1', 'darkolivegreen2', 'darkolivegreen3'))

            perc <- table(data$ftype)/sum(table(data$ftype)) * 100
            perc
            barplot(perc, main = '–•–≤–æ–π–Ω—ñ –¥–µ—Ä–µ–≤–∞',
                    col = c('palegreen1', 'palegreen2', 'palegreen3', 'palegreen4'))"""
            st.code(code, language='r')
            st.image(Image.open("images/r28.png"))
            # 8
            st.write("**8. Build a histogram for tree height. Add a title to the histogram, change its color, and label the axes.**")
            code = """
            par(mfrow = c(1, 1))
            hist(data$height, 
                main = "Histogram of height",  
                xlab = "Height (in cm)",      
                ylab = "Counts",                   
                col = "lightgreen")"""
            st.code(code, language='r')
            st.image(Image.open("images/r29.png"))
            # 9
            st.write("**9. Build a pie chart showing the distribution of coniferous tree types. Change the colors of the chart to your own.**")
            code = """
            pie(table(data$ftype), 
                main = "–•–≤–æ–π–Ω—ñ –¥–µ—Ä–µ–≤–∞",   
                col = c("lightcoral", "lightskyblue", "peachpuff", "plum")) """
            st.code(code, language='r')
            st.image(Image.open("images/r30.png"))
            # 10
            st.write("**10. Add percentage values of tree distribution to the pie chart. Add a legend to the pie chart.**")
            code = """
            tree_counts <- table(data$ftype)
            tree_percentages <- round(100 * tree_counts / sum(tree_counts), 1)
            labels <- paste(tree_percentages, "%")

            pie(table(data$ftype), 
                labels = labels, 
                main = "–•–≤–æ–π–Ω—ñ –¥–µ—Ä–µ–≤–∞",   
                col = c("lightcoral", "lightskyblue", "peachpuff", "plum"))

            legend("topright", legend = names(tree_counts), fill = c("lightcoral", "lightskyblue", "peachpuff", "plum"))"""
            st.code(code, language='r')
            st.image(Image.open("images/r31.png"))

        elif selected == "Advanced Visual Analytics Tools":
            st.title("Advanced Visual Analytics Tools")
            # 1
            st.write("""**1. Install the vcd package. Load the Arthritis table, which is part of the installed package. 
                     The table contains data from a clinical trial that investigated a new treatment for rheumatoid arthritis. 
                     Create simple vertical and horizontal bar charts.**""")
            code = """
            install.packages("vcd")
            library(vcd)
            Arthritis

            freq <- table(Arthritis$Improved)  
            freq
            par(mfrow = c(1,2))
            barplot(freq, 
                    main = "–í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–æ–≤–∞ –¥—ñ–∞–≥—Ä–∞–º–∞", 
                    xlab = "–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è", 
                    ylab = "–ß–∞—Å—Ç–æ—Ç–∞", 
                    col = "red")
            barplot(freq, 
                    main = "–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–æ–≤–∞ –¥—ñ–∞–≥—Ä–∞–º–∞", 
                    xlab = "–ß–∞—Å—Ç–æ—Ç–∞", 
                    ylab = "–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è", 
                    horiz = TRUE, 
                    col = "green")"""
            st.code(code, language='r')
            st.image(Image.open("images/r32.png"))
            # 2
            st.write("**2. Create bar charts with vertical and horizontal subgroup distributions.**")
            code = """
            freq_subgroups <- table(Arthritis$Improved, Arthritis$Treatment)
            freq_subgroups
            par(mfrow = c(1,2))
            barplot(freq_subgroups, 
                    main = "–°—Ç–æ–≤–ø—á–∏–∫–æ–≤–∞ –¥—ñ–∞–≥—Ä–∞–º–∞ –∑ –≤–µ—Ä—Ç —Ä–æ–∑–±–∏—Ç—Ç—è–º", 
                    xlab = "–õ—ñ–∫—É–≤–∞–Ω–Ω—è", 
                    ylab = "–ß–∞—Å—Ç–æ—Ç–∞", 
                    col = c("red", "yellow", "green"), 
                    legend = rownames(freq_subgroups))
            barplot(freq_subgroups, 
                    main = "–°—Ç–æ–≤–ø—á–∏–∫–æ–≤–∞ –¥—ñ–∞–≥—Ä–∞–º–∞ –∑ –≥–æ—Ä–∏–∑ —Ä–æ–∑–±–∏—Ç—Ç—è–º", 
                    xlab = "–õ—ñ–∫—É–≤–∞–Ω–Ω—è", 
                    ylab = "–ß–∞—Å—Ç–æ—Ç–∞", 
                    beside = TRUE, 
                    col = c("red", "yellow", "green"), 
                    legend = rownames(freq_subgroups))"""
            st.code(code, language='r')
            st.image(Image.open("images/r33.png"))
            # 3
            st.write("**3. Create spine plots ‚Äì a special type of bar chart.**")
            code = """
            library(vcd)
            attach(Arthritis)
            freq <- table(Treatment, Improved)
            spine(freq, 
                main = "–ü—Ä–∏–∫–ª–∞–¥ —Å–ø—ñ–Ω–æ–≥—Ä–∞–º–∏")
            detach(Arthritis)"""
            st.code(code, language='r')
            st.image(Image.open("images/r34.png"))
            st.markdown("*(I tried several options, but unfortunately, I couldn‚Äôt manage to color them.)*")
            # 4
            st.write("""**4. Using R base visualization commands, reproduce the given chart.**""")
            code = """
            head(InsectSprays)
            str(InsectSprays)
            par(mfrow = c(1, 2))
            boxplot(InsectSprays$count, 
                    main = "Effectiveness of Spray", 
                    ylab = "Count", 
                    xlab = "All Sprays", 
                    col = "coral")
            boxplot(InsectSprays$count~InsectSprays$spray,
                    main = "Effectiveness by Spray",
                    ylab = "Count",
                    xlab = "Type of sprays", 
                    col = "lightblue")"""
            st.code(code, language='r')
            st.image(Image.open("images/r35.png"))
            # 5
            st.write("""**5. For the mtcars dataset, create enhanced boxplots.**""")
            code = """
            mtcars
            par(mfrow = c(1, 1))
            boxplot(mpg ~ cyl, 
                    data = mtcars, 
                    notch = TRUE, 
                    varwidth = TRUE, 
                    col = c("lightblue", "lightgreen", "salmon"),
                    main = "–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∞–≤—Ç–æ", 
                    ylab = "–í–∏—Ç—Ä–∞—Ç–∏ –ø–∞–ª—å–Ω–æ–≥–æ", 
                    xlab = "–ß–∏—Å–ª–æ —Ü–∏–ª—ñ–Ω–¥—Ä—ñ–≤")"""
            st.code(code, language='r')
            st.image(Image.open("images/r36.png"))
            # 6
            st.write("""**6. For example, let‚Äôs set the task of examining fuel consumption not only by the number of cylinders but also by the type of car transmission (manual or automatic).**""")
            code = """
            mtcars$cyl.f <- factor(mtcars$cyl, 
                                levels = c(4, 6, 8),
                                labels = c("4—Ü", "6—Ü", "8—Ü"))
            mtcars$am.f <- factor(mtcars$am,
                                levels = c(0, 1),
                                labels = c("–ê–¢", "–ú–¢"))

            boxplot(mpg ~ am.f * cyl.f, 
                    data = mtcars, 
                    varwidth = TRUE, 
                    col = c("darkolivegreen1", "lightskyblue"), 
                    main = "–í–∏—Ç—Ä–∞—Ç–∏ –ø–∞–ª—å–Ω–æ–≥–æ –≤ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ —Ü–∏–ª—ñ–Ω–¥—Ä—ñ–≤ —ñ —Ç–∏–ø—É —Ç—Ä–∞–Ω—Å–º—ñ—Å—ñ—ó", 
                    ylab = "–í–∏—Ç—Ä–∞—Ç–∏ –ø–∞–ª—å–Ω–æ–≥–æ (mpg)", 
                    xlab = "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ü–∏–ª—ñ–Ω–¥—Ä—ñ–≤ —Ç–∞ —Ç–∏–ø —Ç—Ä–∞–Ω—Å–º—ñ—Å—ñ—ó")

            legend("topright", 
                legend = c("–ê–¢", "–ú–¢"), 
                fill = c("darkolivegreen1", "lightskyblue"), 
                title = "–¢–∏–ø —Ç—Ä–∞–Ω—Å–º—ñ—Å—ñ—ó")"""
            st.code(code, language='r')
            st.image(Image.open("images/r37.png"))
            # 7
            st.write("**7. For the mtcars dataset, create another modification of the fuel consumption plots for cars with four, six, and eight cylinders ‚Äì violin plots.**")
            code = """
            install.packages("vioplot")
            library(vioplot)
            x1 <- mtcars$mpg[mtcars$cyl == 4]
            x2 <- mtcars$mpg[mtcars$cyl == 6]
            x3 <- mtcars$mpg[mtcars$cyl == 8]
            par(mfrow = c(1, 1))
            vioplot(x1, x2, x3, 
                    names = c("4 —Ü–∏–ª—ñ–Ω–¥—Ä–∏", "6 —Ü–∏–ª—ñ–Ω–¥—Ä—ñ–≤", "8 —Ü–∏–ª—ñ–Ω–¥—Ä—ñ–≤"), 
                    col = "mediumorchid1")
            title("–°–∫—Ä–∏–ø–∫–æ–≤—ñ –¥—ñ–∞–≥—Ä–∞–º–∏ –≤–∏—Ç—Ä–∞—Ç –ø–∞–ª—å–Ω–æ–≥–æ")"""
            st.code(code, language='r')
            st.image(Image.open("images/r38.png"))

# GOOGLE SHEETS
# if selected == 'Google Sheets':
    st.markdown("<h1 style='text-align: center;'>Google Sheets</h1>", unsafe_allow_html=True)

# RECOMMENDER SYSTEM
if selected == 'Recommender System':
    # DATASETS
    l_data = pd.read_csv('grs/data extraction and preprocessing/data3.csv')
    l_data.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)

    similarity = pickle.load(open('grs/data/similarity.pkl','rb'))


    # MAINPAGE
    st.markdown("<h1 style='text-align: center;'>üéÆ Game Recommender system</h1>", unsafe_allow_html=True)

    st.write("Imagine you're wandering through a vast library of games, and suddenly, a quirky little friend pops up, saying, ")

    st.markdown("""<div style="text-align: center;">
                    <p><em><strong>"Hey there! Feeling lost in this jungle of pixels? Let me be your guide!"</strong></em></p>
                </div>
            """, unsafe_allow_html=True)

    st.write("That's what a game recommender system is like‚Äîit's your trusty sidekick in the world of gaming, "
         "helping you find the perfect game without getting lost in the maze of choices."
         " Think of it as your own personal game genie üßû‚Äç‚ôÇÔ∏è, but instead of granting wishes, it grants you "
         "hours of entertainment and fun! It's like having a friend who knows you better than you know "
         "yourself when it comes to gaming...")

    st.write("\nSo, next time you're scratching your head, wondering what "
         "game to play next, just come here and follow the set of instructions on the sidebar!")



    # RECOMMENDATION

    st.write("<h4>Choose games here</h4>", unsafe_allow_html=True)
        
    games = st.multiselect('', l_data['name'], [], key='games')
        
    recommed = st.button('Recommend')
    col1,col2 = st.columns(2)

    if recommed:
        top_recommendations = []

        for game in games:
            index = l_data[l_data['name'] == game].index[0]
            top_six = sorted(list(enumerate(similarity[index])),reverse=True,key=lambda x:x[1])[1:7]
            top_recommendations.extend([top_six[i][0] for i in range(len(top_six))]) 

        top_recommendations = list(set(top_recommendations))[:6]
            
        for i, k in enumerate(top_recommendations):
            if i % 2 == 0:
                with col1:
                    st.markdown("""
                            <div style="display: flex; flex-direction: column; align-items: center; margin-bottom: 20px;">
                            <a href="{link}" target="_blank"><img src="{image}" style="width:500px;height:250px; margin-bottom: 10px;"></a>
                            <div style="text-align: center;">
                                <p style="margin: 0; font-size: 20px;"><b>Title: </b>{name}</p>
                                <p style="margin: 0; font-size: 20px;"><b>Developer: </b>{developer}</p>
                            </div>
                            </div>
                            """.format(link=l_data["Steam Page"][k], image=l_data["header_image"][k], name=l_data["name"][k], developer=l_data['developer'][k]), unsafe_allow_html=True)
            else:
                with col2:
                    st.markdown("""
                            <div style="display: flex; flex-direction: column; align-items: center; margin-bottom: 20px;">
                            <a href="{link}" target="_blank"><img src="{image}" style="width:500px;height:250px; margin-bottom: 10px;"></a>
                            <div style="text-align: center;">
                                <p style="margin: 0; font-size: 20px;"><b>Title: </b>{name}</p>
                                <p style="margin: 0; font-size: 20px;"><b>Developer: </b>{developer}</p>
                            </div>
                            </div>
                            """.format(link=l_data["Steam Page"][k], image=l_data["header_image"][k], name=l_data["name"][k], developer=l_data['developer'][k]), unsafe_allow_html=True)
 

    # SIDEBAR
    # sidebar.instructions
    st.sidebar.header("Set of instructions üôÇ")

    st.sidebar.markdown("1. Choose a game from the dropdown menu.")
    st.sidebar.markdown("2. Click the 'Recommend' button.")
    st.sidebar.markdown("3. Explore the recommended games.")
    st.sidebar.markdown("4. Click on image to visit Steam page. ")
    st.sidebar.markdown("5. Enjoy!")

    st.sidebar.markdown('---')

    # sidebar.tags
    for game in games:
        selected_game_categories = l_data.loc[l_data['name'] == game, 'Tags'].iloc[0]
    
        if selected_game_categories:
            st.sidebar.subheader(f"The chosen game '{game}' belongs to:")
            categories_list = selected_game_categories.split(',')
            for category in categories_list:
                st.sidebar.write(f"- {category.strip()}")

    
    # TRENDING

    st.markdown('---')
  
    trending = st.subheader('Trending games')

    st.write("**Need a break from reality?** üôÉ")

    st.write("\nGames offer the perfect escape! They whisk us away to fantastical realms, where we can "
                 "embark on thrilling adventures, solve mind-bending puzzles, or simply unwind after a long day."
                 " Whether you're battling dragons in a mythical land or building your dream city from scratch, "
                 "games provide a welcome reprieve from the stresses of everyday life. ")
        
    st.write("\nDive into the world of gaming and discover why millions around the globe turn to Steam for their dose of fun and relaxation. "
                 "Check out the trending games on Steam now and treat yourself to some well-deserved entertainment! **Click on the image...**")
                 

    col3, col4 = st.columns(2)
    for i in range(4):
            if i % 2 == 0:
                with col3:
                    st.markdown("""
                        <div style="display: flex; flex-direction: column; align-items: center; margin-bottom: 20px;">
                        <a href="{link}" target="_blank"><img src="{image}" style="width:500px;height:250px; margin-bottom: 10px;"></a>
                        <div style="text-align: center;">
                            <p style="margin: 0; font-size: 20px;"><b>Title: </b>{name}</p>
                            <p style="margin: 0; font-size: 20px;"><b>Developer: </b>{developer}</p>
                        </div>
                        </div>
                        """.format(link=l_data["Steam Page"][i], image=l_data["header_image"][i], name=l_data['name'][i], developer=l_data['developer'][i]), unsafe_allow_html=True)
            else:
                with col4:
                    st.markdown("""
                        <div style="display: flex; flex-direction: column; align-items: center; margin-bottom: 20px;">
                        <a href="{link}" target="_blank"><img src="{image}" style="width:500px;height:250px; margin-bottom: 10px;"></a>
                        <div style="text-align: center;">
                            <p style="margin: 0; font-size: 20px;"><b>Title: </b>{name}</p>
                            <p style="margin: 0; font-size: 20px;"><b>Developer: </b>{developer}</p>
                        </div>
                        </div>
                        """.format(link=l_data["Steam Page"][i], image=l_data["header_image"][i], name=l_data['name'][i], developer=l_data['developer'][i]), unsafe_allow_html=True)

# DIAGRAMS                
if selected == 'Diagrams':
    st.markdown("<h1 style='text-align: center;'>Diagrams</h1>", unsafe_allow_html=True)
    st.markdown("""
                This section showcases a collection of diagrams (BPMN, Flowcharts, Use Case, Activity Diagram, IDEF0, IDEF3, DFD) 
                that I created during a university course. While they were technically part of an academic assignment, the focus 
                was on practical, real-world business scenarios‚Äîbecause companies don‚Äôt really care how pretty your arrows are if 
                the process still gets stuck halfway. We modeled common business situations, identified where things could (and often do) 
                go wrong, and worked on making processes more efficient.
                
                These diagrams highlight not only my ability to use different visualization and modeling techniques but also my 
                problem-solving mindset. In short, I didn‚Äôt just draw boxes and lines‚ÄîI tried to make sense of chaos and turn it into 
                something that actually works. (And yes, sometimes it felt like detective work for business processes!)
                """)
    selected_optipns = ["1. IDEF0 Notation. Context Diagram",
                        "2. IDEF0 Notation. Decomposition",
                        "3. IDEF0 + IDEF3",
                        "4. Data Flow Diagram (DFD)",
                        "5. IDEF0 + IDEF3 + DFD",
                        "6. Flowchart. Customer Satisfaction Analysis",
                        "7. Use Case Diagrams",
                        "8. Activity Diagram",
                        "9. BPMN Models",
                        "10. BPMN. Data collection and processing for decision-making",
                        "11. BPMN Collaboration Diagram",
                        "12. BPMN Process Diagram",
                        "13. BPMN Process Model. Needs Analysis"]
    selected = st.selectbox("Select a project", options = selected_optipns)
    st.write("Current selection:", selected)
    if selected == "1. IDEF0 Notation. Context Diagram":
        st.markdown("""
                    ## *IDEF0 Notation. Context Diagram*
                    - **Define the business process you will model (either the one selected earlier or a new one).**
                    - **Identify and specify the following for the entire process and its subprocesses:**
                        1. Inputs (resources that enter the process),
                        2. Outputs (results produced by the process),
                        3. Mechanisms (tools, people, or systems performing the process),
                        4. Controls (rules, standards, or management influencing the process).
                    - **Create a context diagram in IDEF0 notation and add arrows to represent the attributes listed above.**""")
        st.image(Image.open("images/d9.jpeg"), use_column_width=True)
    
    elif selected == "2. IDEF0 Notation. Decomposition":
        st.markdown("""
                    ## *IDEF0 Notation. Decomposition*
                    - **Create decomposition diagrams**
                        1. Develop two IDEF0 decomposition diagrams based on your context diagram and the defined subprocesses and operations.
                        2. If you identify errors in the context diagram, correct them in the working file before proceeding.
                    - **Describe each subprocess. For every subprocess, provide the following details**:
                        1. Name of the subprocess,
                        2. Unit of measurement (e.g., duration),
                        3. Short description,
                        4. Possible costs in the Cost Center (if applicable).
                    - **Add connections**
                        1. Attach external arrows to the functional blocks at the first level of decomposition.
                        2. Add arrows to indicate relationships between subprocesses.
                    - **Perform deeper decomposition**
                        1. Select one or two subprocesses and create their second-level decomposition diagrams.""")
        st.image(Image.open("images/d10.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the first level*")
        st.image(Image.open("images/d11.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the second level*")
        st.image(Image.open("images/d12.jpeg"), use_column_width=True)
        
    elif selected == "3. IDEF0 + IDEF3":
        st.markdown("""
                    ## *IDEF0 + IDEF3. General Report*
                    - **Create a model for one of the following business processes (BPs) using IDEF0 notation (context diagram) and IDEF3 notation (Level 1 decomposition, Level 2 decomposition):**
                        1. Design of an information system for resource management or management subsystems
                        2. Analysis of market segments
                        3. Analysis of company business indicators
                        4. Monitoring of labor productivity
                        5. Monitoring of team/project performance
                    - **In the IDEF0 diagram, specify the name of the BP and its external arrows.**
                    - **In the decomposition diagrams, specify for each Unit of Work (UOW):**
                        1. name,
                        2. duration (cost, where applicable),
                        3. short description,
                        4. junctions,
                        5. arrows,
                        6. objects (if needed).
                    - **Demonstrate different types of relationships and junctions. Pay attention to synchronization and types of operators (AND, OR) in junctions, as well as their compatibility in conjunctions and disjunctions.**
                    - **Ensure that arrows are connected to blocks/UOWs and that they contain the required level of detail.**""")
        st.image(Image.open("images/d13.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the first level*")
        st.image(Image.open("images/d14.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the second level*")
        st.image(Image.open("images/d15.jpeg"), use_column_width=True)

    elif selected == "4. Data Flow Diagram (DFD)":
        st.markdown("""
                    ## *Data Flow Diagram (DFD)*
                    - **Create a Data Flow Diagram (DFD) for the product purchase process on the Rozetka website ‚Äî from selecting a product to completing the order.**
                    - **Use the following short description of the process, taking into account the requirements for naming activities and the number of tasks on one level. Divide subprocesses into two decomposition levels:**
                        1. The user visits the Rozetka website.
                        2. Selects the desired product from the catalog (search or filtering may be used).
                        3. Adds the product to the shopping cart.
                        4. Proceeds to checkout: enters personal data, selects delivery and payment methods.
                        5. The system processes the order and sends confirmation.
                    - **Consider the following elements in your DFD:**
                        1. **External entities:** User / Buyer, Payment System, Delivery Service
                        2. **Processes / Activities**
                        3. **Data stores:** Product Catalog, Shopping Cart, Customer Data
                        4. **Data flows**
                    - **Create the model structure as follows:**
                        1. DFD Level 0 (Context Diagram): general view of the purchase process, showing the overall process and external entities.
                        2. DFD Level 1 and Level 2 Decomposition Diagrams: break the process down into individual subprocesses.""")
        st.image(Image.open("images/d16.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the first level*")
        st.image(Image.open("images/d17.jpeg"), use_column_width=True)
        st.markdown("## *Decomposition of the second level*")
        st.image(Image.open("images/d18.jpeg"), use_column_width=True)
        
    elif selected == "5. IDEF0 + IDEF3 + DFD":   
        st.markdown("""
                    ## *IDEF0 + IDEF3 + DFD*
                    - **General Task:**
                    Create a model of one of the suggested business processes below, or use an example of your own business process from previous tasks, and build a single model that incorporates all three notations: IDEF0, IDEF3, and DFD.
                    - **Workflow Algorithm:**
                        1. Describe the selected business process in the context of the organization, considering model attributes (approx. 0.5 page). Also, state the purpose of modeling.
                        2. Build the AS-IS model (the model should include at least two levels of process decomposition).
                        3. Describe the model and attach figures from the report (1‚Äìn pages).
                        4. Identify bottlenecks / problematic areas in this process (0.5‚Äì1 page).
                        5. In a copy of your model, restructure the operations/subprocesses to show improvements. This will be the TO-BE model.
                        6. Describe the TO-BE model and attach figures from the report (0.5‚Äìn pages).""")
        st.markdown("""
                    ## *Task 1*
                    The business process considered in this report covers the full cycle of **elevator equipment manufacturing** ‚Äî from the development of technical documentation to the delivery of the finished product to the client. This process is a key activity at the machine-building enterprise, as it ensures order fulfillment, revenue generation, and customer satisfaction.
                    
                    **Purpose of modeling:**
                    To identify shortcomings (bottlenecks) in the existing process, formalize its structure using **IDEF0, IDEF3, and DFD** models, and propose optimization through the development of a **TO-BE model**.
                    
                    **Main objectives of the model:**
                    - Standardize approaches to design and production
                    - Minimize time and resource costs
                    - Improve product quality
                    - Automate routine processes
                    
                    **Model attributes:**
                    - Name: Elevator Equipment Manufacturing
                    - Author: Shcherbyna J.O.
                    - Context diagram: A-0
                    - Creation date: 28.04.2025
                    - Project: lab 13‚Äì14""")
        st.markdown("""
                    ## *Task 2*
                    **2.1. Level 1 ‚Äî IDEF0 Context Diagram**
                    At the first level, the overall process **‚ÄúManufacturing of Elevator Equipment‚Äù** is shown. The entire business process takes **32 days**. Inputs, outputs, controls, and mechanisms are represented as follows:
                    - **Inputs:** Technical requirements and specifications; Raw materials and supplies.
                    - **Controls:** Product quality control regulations; Production process management regulations.
                    - **Mechanisms:** Human resources; Materials and equipment; Financial resources for equipment procurement.
                    - **Outputs:** Finished elevator equipment.""")
        st.image(Image.open("images/d19.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.2. Decomposition of the Process "Manufacturing of Elevator Equipment" (IDEF0)**
                    This level reveals the internal structure of the key production process, including the following subprocesses:
                    - Development of technical documentation
                    - Production of parts
                    - Quality control
                    - Delivery to the client""")
        st.image(Image.open("images/d20.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.3. Decomposition of the Process "Production of Parts" (IDEF3)**
                    
                    This sub-level details the manufacturing part of the process. It shows how raw materials and drawings are transformed into finished parts. The model describes which operations make up the production of elevator equipment components.
                    
                    **Main processes:**

                    | ‚Ññ | Subprocess name                | Function description                                                    |
                    | - | ------------------------------ | ----------------------------------------------------------------------- |
                    | 1 | Preparation of blanks          | Receiving drawings and technical requirements, selecting materials      |
                    | 2 | Processing of blanks           | Mechanical and/or thermal processing according to specifications        |
                    | 3 | Dimensional control            | Verification of compliance of processed blanks with required dimensions |
                    | 4 | Surface quality control        | Visual or technical assessment of blank surfaces                        |
                    | 5 | Analysis of inspection results | Determination of suitability / unsuitability of parts                   |
                    | 6 | Welding                        | Joining of units after quality confirmation                             |
                    | 7 | Rework and welding             | Reprocessing or welding of unsuitable parts                             |
                    | 8 | Assembly                       | Assembly of suitable and welded units into finished parts               |

                    **Process logic:**

                    * **J1 ‚Äî AND:** processed blanks must undergo both dimensional control and surface quality control.
                    * **J2 ‚Äî AND:** results of both inspections are passed to analysis.
                    * **J3 ‚Äî OR:** if the part is suitable, it goes directly to welding or to rework and welding.
                    * **J4 ‚Äî OR:** units after welding or rework and welding can be immediately transferred to assembly.""")
        st.image(Image.open("images/d21.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.3. Decomposition of the Process ‚ÄúDelivery to the Client‚Äù (DFD)**

                    At this stage, a detailed description of the information flows that occur during the organization of the delivery of finished elevator equipment is presented. The DFD model demonstrates which data are transmitted between subsystems, databases, the client, and internal departments.

                    **Main processes:**

                    | ‚Ññ | Process name                  | Action description                                                                   |
                    | - | ----------------------------- | ------------------------------------------------------------------------------------ |
                    | 1 | Receiving inspected equipment | Arrival of equipment from production after quality control                           |
                    | 2 | Creating a delivery order     | Summarizing client data, address, type of delivery, creating a request               |
                    | 3 | Organizing transportation     | Communication with the logistics department, preparation of waybills, transportation |
                    | 4 | Shipping to the client        | Transfer of equipment to the client, confirmation of completion                      |

                    
                    **Databases:**

                    | Name                            | Purpose                                 |
                    | ------------------------------- | --------------------------------------- |
                    | Customer Database               | Data on regular customers               |
                    | Delivery Registry               | Log of created delivery requests        |
                    | Supporting Documents / Waybills | Generation of legally binding documents |

                    **External participants:**

                    | Name                 | Role                         |
                    | -------------------- | ---------------------------- |
                    | Client               | Equipment orderer, recipient |
                    | Logistics Department | Organizes transportation     |""")
        st.image(Image.open("images/d22.jpeg"), use_column_width=True)
        st.markdown("""
                    ## *Task 3*
                    **Description of Problematic ‚ÄúBottlenecks‚Äù in the Process**
                    1. **Manual documentation development**
                    The lack of automation in creating technical documentation leads to delays and increases the risk of errors.
                    2. **Inefficient quality control**
                    An excessive number of manual inspections slows down the process and does not guarantee consistent quality.
                    3. **Lack of production automation**
                    Processing parts on conventional machines without integrated monitoring reduces productivity.
                    4. **Complex logistics**
                    Order creation and delivery are performed manually, creating a risk of information loss or duplication.""")
        st.markdown("""
                    ## *Task 4*
                    **Description of the TO-BE Model**
                    
                    Based on the analysis of the current model and identification of bottlenecks, a **TO-BE model** was developed that incorporates modern digital technologies to improve the efficiency of manufacturing and delivery of elevator equipment to clients.
                    
                    **Key improvements in the TO-BE model:**
                    1. **Automated generation of technical documentation**
                        * Drawings, specifications, and requirements are created using **AutoCAD** integrated with the **ERP system**.
                        * All documents are stored centrally and synchronized with production modules.
                    2. **Part manufacturing using CNC machines**
                        * Blank processing is performed on modern **CNC equipment**.
                        * Quality control is integrated into the process via built-in sensors and automated controllers.
                    3. **Integrated quality control in MES**
                        * All inspection results are transmitted to the **MES system**.
                        * MES generates inspection reports and analytics without operator intervention.
                    4. **Automation of logistics and delivery**
                        * After equipment inspection, a delivery order is automatically created via the **CRM system**.
                        * Logistics are fully coordinated by the ERP module; documents are generated automatically, and the client receives notifications.
                    5. **Optimized client interaction**
                        * The **CRM system** stores order history, automatically populates client data, and tracks delivery status.
                        * Delivery tracking and confirmation are implemented via **API** or **email notifications**.
                    **Illustrations in the report**:
                        * Context diagram (IDEF0) showing inputs, outputs, controls, and resources with automated modules.
                        * Subprocess decomposition (IDEF3) highlighting CNC and MES integration in production.
                        * DFD diagram showing automated data flows between ERP, MES, CRM, logistics, and client.""")
        st.markdown("""
                    **2.1. Level 1 ‚Äî IDEF0 Context Diagram**
                    
                    ERP system and MES have been added as management methods, while AutoCAD, CNC machines, and automated controllers have been added as implementation mechanisms.""")
        st.image(Image.open("images/d23.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.2. Decomposition of the Process ‚ÄúElevator Equipment Manufacturing‚Äù (IDEF0)**
                    
                    Instead of manual documentation development, a block **‚ÄúAutomated Generation‚Äù** is implemented. Manufacturing and quality control are carried out via **CNC machines, AutoCAD, and the ERP system**. Quality control has been enhanced to **‚ÄúIntegrated Quality Control in MES‚Äù** using automated controllers.""")
        st.image(Image.open("images/d24.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.3. Decomposition of the Process ‚ÄúParts Manufacturing‚Äù (IDEF3)**
                    
                    Processing and inspection have been combined into a single block ‚Äî **‚ÄúCNC Processing of Blanks with Embedded Quality Control.**‚Äù""")
        st.image(Image.open("images/d25.jpeg"), use_column_width=True)
        st.markdown("""
                    **2.4. Decomposition of the Process ‚ÄúDelivery to Client‚Äù (DFD)**
                    
                    Logistics request creation and transportation management are carried out through **CRM and ERP systems**. A block **‚ÄúAutomatic Order Generation‚Äù** has been added.""")
        st.image(Image.open("images/d26.jpeg"), use_column_width=True)

    elif selected == "6. Flowchart. Customer Satisfaction Analysis":
        st.markdown("""
                    ## *Flowchart ‚ÄúCustomer Satisfaction Analysis‚Äù*
                    - **Create a flowchart for the business process (BP) ‚ÄúCustomer Satisfaction Analysis‚Äù. The company conducts a customer satisfaction analysis in order to:**
                        1. Understand customer demand
                        2. Perform a root cause analysis of purchases or user stories
                        3. Learn how customers rate their experience
                        4. Identify the most interesting areas for customers
                        5. Determine the level of customer satisfaction
                        6. Define areas for improvement""")
        col1, col2, col3 = st.columns([1, 2, 1])  
        with col2:
            st.image(Image.open("images/15-16. –ê–Ω–∞–ª—ñ–∑ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–æ—Å—Ç—ñ.jpg"), use_column_width=True)
    
    elif selected == "7. Use Case Diagrams":
        st.markdown("""
                    ## *Use Case Diagrams*
                    - **Build two Use Case Diagrams for the online purchase process on the Rozetka website, showing user actions during:**
                        1. product selection
                        2. payment completion
                    - **Represent include or extend relationships between main and additional use cases. Types of users (Actors):**
                        1. Registered User
                        2. Website Administrator
                        3. Payment System (as an external actor)""")
        st.markdown("## *Use case diagram. Product selection*")
        col1, col2, col3 = st.columns([1, 2, 1])  
        with col2:
            st.image(Image.open("images/17. Use case diagram –í–∏–±—ñ—Ä —Ç–æ–≤–∞—Ä—É.jpg"), use_column_width=True)
        st.markdown("## *Use case diagram. Payment completion*")
        col1, col2, col3 = st.columns([1, 2, 1])  
        with col2:
            st.image(Image.open("images/17. Use case diagram –ó–¥—ñ–π—Å–Ω–µ–Ω–Ω—è –æ–ø–ª–∞—Ç–∏.jpg"), use_column_width=True)

    elif selected == "8. Activity Diagram":
        st.markdown("""
                    ## *Activity Diagram*
                    - **Create an activity diagram for the process ‚ÄúData Analysis‚Äù.**
                    - **Apply all diagram elements:**
                        1. Initial nodes
                        2. Control flows
                        3. Actions
                        4. Decisions
                        5. Forks and joins (branching and merging)
                        6. Final nodes""")
        st.markdown("## *Activity Diagram. Data Analysis*")
        col1, col2, col3 = st.columns([1, 2, 1])  
        with col2:
            st.image(Image.open("images/18. Activity diagram –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö.jpg"), use_column_width=True)

    elif selected == "9. BPMN Models":
        st.markdown("""
                    ## *BPMN Models*
                    - **Main task: Build two process models according to the scenarios below.**
                    - **Scenario 1: Business Trip Request**
                        1. Submit a business trip request.
                        2. Send a notification about the new request and ask for approval from managers.
                        3. Agree on cost and duration.
                        4. Reserve tickets and proceed with payment if the manager approves.
                        5. Cancel the business trip if the manager rejects the request.
                        6. Prepare a report on the trip results after returning.
                    - **Scenario 2: Event Organization**
                        1. Collect information regarding participants and the organizing committee (e.g., booth, presentation, logo, etc.).
                        2. Create a list of participants attending the event.
                        3. Consolidate data ‚Äî coordinate press releases, logos, and other information for the organizers.
                        4. Select a speaker, approve the presentation topic, and prepare the presentation.
                        5. Conduct the event.
                        6. Summarize the event results.""")
        st.markdown("## *Scenario 1. Business Trip Request*")
        st.image(Image.open("images/d2.jpg"), use_column_width=True)
        st.markdown("## *Scenario 2. Event organization*")
        st.image(Image.open("images/d1.jpg"), use_column_width=True)

    elif selected == "10. BPMN. Data collection and processing for decision-making":
        st.markdown("""
                    ## *BPMN. Data collection and processing for decision-making*
                    **Build a BPMN model**
                    - **Business Process Option:** Data collection and processing for decision-making
                    - **Context of the Business Process in the Digital Economy:** Analysis of online transaction data, user behavior, and market trends.
                    - **In Business Analytics:** Using tools to collect, clean, and analyze large volumes of data.""")
        st.image(Image.open("images/d3.jpg"), use_column_width=True)

    elif selected == "11. BPMN Collaboration Diagram":
        st.markdown("""
                    ## *BPMN Collaboration Diagram*
                    Build a BPMN Collaboration Diagram that shows the interaction between a company and a software developer.
                    
                    **Context:**
                    The company collaborates with a software developer to implement a CRM system.

                    **Execution Steps:**
                    1. Open software for building a BPMN business process model.
                    2. Create two pools for participants:
                        - Company ‚Äì the system client, provides requirements and checks the results.
                        - Software Developer ‚Äì implements the CRM system and adapts it to the company‚Äôs needs.
                    3. Display the main message flows:
                        - The company sends requirements to the developer.
                        - The developer implements the CRM system and notifies the company that it is ready for testing.
                        - The company sends feedback to the developer regarding the system‚Äôs performance.""")
        st.image(Image.open("images/d4.jpg"), use_column_width=True)
        st.markdown("## *Sub-process (collapsed)*")
        st.image(Image.open("images/d5.jpg"), use_column_width=True)
    
    elif selected == "12. BPMN Process Diagram":
        st.markdown("""
                    ## *BPMN Process Diagram*
                    Build a BPMN Process Diagram for the process ‚ÄúImplementation of a New Analytical Model in the Company.‚Äù
                    
                    **Context:**
                    The company implements an analytical model to forecast sales and optimize inventory levels.
                    
                    **Execution Steps:**
                    1. Open software for building a BPMN business process model.
                    2. Create a process model using notation elements according to the context and process description.
                    3. Process Description:
                        - The Analytics Department develops the model based on historical data.
                        - The system tests the model on real data from the last month.
                        - Test results are analyzed: if the model works correctly, the process continues; if not, the model is adjusted.
                        - The model is integrated into the production system.
                        - The system automatically updates sales forecasts and inventory levels.
                        - The Sales Department (closed pool) receives reports for decision-making.
                    4. Include in the model:
                        - ateway to evaluate the success of testing;
                        - iteration for the iterative task;
                        - intermediate event with a trigger;
                        - sub-process, either expanded or collapsed.""")
        st.image(Image.open("images/d6.jpg"), use_column_width=True)
        st.markdown("## *Sub-process (collapsed)*")
        st.image(Image.open("images/d7.jpg"), use_column_width=True)

    elif selected == "13. BPMN Process Model. Needs Analysis":
        st.markdown("""
                    ## *BPMN Process Model. Needs Analysis*
                    Create a BPMN process model for ‚ÄúNeeds Analysis‚Äù.
                    
                    **Process Context:**
                    A business analyst conducts a needs analysis for a client to gather requirements for a potential digitalization project. The process includes a flow of tasks, interactions, and decision points.
                    
                    **Execution Steps:**
                    1. Include the following sub-processes.
                        - Initial Meeting with Client:
                            - The client schedules a meeting with the business analyst.
                            - The analyst confirms the meeting time and prepares the agenda.
                        - Requirements Gathering:
                            - During the meeting, the analyst collects high-level requirements from the client.
                        - Requirements Analysis:
                            - The analyst analyzes the collected information to identify gaps or unclear points.
                        - Client Feedback:
                            - The analyst shares the initial analysis with the client to get feedback.
                            - Gateway: If feedback is positive, the process continues; otherwise, the analyst returns to the analysis step.
                        - Final Report Preparation:
                            - The analyst prepares a detailed needs analysis report.
                        - Report Delivery:
                            - The final report is sent to the client for approval.
                    2. Include in the model:
                        - Message flows between pools,
                        - An exclusive gateway,
                        - Events with triggers,
                        - Text annotations,
                        - A closed pool for the client.
                    3. Add additional parameters:
                        - Duration for some tasks in alternative flows (e.g., requirements gathering ‚Äì 2 hours; requirements analysis ‚Äì 4 hours).
                        - Probabilities for decision points (e.g., 80% positive feedback, 20% requires re-analysis).
                    4. Analyze the results and create a separate document with suggestions:
                        - Identify bottlenecks or problematic areas (e.g., which part often delays the process).
                        - Provide optimization proposals (e.g., shorten process flow, assign an assistant for specific tasks, automate feedback collection).""")
        st.image(Image.open("images/d8.jpg"), use_column_width=True)
        st.markdown("""
                    **Bottlenecks / Problematic Areas:**
                    - **Lengthy analysis of collected information (4 hours).** This is the longest stage of the process. If the number of clients increases, the analyst may not keep up.
                    - **Waiting for client feedback.** The model only accounts for receiving feedback but does not control the waiting time.
                    - **Repeated analysis in case of negative feedback.** Requires additional time and resources and may repeat several times if feedback remains unsatisfactory.
                    - **Manual transfer of analysis to a competent analyst.**
                    
                    **Proposals for optimizing the process flow:**
                    1. **Automate feedback collection:**
                        - Use an online form (e.g., Google Forms or a CRM module).
                        - Set a time limit for responses (e.g., 24 hours), after which the system generates reminders or escalates the task.
                    2. **Assign an assistant or automate agenda preparation:**
                        - Delegate this task or support it with templates (question builders) to save the analyst‚Äôs time.
                    3. **Reduce analysis time using templates:**
                        - Use a report structure from previous cases.
                        - Use tools for requirement classification (e.g., Trello, Notion, Jira).
                    4. **Optimize handling of negative feedback:**
                        - Implement feedback categorization (technical / communication / expectations) to reduce unnecessary actions.
                        - If negative feedback is minor (e.g., stylistic), correct immediately without a full re-analysis.
                    5. **Implement KPIs for each stage:**
                        - For example: time to feedback, number of feedback cycles, average analysis time, number of cases requiring escalation.""")
